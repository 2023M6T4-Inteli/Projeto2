# Documento Principal do Projeto

Descri√ß√£o sucinta do projeto, com descri√ß√£o do problema, do objetivo e da solu√ß√£o em linhas gerais.

## (Sprint 1) Entendimento do Neg√≥cio

 ### üìà Matriz de avalia√ß√£o de valor Oceano Azul 

A Matriz Oceano Azul √© a ferramenta que ajuda as empresas √† identificar novas oportunidades de crescimento dentro do mercado e quais fatores √†s diferenciam da concorr√™ncia, criando valor para seus clientes.

Para esta an√°lise, foram setadas outras Intelig√™ncias Artificiais, baseadas em Processamento de Linguagem Natural (PLN), como **Open AI, Amazon Comprehend, Microsoft Azure Cognitive Services, Google Cloud Natural Language, IBM Watson.** 

A OpenAI √© uma plataforma de intelig√™ncia artificial que oferece uma ampla gama de ferramentas e recursos para empresas e indiv√≠duos. A plataforma utiliza modelos de linguagem natural avan√ßados, como o GPT-3, para criar solu√ß√µes personalizadas para problemas espec√≠ficos de neg√≥cios.
A Amazon Comprehend √© um servi√ßo de an√°lise de texto fornecido pela AWS que permite a identifica√ß√£o de insights e informa√ß√µes √∫teis a partir de dados textuais, em grande escala. A tecnologia usa t√©cnicas de aprendizado de m√°quina para extrair informa√ß√µes √∫teis a partir de dados textuais, como sentimentos, emo√ß√µes, t√≥picos, entidades, rela√ß√µes e muito mais. O servi√ßo √© capaz de analisar grandes quantidades de texto, em v√°rias l√≠nguas, e produzir informa√ß√µes significativas para apoiar a tomada de decis√µes de neg√≥cios, como por exemplo conjuntos de dados de texto, incluindo documentos, mensagens de texto, posts de redes sociais e outras formas de comunica√ß√£o escrita, para identificar tend√™ncias.

O Microsoft Azure Cognitive Services √© um conjunto de servi√ßos cognitivos com aloca√ß√£o em nuvem que permite que desenvolvedores agreguem recursos de intelig√™ncia artificial (IA) a seus aplicativos sem a necessidade de experi√™ncia em aprendizado de m√°quina ou an√°lise de dados. Esses servi√ßos s√£o constru√≠dos com base em algoritmos de aprendizado de m√°quina, vis√£o computacional, reconhecimento de fala, processamento de linguagem natural e outras tecnologias. Consequentemente, os servi√ßos cognitivos da Azure s√£o altamente escal√°veis e personaliz√°veis, permitindo que as empresas criem aplicativos sob medida para suas necessidades espec√≠ficas, como por exemplo para realizar tarefas como reconhecimento de voz, an√°lise de sentimentos, detec√ß√£o de imagens, tradu√ß√£o de idiomas e muito mais. Al√©m disso, os servi√ßos podem ser facilmente integrados a outras tecnologias da Microsoft, como o Microsoft Power BI, Microsoft Dynamics 365 e Microsoft Office 365.

A Google Cloud Natural Language √© uma ferramenta de processamento de linguagem natural (PLN) criada pela Google que permite aos usu√°rios extrair informa√ß√µes valiosas a partir de textos n√£o estruturado, incluindo e-mails, documentos, artigos e outras fontes de dados. Com a t√©cnica de aprendizado de m√°quina e processamento de linguagem natural, a Google Cloud Natural Language pode identificar entidades ou indiv√≠duos mencionados em um texto, como nomes de pessoas, locais, organiza√ß√µes e datas, al√©m de extrair sentimentos e emo√ß√µes expressos pelo autor. Al√©m disso, h√° possibilidade de agrega√ß√£o com o Google Cloud Natural Language, que permite as empresas analisarem grandes volumes de dados para obter insights √∫teis que podem ajudar a orientar decis√µes importantes de neg√≥cios.

O IBM Watson √© um sistema de PLN, desenvolvido pela IBM. A ferramenta utiliza da an√°lise de dados e aprendizado de m√°quina para processar informa√ß√µes e fornecer insights relevantes para seus usu√°rios. O mesmo √© utilizado em v√°rios setores, incluindo sa√∫de, finan√ßas, educa√ß√£o, manufatura.
O IBM Watson √© uma plataforma de computa√ß√£o cognitiva e intelig√™ncia artificial desenvolvida pela empresa americana IBM. Ele utiliza tecnologias avan√ßadas de processamento de linguagem natural, machine learning, an√°lise de dados e outras t√©cnicas de intelig√™ncia artificial para fornecer insights e solu√ß√µes para uma ampla variedade de aplica√ß√µes.

**Eliminar ‚Üí Assertividade, Escalabilidade, Robustez, confiabilidade, adaptabilidade, Assertividade, Tecnologia**

A a√ß√£o de eliminar, dentro da Matriz Oceano Azul, est√° relacionada √† enumera√ß√£o de fatores que podem ser retirados ou melhorados, ao analisar os atributos do neg√≥cio. Portanto, analisa-se a distribui√ß√£o da ferramentas, em temos de escalabilidade, assertividade, acessibilidade, robustez e outras se√ß√µes atribu√≠dos √† proposta do Chat-BTG, em compara√ß√£o √† outras solu√ß√µes que utilizam Processamento de Linguagem Natural. Dado que a projeto consiste no desenvolvimento de um MVP, atualmente, o chat-BTG √© uma iniciativa sem afunilamento em sua complexidade, por isso, sua escalabilidade √© um fator a ser pensado nos pr√≥ximos passos do projeto, principalmente com √™nfase na robustez e maior assertividade da tecnologia.
J√° os outros modelos, por j√° estarem no mercado e ter uma infraestrutura posicionada, tendo em vista que servi√ßos de cloud e outros ‚Äúweb services‚Äù fornecem agrega√ß√£o em tempo real, com dados estruturados e j√° disponibilizados no pr√≥prio servi√ßo, conseguem maior escalabilidade e confiabilidade, conforme as tecnologias e funcionalidades contidas em seus servi√ßos.

**Aumentar ‚Üí  Personaliza√ß√£o**

A personaliza√ß√£o, nessa an√°lise, √© um fator de destaque no mercado, em compara√ß√£o aos concorrentes. Destaca-se que o modelo de Processamento de Linguagem Natural √© desenvolvido conforme a base de dados fornecido pelo cliente, voltado aos coment√°rios feitos nos posts da conta BTG Pactual no Instagram.
Ressalta-se que a visualiza√ß√£o dos dados tamb√©m ser√° pensada afim de facilitar o processo de tomada das decis√µes nos times de marketing e produto, como vantagem competitiva do banco.

**Criar ‚Üí Custo do Setup**

Com a vis√£o de ampliar, agregar e integrar diversas ferramentas, pode-se citar alguns fatores determinantes na an√°lise financeira para o escopo do projeto. Neste sentido, √© importante ressaltar que o projeto ‚ÄúChat-BTG‚Äù ainda √© limitado ao escopo m√≠nimo de requisitos de uma rede social espec√≠fica, dentro de dados levantados a partir de uma base de dados pr√©-definida, sem atualiza√ß√£o em tempo real. Consequentemente, a n√≠veis de complexidade baixos, possu√≠mos um custo muito interessante com base no benchmark do mercado. Sendo uma solu√ß√£o nativa, o custo de setup da solu√ß√£o √© muito baixo, e outros custos de ‚Äúweb services‚Äù s√£o mitigados com o processo interno realizado.
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Matriz_Oceano_Azul_CHATBTG.png">

### ‚ö†Ô∏è Matriz de riscos

A Matriz de Riscos consiste em uma ferramenta de gest√£o de riscos e oportunidades, a fim de identificar, avaliar e priorizar os fatores associados ao projeto, por grau de impacto e probabilidade de ocorr√™ncia.
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Matriz%20de%20Risco.png">

Por tanto, entende-se ent√£o, que o desenvolvimento de um Plano de A√ß√£o √© de indubit√°vel import√¢ncia, dado que, o documento descreve as atividades que precisam ser realizadas para mitigar riscos e alcan√ßar objetivos estipulados. O Plano define o que ser√° feito, quem far√°, como, quando e com quais recursos. <br>
No que diz respeito aos riscos evidenciados na Matriz acima, ressalta-se o plano de a√ß√£o desenvolvido:

**Amea√ßa 01:** Thain√° - Identificar quais s√£o as expectativas do cliente e manter o foco nelas durante a produ√ß√£o do modelo, al√©m de revisar o modelo ap√≥s cada mudan√ßa, comparando-o com as expectativas definidas para o projeto.

**Amea√ßa 02:**  Vinicius - Identificar as causas da falha, e implementar medidas de solu√ß√£o monitorar os testes no tratamento dos dados. Revisar a metodologia de pr√©-processamento de dados.

**Amea√ßa 03:** Kathy e Lucas -  Verificar por quais poss√≠veis motivos existe um erro na coleta das palavras chaves, e indentificar prad√µes de poss√≠veis palavras que estejam causando esses erros, buscando informa√ß√µes em rela√ß√£o a campanha de marketing. Al√©m disso, caso o problema n√£o for na coleta das palavras, ajustariamos o algoritmo e procurariamos poss√≠veis falhas e tentariamos treinar o modelo  om mais dados e exemplos para melhorar a precis√£o, realizando testes regulares. 

**Amea√ßa 04:** Henri e Rodrigo -  Avaliar se a m√©trica de classifica√ß√£o dos coment√°rios √© suficientemente complexa para a correta classifica√ß√£o dos coment√°rios, conferir com testes se n√£o h√° algum caso que produza resultados incorretos/indesejados.

**Amea√ßa 05:** Jo√£o - Aplicar o 'word embedding' ap√≥s a realiza√ß√£o de testes, definindo as adapta√ß√µes necess√°rias, validando-as constantemente e utilizando diferentes frases para testar a identifica√ß√£o de contexto no modelo.

### üíä Canvas Proposta de Valor

O Canvas Proposta de Valor √© uma ferramenta de neg√≥cios que auxilia no entendimento e cria√ß√£o do posicionamento do projeto (como um produtos que ser√° desenvolvido), com base na cria√ß√£o de ganho que o cliente realmente valoriza e precisa. 

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Canva%20value%20proposition.png">

### üíµ‚ÄäAn√°lise Financeira

A an√°lise financeira corresponde √† uma avalia√ß√£o dos aspectos econ√¥micos e financeiros que permeiam o projeto, com o objetivo de estimar a viabilidade e escabilidade do mesmo.

Conforme o site de not√≠cias Reuters, o BTG Pactual (BTG Pactual S.A.). √© um banco de investimentos brasileiro especializado em capital de investimento e capital de risco. O BTG se configura como uma empresa de capital aberto com cerca de 200 s√≥cios (constitu√≠do por funcion√°rios internos). Mesmo sua sede sendo no Rio de Janeiro, Brasil, sua atua√ß√£o ocorre em escala global, alcan√ßando EUA, Chile, M√©xico, Reino Unido, Portugal, Argentina, Col√¥mbia e Peru. Al√©m das j√° citadas, o banco atua em √°reas como ‚ÄòCorporate Lending (Empr√©stimos e Financiamentos)‚Äô, Sales and Trading, Asset Management, Wealth Management e Ativos Florestais. Apesar da queda das receitas anuais totais (de 2022 para 2023) e tamb√©m das a√ß√µes nos √∫ltimos 6 meses, o banco BTG continua sendo uma das empresas mais relevantes e consolidadas do Brasil. 

Custos em rela√ß√£o ao projeto:

O custos estimados pelo cliente foram de R$250.000. N√£o foram informadas proje√ß√µes de receita pelo cliente (projeto interno).

Em paralelo, foi feito uma proje√ß√£o dos custos do projeto, contabilizando os gastos com funcion√°rios e servi√ßos de infraestrutura da AWS, tendo como m√©dia o valor de R$ 167.784.  


**Profissionais Necess√°rios Para o Desenvolvimento da Solu√ß√£o:**

- Engenheiro em Machine Learning
- Desenvolver de Software
- Cientista de Dados
Opcional (Incomum no Brasil):
- Especialista em lingu√≠stica Computacional

Sal√°rio M√©dio para Engenheiro em Machine Learning: R$ 8.900 /m√™s (Glassdoor) <br>
Sal√°rio M√©dio para Desenvolver Pleno: R$ 10.200 /m√™s (Glassdoor) <br>
Sal√°rio M√©dio para Cientista de Dados: R$ 8.710 /m√™s (Glassdoor)<br>
 
Gasto Mensal com os tr√™s profissionais: R$ 27.810<br>
Gasto em dois meses: R$ 55.620<br>
Gasto em tr√™s meses: R$ 83.430<br>

**Custos da infraestrutura em AWS:**

Sem o Amazon Comprehend: 

Custo Inicial - R$12.312,18<br>
Custo Mensal - R$34.341,58<br>

Gastos em dois meses: R$ 80.995,34<br>
Gastos em tr√™s meses: R$ 115.336,92<br>


Com o Amazon Comprehend: 

Custo Inicial - R$12.312,18 <br>
Custo Mensal - R$34.404,18 <br>

Gastos em dois meses: R$ 81.120,54 <br>
Gastos em tr√™s meses: R$ 115.524,72 <br>


**Contabiliza√ß√£o total:**

Considerando os custos dos funcion√°rios, somado ao da AWS, o valor do projeto pode variar entre R$136.615 e R$ 198.954, com uma m√©dia de R$ 167.784.  


## (Sprint 1) Entendimento da Experi√™ncia do Usu√°rio

### üë© Persona

A Persona √© a representa√ß√£o fict√≠cia do cliente ideal para o projeto, com o objetivo de compreender os seus comportamentos e necessidades. Nesse momento, entende-se que as personas configuram-se em representantes dos times de marketing, produto e automa√ß√µes.

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Persona%20-%20Juliano.png">
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Persona%20-%20Renata.png">
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Persona%20-%20Thiago.png">

### üìÑ User Stories

A User Stories s√£o representa√ß√µes simples e clara dos requisitos e funcionalidades de um software, escritas do ponto de vista do usu√°rio final. Essas hist√≥rias ajudam a manter o foco nas necessidades dos usu√°rios e a priorizar as funcionalidades mais importantes para o projeto. Portanto, a seguir, existem duas User Story por persona (Marketing, Produto e Automa√ß√µes)

![image](https://user-images.githubusercontent.com/99209230/235374746-f4fcbf5d-c13b-491c-a069-743cc575f91c.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374769-140f5bc8-12ba-470f-a625-c0979539f583.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374825-d97405fd-d990-4eaa-8755-552ff7552542.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374856-03169cd6-0cf1-469a-a458-776be352b7d7.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374896-b24aa25c-29b0-4b3b-aa57-20777dd3d50f.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374917-cb1c8680-9119-4e06-8d2d-769b34971709.png)


## (Sprint 2) Modelo Bag Of Words (IPYNB)

Link do Modelo Bag of Words: <a href="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/1_BagOfWords.ipynb">Jupyter Notebook do Modelo Bag of Word</a>

### Prepara√ß√£o dos dados 

A fase de prepara√ß√£o de dados come√ßou com a obten√ß√£o da base de dados com os coment√°rios de posts do Instagram do parceiro de projeto (@btgpactual). Depois que os dados foram coletados, foi feita a prepara√ß√£o do ambiente de desenvolvimento (Colab Notebook), na qual foi feita uma an√°lise explorat√≥ria dos dados, que inclui a verifica√ß√£o da qualidade dos dados, identifica√ß√£o de valores ausentes, duplicados, outliers, infer√™ncias e formula√ß√£o de hip√≥teses, entre outros, e posteriormente as etapas de pr√©-processamento.

###  Importa√ß√£o das bibliotecas:

No Pandas, as importa√ß√µes de bibliotecas s√£o usadas para trazer funcionalidades espec√≠ficas de bibliotecas externas para o seu c√≥digo. O Pandas √© uma biblioteca popular para an√°lise de dados em Python, mas para aproveitar ainda mais recursos, pode ser necess√°rio importar outras bibliotecas. As importa√ß√µes no Pandas geralmente s√£o feitas no in√≠cio do c√≥digo e s√£o usadas para importar m√≥dulos adicionais que fornecem funcionalidades extras.

Em primeira inst√¢ncia utilizamos as seguintes bibliiotecas:

pip install emoji: Este comando utiliza o gerenciador de pacotes pip para instalar a biblioteca emoji. A biblioteca emoji √© uma biblioteca Python que fornece funcionalidades para trabalhar com emojis, como a exibi√ß√£o, codifica√ß√£o e manipula√ß√£o de emojis em texto. Ao executar esse comando, voc√™ estar√° instalando a biblioteca emoji em seu ambiente Python.

pip install pyspellchecker: Este comando utiliza o gerenciador de pacotes pip para instalar a biblioteca pyspellchecker. A biblioteca pyspellchecker √© uma biblioteca Python que fornece corre√ß√£o ortogr√°fica em texto. Ela pode ser usada para verificar e corrigir erros ortogr√°ficos em palavras. Ao executar esse comando, voc√™ estar√° instalando a biblioteca pyspellchecker em seu ambiente Python.

Segue abaixo os c√≥digos:
- ```!pip install emoji```
- ```!pip install pyspellchecker```

### Compreens√£o dos Dados 
Foi implementado o m√©todo de carregamento do Dataframe utilizado. Sendo assim, foi criado o caminho da pasta no Google Drive e sua leitura usando "pd.read_csv".
Reorganizamos dessa forma, e renomeamos algumas colunas com o intuito de facilitar o processo de an√°lise.

Nesta Etapa ser√° explicado as colunas da base de dados, ‚ÄúBASE-DEPRECATED‚Äù, fornecida pela empresa BTG Pactual durante a Sprint 2. A base de dados possui 4549 linhas e foram utilizadas apenas 11 colunas para a realiza√ß√£o das an√°lises. Tais colunas ser√£o descritas abaixo:
  
- **Coluna 'id':** apresenta os √≠ndices das linhas da tabela. | Tipo = *integer64*
- **Coluna 'autor':** mostra a conta do autor do coment√°rio. | Tipo = *object*
- **Coluna 'texto':** o coment√°rio escrito pelo autor. | Tipo = *object*
- **Coluna 'tipoInteracao':** classifica√ß√£o de como foi feito o texto do coment√°rio(resposta, coment√°rio ou marca√ß√£o). | Tipo = *object*
- **Coluna 'tipoMidia':** categoria do tipo de post(reels, feed, image, carousel). | Tipo = *object*
- **Coluna 'anomalia':**  indica se o coment√°rio pode ser golpe ou spam. | Tipo = *float64*
- **Coluna 'probabilidadeAnomalia':** porcentagem de quanto o coment√°rio pode ser um golpe ou spam. | Tipo = *object*
- **Coluna 'processado':** indica se j√° houve uma an√°lise de sentimento daquele coment√°rio. | Tipo = *object*
- **Coluna 'contemHyperlink':** mostra se o coment√°rio tem algum link. | Tipo  = *float64*
- **Coluna 'dataPublicada':** retorna quando o coment√°rio foi publicado. | Tipo = *object*
- **Coluna 'URL':** direciona ao link em que est√° inserido o coment√°rio na rede social. | Tipo = *object*

### An√°lise Descritiva

A an√°lise descritiva √© uma etapa fundamental no acompanhamento e an√°lise de dados. √â uma t√©cnica que aplicada no contexto do nosso proejto em parceria com o BTG-Pactual, envolve a interpreta√ß√£o, comprreens√£o e organiza√ß√£o dos dados de forma a obterpadr√µes e tend√™ncias. Em nosso projeto, essa an√°lise ser√° feita com o intuito de realizar uma an√°lise de sentimentos dos usu√°rios em rela√ß√£o √†s campanhas do BTG, al√©m de facilitar o banco no processo de desenvolver futuras estrta√©gias e ompreender melhor como eles podem gerenciar um bom relacionamento com os clientes. Utilizaremos a an√°lise descritiva para identificar:

- **Coment√°rios por tipo de post (Reels, Foto, V√≠deo, Carrossel):**
    Dado que cada tipo de m√≠dia possui um objetivo diferente, entende-se que, conforme as suas diferencia√ß√µes, as palavras mais comentadas podem ser diferentes e podem agregar para o usu√°rio.

- **Palavras que mais aparecem nos coment√°rios:**
    Com a finalidade de entender quais palavras mais se repetem em todos os coment√°rios no perfil do BTG Pactual, desenvolve-se a an√°lise descritiva tendo a *wordcloud*, al√©m dos gr√°ficos de barra e dispers√£o, como representa√ß√µes visuais.

- **Conjunto de palavras com maior frequ√™ncia (n-grams = 3):**
    Visualizar a frequ√™ncia de poss√≠veis frases para identificar padr√µes em textos que podem ser associados a determinados sentimentos.

-  **Rela√ß√£o determin√≠stica entre as colunas Anomalia e Coment√°rio:**
    Verificar e a partir das hip√≥teses estruturadas, atribuir possibilidades de atua√ß√£o na coluna "anomalia"

- **Emojis na Base de Dados:**
    Entendimento de qual seria o melhor tratamento para os emojis, para que a an√°lise de sentimento seja mais precisa, com base nas apari√ß√µes no dataset.
   
### Pr√©-Processamento

O pr√© processamento √© uma etapa crucial na an√°lise de dados. Esse processo consiste no conjunto de tecnicas aplicados nos dados quando em desenvolvimento de modelos de aprendizado de m√°quina. No contexto do Processamento de Linguagem Natural (PLN), o pr√©-processamento refere-se no na t√©cnica de transformar e preparar os dados em uma forma mais adequada para a realiza√ß√£o de an√°lise de textos. 

Este processo √© crucial no momento de constru√ß√£o de uma an√°lise de dados, e nos modelos de machine learning e geralmente seguem as seguintes etapas: 
* Tokeniza√ß√£o: Processo de dividir um texto em pequenas unidades de texto chamadas de "token". 
* Remo√ß√£o de pontua√ß√µes: Elimina√ß√£o de caracteres de pontua√ß√£o: v√≠rculas, pontos, aspas, entre outros. 
* Convers√£o para min√∫scula: Padronizar as palavras. 
* Remo√ß√£o de stopwords: Remo√ß√£o das palavrad comuns e que n√£o costumam contribuir significativamente para o texto. 
* Stemming e Lematiza√ß√£o: T√©cnica de reduzir as palavras em seus radicais, ou formas mais b√°sicas. 

### Testando etapas do Pr√©-processamento
#### Estrutura√ß√£o do Pr√©-processamento

##### Fun√ß√£o: Retirando valores nulos
Descri√ß√£o: Essa fun√ß√£o remove linhas do DataFrame dados que possuem valores nulos nas colunas 'autor' e 'texto'. O resultado √© armazenado na vari√°vel df_textoAutor.
``` df_textoAutor = dados[['autor', 'texto']].dropna() ```

##### Fun√ß√£o: Retirando posts do btg
Descri√ß√£o: Essa fun√ß√£o remove do DataFrame dados todas as linhas em que o valor da coluna 'autor' √© igual a 'btgpactual'. O resultado √© armazenado na vari√°vel 
```df_textoAutor = dados.drop(dados[dados['autor'] == 'btgpactual'].index) ```

##### Fun√ß√£o: Shape
Descri√ß√£o: Essa fun√ß√£o retorna a dimens√£o do DataFrame df_textoAutor, ou seja, o n√∫mero de linhas e colunas. O resultado ser√° uma tupla com dois elementos, em que o primeiro elemento representa o n√∫mero de linhas e o segundo elemento representa o n√∫mero de colunas.
```df_textoAutor.shape```

#### Fun√ß√£o: Transformando uma frase em min√∫sculas
Descri√ß√£o: Essa fun√ß√£o extrai a frase localizada na linha 100 da coluna 'texto' do DataFrame dados. Em seguida, a fun√ß√£o lower() √© aplicada para converter todos os caracteres da frase em min√∫sculas. O resultado √© armazenado na vari√°vel sentence_teste. Essa transforma√ß√£o √© comumente utilizada para normalizar o texto, tornando-o uniforme e facilitando compara√ß√µes e an√°lises, independentemente das diferen√ßas de capitaliza√ß√£o.
```sentence_teste = dados['texto'].iloc[100].lower()```

### Tokeniza√ß√£o
A tokeniza√ß√£o √© uma etapa importante no pr√©-processamento de texto que envolve a divis√£o de uma sequ√™ncia de texto em unidades menores chamadas de tokens. Esses tokens podem ser palavras individuais, frases, s√≠mbolos ou outros elementos, dependendo do objetivo do processamento.
No contexto do pr√©-processamento de texto no Pandas, a tokeniza√ß√£o geralmente √© realizada em um DataFrame que cont√©m uma coluna de texto. Cada valor nessa coluna, que representa uma senten√ßa ou um documento, √© dividido em tokens individuais. Isso √© √∫til para v√°rias tarefas de processamento de texto, como contagem de palavras, an√°lise de sentimentos, classifica√ß√£o de texto e muito mais.
Existem diferentes abordagens de tokeniza√ß√£o dispon√≠veis, como tokeniza√ß√£o com base em espa√ßos em branco, tokeniza√ß√£o com base em pontua√ß√£o, tokeniza√ß√£o com base em express√µes regulares e tokeniza√ß√£o com base em modelos de linguagem pr√©-treinados. A escolha da t√©cnica de tokeniza√ß√£o depende da natureza dos dados e do objetivo espec√≠fico do processamento de texto que est√° sendo realizado.

#### Fun√ß√µes utilizadas
A fun√ß√£o lower().split() √© utilizada para realizar a tokeniza√ß√£o de uma frase ou sequ√™ncia de texto em Python.

O m√©todo lower() √© aplicado √† vari√°vel sentence_teste e converte todos os caracteres da sequ√™ncia de texto em letras min√∫sculas. Isso √© √∫til para normalizar o texto e garantir consist√™ncia ao realizar a tokeniza√ß√£o.

Em seguida, o m√©todo split() √© chamado para dividir a sequ√™ncia de texto em tokens individuais. Esse m√©todo divide a sequ√™ncia em espa√ßos em branco, resultando em uma lista de tokens.

A lista de tokens √© atribu√≠da √† vari√°vel tokens, que pode ser usada posteriormente para an√°lise de texto, processamento de linguagem natural ou outras tarefas relacionadas ao processamento de texto.

Essa fun√ß√£o √© simples e eficaz para realizar a tokeniza√ß√£o b√°sica de uma frase em Python, dividindo-a em palavras individuais com base nos espa√ßos em branco. No entanto, √© importante observar que essa fun√ß√£o n√£o trata outros tipos de pontua√ß√µes ou casos mais complexos de tokeniza√ß√£o, que podem exigir o uso de bibliotecas ou t√©cnicas mais avan√ßadas.
Segue o c√≥digo abaixo:
```tokens = sentence_teste.lower().split() ```

### Stop-Words
Stop words s√£o palavras comuns que geralmente s√£o removidas durante o pr√©-processamento de texto, pois s√£o consideradas pouco informativas para a an√°lise de texto. Essas palavras incluem artigos, conjun√ß√µes, preposi√ß√µes e outros termos frequentemente encontrados na linguagem, como "a", "o", "em", "de", "e", entre outros.

A remo√ß√£o de stop words √© uma etapa comum no pr√©-processamento de texto, pois ajuda a reduzir o ru√≠do e o tamanho do vocabul√°rio utilizado na an√°lise. Ao remover essas palavras, √© poss√≠vel focar em termos mais relevantes e significativos para a tarefa em quest√£o, como an√°lise de sentimentos, classifica√ß√£o de texto ou minera√ß√£o de t√≥picos.

No contexto do pandas, a remo√ß√£o de stop words geralmente envolve o uso de bibliotecas de processamento de linguagem natural, como NLTK (Natural Language Toolkit) ou spaCy. Essas bibliotecas possuem listas predefinidas de stop words em diferentes idiomas, que podem ser aplicadas aos dados textuais para remover essas palavras desnecess√°rias antes de prosseguir com a an√°lise ou modelagem de texto.

#### Fun√ß√µes utilizadas
A fun√ß√£o translate() √© utilizada para remover pontua√ß√µes de uma string. Nesse caso espec√≠fico, a fun√ß√£o ```str.maketrans('', '', string.punctuation)``` cria uma tabela de tradu√ß√£o que mapeia os caracteres de pontua√ß√£o para um valor vazio (''). Em seguida, a fun√ß√£o translate() aplica essa tabela de tradu√ß√£o √† string sentence_teste, removendo todas as pontua√ß√µes.

J√° a fun√ß√£o ```strip()``` √© utilizada para remover espa√ßos em branco (espa√ßos, tabula√ß√µes, quebras de linha) no in√≠cio e no final de uma string. Ela retorna a vers√£o da string sem os espa√ßos em branco.

Essas duas fun√ß√µes em sequ√™ncia t√™m o objetivo de remover pontua√ß√µes e espa√ßos em branco extras da string sentence_teste, deixando-a limpa e pronta para ser processada ou analisada posteriormente.
Segue o c√≥digo abaixo:
- ```sentence_teste = sentence_teste.translate(str.maketrans('', '', string.punctuation)) ```
- ```sentence_teste = sentence_teste.strip()```

Tamb√©m encontra-se nesse c√≥digo, a vari√°vel stop_words √© inicializada com um conjunto de palavras de parada (stop words) em portugu√™s, obtidas a partir do m√≥dulo nltk.corpus.stopwords. Essas palavras s√£o geralmente consideradas irrelevantes para a an√°lise de texto, como artigos, preposi√ß√µes e pronomes.

Em seguida, uma lista adicional chamada stop_words_add √© criada, contendo palavras adicionais que ser√£o inclu√≠das nas stop words. Essas palavras podem ser personalizadas de acordo com as necessidades do projeto.

O m√©todo update √© usado para adicionar as palavras da lista stop_words_add ao conjunto de stop words existente.

Em seguida, √© criada uma lista vazia chamada new_words. Em um loop, cada palavra em sentence_teste √© verificada se est√° presente no conjunto de stop words. Se a palavra n√£o for uma stop word, ela √© adicionada √† lista new_words.

Por fim, a vari√°vel sentence_teste √© atualizada, substituindo seu valor original pela concatena√ß√£o das palavras contidas em new_words, formando assim uma nova vers√£o da senten√ßa sem as stop words.

Segue o c√≥digo abaixo: 
```top_words = set(nltk.corpus.stopwords.words('portuguese'))```

```
stop_words_add = ['ola', 'ol√°', 'pra', 'para', 'bemvindo','benvindo', 'bem-vindo', 'bemvindos', 'aqui', 'vai', 'btgpactual']
stop_words.update(stop_words_add)
new_words = []
for word in sentence_teste:
    if word not in stop_words:
        new_words.append(word)
        sentence_teste = ''.join(new_words) 
```
        
### Testando corretor de palavras
Nesse c√≥digo, a biblioteca spellchecker √© importada, e em seguida, uma frase incorreta √© atribu√≠da √† vari√°vel frase_errada. A frase √© dividida em palavras individuais usando o m√©todo split() e armazenada na lista words.

A classe SpellChecker √© inicializada com o par√¢metro language='pt', indicando que o corretor ortogr√°fico ser√° utilizado para o idioma portugu√™s.

Por fim, um objeto spell do tipo SpellChecker √© criado e est√° pronto para ser usado para corre√ß√£o ortogr√°fica das palavras contidas na frase incorreta.

Segue o c√≥digo abaixo:
```from spellchecker import SpellChecker```
```frase_errada = 'As veses estol gostandu di vose```
```words = frase_errada.split()```
```spell = SpellChecker(language='pt')``` 

### Testando corretor de abrevia√ß√µes e deletar emojis

Nesse c√≥digo, a biblioteca enelvo √© importada e em seguida a classe Normaliser √© utilizada.

O objetivo desse c√≥digo √© realizar a normaliza√ß√£o de texto, que consiste em aplicar transforma√ß√µes espec√≠ficas para padronizar ou corrigir palavras em um texto.

Na primeira linha, a classe Normaliser √© importada da biblioteca enelvo.

Em seguida, uma mensagem √© atribu√≠da √† vari√°vel msg, que cont√©m o texto a ser corrigido ou normalizado.

Por fim, um objeto norm do tipo Normaliser √© criado, e √© utilizado o par√¢metro tokenizer='readable', indicando que o texto ser√° tokenizado de forma leg√≠vel, ou seja, separando-o em palavras individuais considerando a estrutura gramatical.

O objeto norm est√° pronto para ser utilizado para realizar as corre√ß√µes ou normaliza√ß√µes no texto contido na vari√°vel msg.
Segue o c√≥digo abaixo:
```from enelvo.normaliser import Normaliser```
```msg = 'hj vou usar meu cart√£o do banco btg, pq gosto mt deleüëä'```
```norm = Normaliser(tokenizer='readable')```

### Stemming
Em pr√©-processamento de texto, stemming √© uma t√©cnica utilizada para reduzir palavras √† sua forma b√°sica ou raiz, removendo sufixos e prefixos. O objetivo √© simplificar a an√°lise de texto, tratando diferentes varia√ß√µes da mesma palavra como uma √∫nica forma, o que pode facilitar a compara√ß√£o e agrupamento de palavras semelhantes.

Na pr√°tica, j√° no c√≥digo, a biblioteca nltk √© importada e a classe SnowballStemmer √© utilizada.

O objetivo desse c√≥digo √© realizar a t√©cnica de stemming, que consiste em reduzir palavras √† sua forma b√°sica (ou raiz) removendo sufixos e prefixos, para facilitar a an√°lise de texto.

Na primeira linha, a classe SnowballStemmer √© importada da biblioteca nltk e √© especificado o idioma 'portuguese' como par√¢metro para o construtor do stemmer, indicando que o stemming ser√° realizado para palavras em portugu√™s.

Em seguida, um loop for √© utilizado para iterar sobre cada palavra presente no texto sentence_teste, que provavelmente cont√©m v√°rias palavras tokenizadas.

Dentro do loop, a fun√ß√£o stem() do objeto stemmer √© chamada para cada palavra, retornando a raiz ou forma b√°sica da palavra.

A raiz de cada palavra √© impressa na sa√≠da utilizando a fun√ß√£o print(), representando o resultado do stemming para cada palavra no texto.

Assim, o c√≥digo realiza o processo de stemming para cada palavra no texto sentence_teste, retornando suas formas b√°sicas ou ra√≠zes.
Segue o c√≥digo abaixo:
```from nltk.stem.snowball import SnowballStemmer```
```stemmer = SnowballStemmer('portuguese')```
```for word in sentence_teste.split():```
    ```print(stemmer.stem(word))```
    
    
##### Pipeline 
Utiliza-se o pipeline com a finalidade de evidenciar as etapas utilizadas nesse processo, demonstando que os outputs de um procedimento torna-se input da sequente.

![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/pipeline.jpeg)
    
### Bag Of Words

O modelo Bag-of-Words √© uma abordagem comum no pr√©-processamento de texto usada para representar documentos de texto como vetores num√©ricos. √â uma t√©cnica simples e amplamente utilizada em tarefas de processamento de linguagem natural.

No contexto do Google Colab, o pr√©-processamento com o modelo Bag-of-Words envolve as seguintes etapas:

Tokeniza√ß√£o: O texto √© dividido em unidades menores chamadas "tokens". Geralmente, os tokens s√£o palavras individuais, mas tamb√©m podem ser caracteres, n-grams (sequ√™ncias de n tokens consecutivos) ou outras unidades, dependendo do caso de uso.

Constru√ß√£o do vocabul√°rio: O vocabul√°rio √© criado coletando todos os tokens √∫nicos presentes nos documentos de texto. Cada token √∫nico √© atribu√≠do a um √≠ndice √∫nico no vocabul√°rio.

Codifica√ß√£o dos documentos: Cada documento de texto √© codificado como um vetor num√©rico de acordo com o vocabul√°rio constru√≠do. O tamanho do vetor √© igual ao tamanho do vocabul√°rio. Cada posi√ß√£o no vetor representa uma palavra do vocabul√°rio, e o valor naquela posi√ß√£o indica a frequ√™ncia ou outra medida de import√¢ncia do termo no documento.

Matriz de documentos-termos: Todos os documentos s√£o representados em uma matriz, em que cada linha corresponde a um documento e cada coluna corresponde a um termo do vocabul√°rio. Os valores da matriz s√£o geralmente contagens de frequ√™ncia, mas tamb√©m podem ser pesos TF-IDF (term frequency-inverse document frequency) ou outros esquemas de pondera√ß√£o.

Essa representa√ß√£o baseada no modelo Bag-of-Words permite que os algoritmos de aprendizado de m√°quina trabalhem com dados de texto, que normalmente requerem entrada num√©rica. No Google Colab, voc√™ pode implementar essas etapas usando bibliotecas de processamento de texto, como NLTK (Natural Language Toolkit), e aplic√°-las aos seus dados de texto para prepar√°-los para tarefas de classifica√ß√£o, agrupamento ou outras an√°lises.

#### Fun√ß√µes utilizadas:
O c√≥digo fornecido realiza a vetoriza√ß√£o de texto usando o CountVectorizer da biblioteca scikit-learn. Vejamos o que cada linha faz:

- Importa√ß√£o das bibliotecas:
-Essas linhas importam as classes CountVectorizer e TfidfVectorizer da biblioteca sklearn.feature_extraction.text, necess√°rias para realizar a vetoriza√ß√£o de texto.
```from sklearn.feature_extraction.text import CountVectorizer```
```from sklearn.feature_extraction.text import TfidfVectorizer```

- Instancia√ß√£o do vetorizador:
-Aqui, um objeto CountVectorizer √© criado e atribu√≠do √† vari√°vel vectorizer. O CountVectorizer √© usado para converter o texto em uma matriz de contagens de palavras.
```vectorizer.fit(frases_pre)```

- Ajuste do vetorizador aos dados de entrada:
-Essa linha ajusta o vetorizador aos dados de entrada frases_pre. Ele analisa o texto fornecido, constr√≥i o vocabul√°rio e atribui um √≠ndice num√©rico √∫nico a cada palavra encontrada nas frases.
 ``` vectorizer.fit(frases_pre)```

- Exibi√ß√£o do vocabul√°rio ordenado:
-Essa linha imprime o vocabul√°rio ordenado alfabeticamente. O vocabul√°rio √© um dicion√°rio que mapeia as palavras encontradas nas frases para seus respectivos √≠ndices num√©ricos.
```print(sorted(vectorizer.vocabulary_)) ```
- Transforma√ß√£o dos dados em uma representa√ß√£o vetorial:
-Aqui, o m√©todo transform √© chamado para converter as frases pr√©-processadas frases_pre em uma matriz vetorial. Cada linha da matriz representa uma frase, e cada coluna representa uma palavra do vocabul√°rio. O valor em cada posi√ß√£o da matriz representa a contagem de ocorr√™ncias da palavra correspondente na frase.
```vector = vectorizer.transform(frases_pre)```

- Sumariza√ß√£o dos resultados:
-Essas linhas exibem a forma (shape) da matriz resultante, que indica o n√∫mero de frases e o tamanho do vocabul√°rio. Em seguida, √© impressa a representa√ß√£o em formato de array da matriz vetorizada, mostrando as contagens de palavras para cada frase.
```print(vector.shape)```
```print(vector.toarray())```

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/0cae7c4b-5c5b-43cf-a164-3917b19e4779)

## TFID
 O TfidVectorizer calcula o inverso das frequ√™ncias e codifica os vetores a fim de calcular a relev√¢ncia de cada termo nos documentos. Diferente do CountVectorizer, este algoritmo calcula 'word frequencies'. Isso impede que, por exemplo, artigos ou palavras n√£o muito significantes acabem sendo reconhecidos como muito relevantes apenas pelo grande n√∫mero de ocorr√™ncias na base de dados, uma vez que essa frequ√™ncia inversa leva mais em conta o contexto das palavras empregadas em cada frase.
 Segue o c√≥digo abaixo:
``` vectorizer = TfidfVectorizer()```
``` vectorizer.fit(frases_pre)```
```print(sorted(vectorizer.vocabulary_))```
```vector = vectorizer.transform([frases_pre[0]])```

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/7161d030-b594-4156-82ea-95336c3f50b7)

## Resultados Obtidos

### An√°lise Descritiva

Na An√°lise Descritiva dos dados, trabalhamos com a cria√ß√£o de tabelas para oganizarmos t√©cnicas de visualiza√ß√£o, para identificar informa√ß√µes importantes e relevantes das campanhas, e podermos obter insights valiosos dos dados analisados. 

#### - Coment√°rios por tipo de post (Reels, Foto, V√≠deo, Carrossel): 

O c√≥digo apresentado abaixo foi feito com o intuito de identificar as palavras mais comentadas pelos usu√°rios conforme o tipo de post. Foi identificado a grande quantidade de coment√°rios feitos pelo pr√≥prio BTG-Pactual, dessa forma optamos por remover os coment√°rios feitos pelo BTG para termos uma an√°lise apenas dos autores externos. 

O c√≥digo abaixo foi feito para retirarmos os coment√°rios feitos pelo BTG: 
```df_repete = df_repete.drop(df_repete[df_repete['autor'] == 'btgpactual'].index)```
```df_repete```

#### - Palavras que mais aparecem nos coment√°rios (sem stemming): 

Nessa etapa, passamos pelo processo de pr√©-processamento e fizemos a jun√ß√£o de textos em uma string para ent√£o calcular a frequ6encia de cada palavra como mostra o c√≥digo a seguir: 

```freq_dist = FreqDist(palavras)```
```print(freq_dist.most_common(100))```

Ap√≥s esse processo o c√≥digo abaixo foi criado, com o intuito de exibir em formato Word Cloud os resultados obtidos

```wordcloud = WordCloud(width=2000, height=1200, background_color='white').generate(' '.join(frases_pre)))```
```plt.imshow(wordcloud, interpolation='bilinear'))```
```plt.axis('off'))```
```plt.show())```

![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/wordcloudpalavras.png)

Al√©m disso, criamos a partir do c√≥digo abaixo uma visualiza√ß√£o gr√°fica da frequ√™ncia e dispers√£o das palavras
![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/freqpalavras.png)


#### - Conjunto de tr√™s palavras com maior frequ√™ncia:

Nessa etapa, a fim de ter maior arcabou√ßo de palavras frequentes nos coment√°rios, opta-se pela sele√ß√£o dos conjuntos de tr√™s palavras. O c√≥digo abaixo foi realizado e nos trouxe resultados para entendermos melhor quais os conjuntos de plaavras que aparecem com a maior frequ√™ncia. 

```for frase in frases_pre:```
    ```words = frase.split()```
    ```trigrams += list(ngrams(words, 3))```

```freq_tri = nltk.FreqDist(trigrams)```

```top = freq_tri.most_common(100)```
```top```

Depois disso, criamos uma tabela para represnetarmos qual a frequ√™ncia dos conjuntos. 
![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/freqtigramas.png)


#### - Uso de emoji na base de Dados:

o objetivo desta hip√≥tese foi entender quais s√£o emojis que mais aparecem no dataset e qual seria o melhor tratamento para os mesmos, com o intuito de que a an√°lise de sentimento seja mais precisa, com base nas apari√ß√µes no dataset. Dessa forma, utilizamos o c√≥digo abaixo para calcular a porcentagem de apari√ß√£o dos emojis nos coment√°rios

```emoji_dict = dict(Counter(c for c in texto if emoji.is_emoji(c)))```

```most_common_emojis = Counter(emoji_dict).most_common(15) ```

```total_emojis = sum(emoji_dict.values()) ```
```emoji_percentages = {k: v / total_emojis for k, v in most_common_emojis}```

```df = pd.DataFrame({'emoji': list(emoji_percentages.keys()), 'percentage': list(emoji_percentages.values())})```
```df = df.sort_values(by='percentage', ascending=False)```
```print(df)```

![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/emojidataframe.png)


###  Gr√°fico Word Cloud

O Gr√°fico de Nuvem de Palavras, conhecido tamb√©m como como Word Cloud, √© uma ferramenta de representa√ß√£o visual que trabalha com a *plotagem* das palavras mais frequentes em um conjunto de textos. Nesse contexto, foi desenvolvido com o intuito de mostrar as palavras mais recorrentes e utilizadas pelos usu√°rios nos coment√°rios das postagens. 

![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/wordcloud.png)

### Tokeniza√ß√£o:

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/cd243004-d587-4c77-8be5-4e53b45b750f)

Esta √© uma pequena amostra dos resultados do processo de tokeniza√ß√£o. A partir dele conseguimos ter alguns insights de para novos tratamentos de dados e futuras estapas de pr√©-processamento. A partir desse tokeniza√ß√£o √© que foi poss√≠vel progredir com os outros m√©todos do pr√©-processamento.

### Stop Words:

Remo√ß√£o de caracteres especiais

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/aa0376df-999c-4e07-af04-e66cbebd8546)

Remo√ß√£o de stop words

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/b5c7e5c3-1f48-41eb-8245-f3edd3946fdf)

Esse foi uns dos resultados que obtivemos ao aplicar o m√©todo de remo√ß√£o de stop words, consistindo em retirar das frases palavras descess√°rias que n√£o contribu√©m para a an√°lise de sentimentos.

### Stemming:

Stemming √© uma t√©cnica utilizada para reduzir palavras √† sua forma b√°sica ou raiz, removendo sufixos e prefixos.
 
![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/ccd9595b-7f7c-44bb-8692-03c7fb5aeba8)

O output retorna o texto sem caracteres especiais e sem espa√ßos. 

### Bag of Words:

O modelo Bag-of-Words √© uma abordagem comum no pr√©-processamento de texto usada para representar documentos de texto como vetores num√©ricos. √â uma t√©cnica simples e amplamente utilizada em tarefas de processamento de linguagem natural.

Count Vectorizer

O CountVectorizer tokeniza uma cole√ß√£o de textos e cria um voc√°bulo com as palavras encontradas. 

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/0cae7c4b-5c5b-43cf-a164-3917b19e4779)

TFID Vectorizer

O TfidVectorizer calcula o inverso das frequ√™ncias e codifica os vetores a fim de calcular a relev√¢ncia de cada termo nos documentos. 

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/7161d030-b594-4156-82ea-95336c3f50b7)

## Modelo utilizando Word2Vec (IPYNB)

<a href="https://https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/Notebook/2_Word2Vec.ipynb">Jupyter Notebook do Modelo Word2Vec</a>

## (Sprint 3) Documenta√ß√£o do Modelo utilizando Word2Vec

### Objetivo da Sprint
 
Pr√©-processamento para utiliza√ß√£o de Word2Vec (carregando vetores para cada palavras num modelo j√° treinado) e entrega do modelo word2vec em algortimos classificat√≥rios.

# Pr√©-processamento do Dataset
No pr√©-processamento do dataset, s√£o realizadas etapas de limpeza e prepara√ß√£o dos dados antes de serem usados em an√°lises ou modelos de machine learning. Isso envolve a aplica√ß√£o de t√©cnicas e bibliotecas para transformar os dados brutos em um formato adequado para an√°lise.

Neste caso espec√≠fico, foram utilizadas as seguintes bibliotecas adicionais para realizar o pr√©-processamento:

### Normaliser:
A biblioteca "Normaliser" √© uma ferramenta utilizada para normalizar e padronizar textos. Ela oferece recursos para lidar com a normaliza√ß√£o de caracteres, remo√ß√£o de caracteres especiais, convers√£o de letras mai√∫sculas para min√∫sculas, entre outros. Essa biblioteca pode ser √∫til para garantir que os textos do dataset estejam em um formato consistente e pronto para serem processados.

### SpaCy: 
A biblioteca "SpaCy" √© uma biblioteca de processamento de linguagem natural (NLP) em Python. Ela fornece uma variedade de recursos para realizar tarefas de processamento de texto, como tokeniza√ß√£o, lematiza√ß√£o, reconhecimento de entidades nomeadas, an√°lise de depend√™ncia, entre outros. O SpaCy √© amplamente utilizado em tarefas de NLP e pode ser aplicado no pr√©-processamento de dados para extrair informa√ß√µes relevantes e realizar an√°lises mais avan√ßadas.

Para utilizar essas bibliotecas, √© necess√°rio instal√°-las previamente no ambiente Python em que o c√≥digo est√° sendo executado. Voc√™ pode instalar as bibliotecas usando os seguintes comandos:

```!pip install Normaliser```
```!pip install spacy```
Esses comandos utilizam o gerenciador de pacotes pip para instalar as bibliotecas "Normaliser" e "SpaCy" em seu ambiente Python. Ap√≥s a instala√ß√£o, voc√™ pode importar essas bibliotecas em seu c√≥digo e utilizar suas funcionalidades para realizar as etapas necess√°rias de pr√©-processamento do dataset.

## Pipeline
No pr√©-processamento do dataset, s√£o realizadas etapas de limpeza e prepara√ß√£o dos dados antes de serem usados em an√°lises ou modelos de machine learning. Isso envolve a aplica√ß√£o de t√©cnicas e bibliotecas para transformar os dados brutos em um formato adequado para an√°lise.

Neste caso, o pr√©-processamento do dataset foi realizado utilizando um pipeline que incluiu as seguintes etapas:

Tokeniza√ß√£o: A tokeniza√ß√£o √© o processo de dividir o texto em unidades menores, chamadas de tokens. Os tokens podem ser palavras individuais, pontua√ß√µes, n√∫meros, ou outras unidades significativas. A biblioteca "SpaCy" foi utilizada para realizar a tokeniza√ß√£o dos textos, dividindo-os em tokens que podem ser processados individualmente.

Retirada de stop words: Stop words s√£o palavras comuns que geralmente n√£o contribuem muito para o significado do texto, como "a", "o", "para", etc. A remo√ß√£o de stop words √© uma etapa comum no pr√©-processamento de textos para reduzir o ru√≠do e melhorar a efici√™ncia da an√°lise. A biblioteca "SpaCy" foi utilizada para remover as stop words dos textos do dataset.

Corretor de palavras: Um corretor de palavras √© utilizado para identificar e corrigir erros ortogr√°ficos em palavras. A biblioteca "pyspellchecker" foi utilizada como corretor de palavras, verificando e corrigindo erros ortogr√°ficos nas palavras do texto.

Transcri√ß√£o de emojis: Emojis s√£o s√≠mbolos usados para expressar emo√ß√µes, sentimentos ou ideias em mensagens de texto. A biblioteca "emoji" foi utilizada para realizar a transcri√ß√£o dos emojis presentes nos textos, convertendo-os em uma representa√ß√£o textual adequada.

Corre√ß√£o de abrevia√ß√µes: Abrevia√ß√µes e formas reduzidas de palavras s√£o comuns em mensagens de texto, mas podem dificultar a compreens√£o e an√°lise dos textos. Foi utilizado um m√©todo para corrigir abrevia√ß√µes, substituindo-as por suas formas completas para facilitar a interpreta√ß√£o dos textos.

Stemming: O stemming √© um processo de redu√ß√£o de palavras √† sua forma base ou radical. Ele remove os sufixos das palavras, mantendo apenas o n√∫cleo significativo. O stemming pode ajudar a reduzir a dimensionalidade dos dados e agrupar palavras relacionadas. No pr√©-processamento, foi aplicado o stemming nas palavras do texto.

Lematiza√ß√£o: A lematiza√ß√£o √© um processo semelhante ao stemming, mas em vez de simplesmente remover sufixos, ele retorna a forma base da palavra com base em seu significado l√©xico. A lematiza√ß√£o considera a parte gramatical da palavra e busca o "lemma" correspondente. O objetivo √© obter uma representa√ß√£o mais precisa das palavras no texto. A biblioteca "SpaCy" foi utilizada para realizar a lematiza√ß√£o das palavras.

Ap√≥s a aplica√ß√£o dessas etapas de pr√©-processamento, o texto tratado estar√° pronto para an√°lises posteriores, como modelagem de t√≥picos, classifica√ß√£o de sentimentos, entre outros.

### Modelos de vetoriza√ß√£o:

Ap√≥s as etapas de pr√©-processamento mencionadas anteriormente, foram utilizados dois modelos de vetoriza√ß√£o para representar os textos de forma num√©rica. Esses modelos s√£o:

Bag of Words (Saco de Palavras): O modelo Bag of Words √© uma abordagem comum na vetoriza√ß√£o de textos. Ele cria uma representa√ß√£o num√©rica dos textos, considerando a frequ√™ncia de ocorr√™ncia de cada palavra em um documento ou em todo o corpus. Cada documento √© representado por um vetor em que cada elemento corresponde a uma palavra e seu valor √© a contagem de ocorr√™ncias dessa palavra no documento. Essa representa√ß√£o √© adequada para muitas tarefas de processamento de texto, como classifica√ß√£o de documentos e an√°lise de sentimentos.

Word2Vec: O Word2Vec √© um modelo de vetoriza√ß√£o baseado em redes neurais que captura as rela√ß√µes sem√¢nticas entre as palavras. Ele mapeia palavras para vetores densos de valores reais, de modo que palavras semanticamente similares fiquem pr√≥ximas no espa√ßo vetorial. Essa t√©cnica permite representar palavras como vetores cont√≠nuos e capturar rela√ß√µes de significado entre elas, como analogias e similaridades. O modelo Word2Vec √© amplamente utilizado para tarefas de processamento de linguagem natural, como an√°lise de sentimento, recomenda√ß√£o de palavras e tradu√ß√£o autom√°tica.

Ap√≥s a aplica√ß√£o desses modelos de vetoriza√ß√£o, o texto tratado √© representado em forma num√©rica, pronta para ser utilizada como entrada para algoritmos de aprendizado de m√°quina ou an√°lises estat√≠sticas. O output dessas etapas de vetoriza√ß√£o ser√° uma matriz num√©rica em que cada documento √© representado por um vetor de caracter√≠sticas correspondentes √†s palavras ou conceitos relevantes extra√≠dos do texto.

#### Bibliotecas para os modelos de vetoriza√ß√£o
Para o modelo acima, foram utilizadas as seguintes bibliotecas e t√©cnicas para realizar o pr√©-processamento do texto:

NLTK (Natural Language Toolkit): O NLTK √© uma biblioteca popular em Python para processamento de linguagem natural. Ele oferece uma ampla gama de recursos e ferramentas para tarefas como tokeniza√ß√£o, lematiza√ß√£o, stemming, an√°lise sint√°tica, entre outros. A biblioteca NLTK foi utilizada para realizar algumas etapas de pr√©-processamento, como tokeniza√ß√£o e stemming.

Gensim: O Gensim √© uma biblioteca de processamento de linguagem natural em Python que oferece ferramentas para modelagem de t√≥picos, indexa√ß√£o e similaridade de documentos. Neste contexto, o Gensim foi utilizado para aplicar o modelo Word2Vec mencionado anteriormente, que permite a vetoriza√ß√£o baseada em palavras.

CountVectorizer: O CountVectorizer √© uma classe da biblioteca scikit-learn em Python que permite a vetoriza√ß√£o de textos usando a abordagem Bag of Words. Ele transforma o texto em uma matriz num√©rica em que cada documento √© representado por um vetor que cont√©m a contagem de ocorr√™ncia de cada palavra. O CountVectorizer foi utilizado como um modelo de vetoriza√ß√£o baseado em frequ√™ncia de palavras.

TfidfVectorizer: O TfidfVectorizer tamb√©m √© uma classe da biblioteca scikit-learn em Python que realiza a vetoriza√ß√£o de textos usando a abordagem TF-IDF (Term Frequency-Inverse Document Frequency). Essa abordagem leva em considera√ß√£o a frequ√™ncia de uma palavra em um documento espec√≠fico e sua frequ√™ncia inversa em todo o corpus. Isso permite atribuir maior import√¢ncia a palavras menos frequentes e reduzir a import√¢ncia de palavras muito comuns. O TfidfVectorizer foi utilizado como um modelo de vetoriza√ß√£o baseado no esquema TF-IDF.

Ap√≥s a aplica√ß√£o dessas t√©cnicas e bibliotecas de pr√©-processamento, o texto tratado √© transformado em uma representa√ß√£o num√©rica adequada para an√°lise posterior, como classifica√ß√£o de documentos, agrupamento de t√≥picos ou outros modelos de aprendizado de m√°quina. O output dessas etapas de vetoriza√ß√£o ser√° uma matriz num√©rica em que cada documento √© representado por um vetor de caracter√≠sticas correspondentes √†s palavras relevantes extra√≠das do texto.



### Sobre o Modelo Word2Vec 

 O modelo Word2Vec √© uma t√©cnica de PLN que permite representar palavras como vetores num√©ricos em um espa√ßo de v√°rias dimens√µes. Este processo consiste em capturar rela√ß√µes entre as palavras com base nos seus contextos. Como resultado final, √© poss√≠vel ter uma representa√ß√£o matem√°tica da similaridade entre as palvras disponibilizas para o treinamento. 
 
 Ele, tamb√©m, permite capturar nuances e contextos que podem influenciar o sentimento de um texto, que s√£o dados pelo n√≠vel de similaridade e proximidade entre as palavras. Al√©m disso, o word2vec pode facilitar a extra√ß√£o de caracter√≠sticas relevantes para a classifica√ß√£o, reduzindo a dimensionalidade e a esparsidade dos dados textuais. Este fatores, s√£o determinates na justificativa da sua utiliza√ß√£o no projeto, t√£o como sua import√¢ncia.
 
 O modelo word2vec pode ser combinado com outros modelos de aprendizado de m√°quina com facilidade, para obter melhores resultados de classifica√ß√£o. O word2vec tamb√©m pode ser usado para gerar incorpora√ß√µes de frases ou documentos inteiros, usando t√©cnicas como m√©dia ou soma dos vetores das palavras (neste caso utilizamos a soma, com adi√ß√£o de uma coluna em um novo dataframe). Portanto, o word2vec √© um modelo muito √∫til para a constru√ß√£o e desenvolvimento de nossa an√°lise, pois permite representar as palavras de forma mais rica e eficiente, capturando aspectos sem√¢nticos e sint√°ticos que afetaram na classifica√ß√£o de determinado corpus.


<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/exemplo_word2vec.png" alt="representa√ß√£o word2vec" width="300" height="auto">

<em> Representa√ß√£o visual do modelo Word2Vec </em>
<br>
<br>

### Vantagens do Modelo Word2Vec 

Em compara√ß√£o ao modelo anterior do Bag of Words, o modelo Word2Vec possui v√°rias vantagens, a principal delas √© conseguir capturar o contexto e sem√¢ntica das palavras. Al√©m disso, o Word2Vec tamb√©m gera uma representa√ß√£o vetorial com uma dimensionalidade muito mais reduzida, ele consegue trabalhar de forma mais efetiva com palavras desconhecidas e √© capaz de fazer esses c√°lculos sem√¢nticos entre similaridade entre palavras.

### Arquitetura do Modelo Word2Vec: CBOW

O modelo Word2Vec possui duas arquiteturas principais: CBOW (Continuos Bag-Of-Words) e Skip-Gram. Nossa equipe optou por utilizar o modelo CBOW pois computacionalmente ele √© mais eficiente. Esse tipo de arquitetura recebe as palavras circundantes e tenta prever a palavra central. Ambos os modelos (CBOW e Skip-Gram) s√£o treinados para maximizar a probabilidade de previs√£o correta das palavras.

<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/modelo_cbow.png" alt="modelo CBOW" width="300" height="auto">

### Constru√ß√£o do Modelo Word2Vec

Para a constru√ß√£o do modelo Word2Vec, a equipe fez uma nova limpeza e pr√©-processamento de dados, s√≥ que agora, na segunda base disponibilizada:

  - Substitui√ß√£o de Emojis: nas frases, substituimos os emojis por palavras. Esse processo melhora e abrange a base de dados trabalhada
   
   <img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/emojis_funcao.png" alt="emoji" width="700" height="auto">
   <br>

  - Substitui√ß√£o de Abrevia√ß√µes: substitu√≠mos abrevia√ß√µes por suas formas originais
    
   <img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/func_abreviacoes.png" alt="abrevia√ßao" width="300" height="auto">
   <br>

  - Lematiza√ß√£o: √© o processo de transforma√ß√£o de palavras para sua forma base (deriva√ß√£o inversa). Para esse m√©todo de pr√©-processamento utilizamos a bibliotecas spaCy
  
   <img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/resultado_pipilina_lematizacao.png" alt="lematizacao" width="900" height="auto">
   <em>Tabela p√≥s pr√©-processamento</em>
   <br>
   <br>
   
  <em>Processos anteriores do √∫ltimo modelo, como remo√ß√£o de stop words e tokeniza√ß√£o, permanecem. O √∫nico processo retirado foi o de stemming, pois a lematiza√ß√£o o substitui.</em>
  
  <br>
  <br>
  
  - Modelo word2Vec e Caracter√≠sticas: j√° na constru√ß√£o do Modelo Word2Vec em si, configuramos seus par√¢metros da seguinte forma: 150 vetores de dimensioanalidade, 5 janelas de contexto, contagem m√≠nima de palavras para 1 e 4 threads para treinamento paralelo
  
  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/modelo_word2vec_persi.png" alt="wor2vec" width="900" height="auto">  
<em>Defini√ß√µes e contru√ß√£o do modelo Wor2Vec</em>
  <br>
  <br>

  
  - Vetoriza√ß√£o para Word2Vec: a vetoriza√ß√£o consiste em transformar dados textuais em representa√ß√µes num√©ricas. √â um processo crucial para a contru√ß√£o do modelo Wor2Vec, s√≥ assim ser√° poss√≠vel organizar a distribui√ß√£o das palavras em um plano. Todos o tokens s√£o vetorizados e suas somas em uma frase tamb√©m s√£o contabilizados.

  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/funcao_vetorizacao.png" alt="vetorizacao" width="900" height="auto">  
<em>Fun√ß√£o para o processo de Vetoriza√ß√£o</em>
  <br>
  <br>
  
  - Output e tabela p√≥s Word2Vec: ao final do processo de Word2Vec, configuramos a tabela que ser√° utilizada para algor√≠timos de aprendizado. Um dos passos √© tranforma√ß√£o da coluna de sentimentos para valores num√©ricos. O segundo passo √© exatamente a atribui√ß√£o dos tokens √†s 150 colunas definidas 

  <br> 
  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/map_sentimentos.png" alt="map_sentimentos" width="800" height="auto">  
<em>Mapeamento da Coluna Sentimentos</em>
  <br> 
  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/alocacao_150vetores.png" alt="150vetores" width="1000" height="auto">  
<em>fun√ß√£o para distribui√ß√£o dos tokens nos vetores</em>
  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/tabel_final_word2Vec.png" alt="tabela w2v" width="900" height="auto">  
<em>Tabela final p√≥s Word2Vec</em>
  <br> 

  <br>
  <br>
  
### Algor√≠timos de Aprendizado: 

 O objetivo no nosso projeto √© exatamente fazer a classifica√ß√£o de frases com o intuito de conferir o desempenho de campanhas de marketing, assim, apenas o modelo Word2Vec n√£o √© o suficente, pois mesmo organizando a similaridade de palavras, ele n√£o consegue fazer a classifica√ß√£o de sentimentos. A solu√ß√£o √© utilizar algor√≠timos de aprendizado supervizionado para fazer esse tipo de classifica√ß√£o. Neste sentido, nesta Sprint, testamos alguns algoritmos, mas optamos pela utiliza√ß√£o do Naive Bayes e do Catboost.


#### Naive Bayes

 O Naive Bayes foi o primeiro algor√≠timo testado pelo grupo, ele se baseia em uma teoria matem√°tica de probabilidades condicionais (teorema de Bayes). O algor√≠timo se detaca por sua efici√™ncia e simplicidade. A biblioteca utilizada para esse m√©todo foi o sklearn (GaussianNB). A principal inten√ß√£o do grupo, era de usar o algoritmo para realizar o c√°lculo da probabilidade condicional de cada palavra ou "n-grama" ocorrer em cada classe, com o intuito de estimar a probabilidade do texto pertencer a uma classifica√ß√£o de sentimento espec√≠fico.

<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/modelo_naive_bayes.png" alt="naive bayes" width="900" height="auto">  
<em>Constru√ß√£o do modelo Naive Bayes aplicado</em>
<br>
<br>

#### CatBoost

 O catboots √© outro algor√≠timo de classifica√ß√£o, se destacando principalemnte com dados com caracter√≠sticas categ√≥ricas e dados desbalanceados. Esse algor√≠timo se baseia em conhecimentos matem√°ticos de gradiente (gradient boosting). √â importante ressaltar que o Catboost √© muito usado na defini√ß√£o de caracter√≠sticas categ√≥ricas como palavras ou frases, sem a necessidade de codific√°-las numericamente, o que pode reduzir a complexidade e o tempo de processamento. Com base nestes fatores, e mediante o uso pr√©vio de alguns membros de nosso grupo, decidimos optar pela sua utiliza√ß√£o nesta Sprint.


<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/modelo_catboost.png" alt="modelo catboost" width="900" height="auto">  
<em>Constru√ß√£o do modelo CatBoost aplicado</em>
<br>
<br>

- Resultados dos Algor√≠timos de Apredizado Supervisionado:

Diante as etapas exemplificadas acima, dividimos nossos dados para realizar o treinamento e avalia√ß√£o do nosso modelo, com base na estrutura√ß√£o realizada. Sendo dividimos em duas se√ß√µes

- Dados de treino: separa√ß√£o com o intuito de fazer o modelo aprender as caracter√≠sticas e os padr√µes dos dados que permitem fazer previs√µes ou classifica√ß√µes. 

- Dados de teste: separa√ß√£o usada como m√©todo de verifica√ß√£o do modelo, com base nas previs√µes ou classifica√ß√µes corretas e precisas.

### Naive Bayes:

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/2858ee49-47b2-4a80-aa4c-19ab348ef506)

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/27f6dd26-3243-4a51-a9e3-31b614bdf577)

<em>Os resultados conferidos pleo Naive Bayes foram satisfat√≥rios mas n√£o ideais. Com 54% de acur√°rica de treinamento e 55% de acur√°cia total</em>


#### CatBoost:

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/3952b8c2-fca2-4af5-8ea1-027dd886e15c)

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/86c8da56-7164-44b6-ba7c-2613cdf7d996)

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/4bbca284-8b45-4c13-94aa-d7e32abcbae5)

<em>Os resultados conferidos pleo CatBoost foram satisfat√≥rios. Entretanto, apresenta um overfitting, j√° que existe 95% de acur√°cia de treinamento e 72% de acur√°cia total, tendo uma diferen√ßa grande entre as duas separa√ß√µes, portanto, sendo necess√°rio entender o motivo. Tamb√©m foi obtido resultados satisfat√≥rios na matriz de confus√£o </em>

#### Avalia√ß√£o e m√©tricas do Modelo

Para avaliar o desempenho do nosso modelo, precisamos definir algumas m√©tricas que nos permitam quantificar a sua capacidade de classificar corretamente os textos em categorias como positivo, negativo ou neutro. Essas m√©tricas devem levar em conta n√£o apenas a taxa de acerto do modelo, mas tamb√©m cada tipo de erro e o balanceamento das classes nos dados.

Nesta se√ß√£o, vamos apresentar as principais m√©tricas que usamos para avaliar o nosso modelo, explicar como elas s√£o calculadas e interpretadas, e mostrar os resultados obtidos com o nosso conjunto de teste. As m√©tricas que vamos usar s√£o:

- Acur√°cia: foi uma das m√©tricas que mais olhamos na defini√ß√£o e relev√¢ncia dos dados que foram passados. A acur√°cia, em termos mais objetivos, se refere a ‚Äútaxa de acerto‚Äù do modelo. Ela √© calculada dividindo o n√∫mero de previs√µes corretas pelo n√∫mero total de previs√µes. √â importante ressaltar que seu balanceamento foi feito de acordo com a base de dados que nos foi deferida.

- Precis√£o: pode ser definido como a propor√ß√£o de previs√µes positivas que s√£o realmente positivas. Ela mede a confiabilidade do modelo em prever a classe positiva.

- Revoca√ß√£o: √© a propor√ß√£o de positivos reais que s√£o corretamente previstos como positivos. Ela mede a sensibilidade do modelo em capturar a classe positiva, mas pode ignorar a quantidade de falsos positivos.

- F1-score: √© a m√©dia harm√¥nica entre precis√£o e revoca√ß√£o. Ela mede o equil√≠brio entre essas duas m√©tricas, dando mais peso aos valores baixos. Ela √© √∫til para comparar modelos que t√™m trade-offs entre precis√£o e revoca√ß√£o.

Para termos de refer√™ncia, recomendamos a an√°lise dos resultados obtidos com o nosso modelo usando estas m√©tricas como base.

Lembrando que diante da denfini√ß√£o e alinhamento com o professor, pudemos definir:

- **Verdadeiro Positivo:** referem-se aos coment√°rios negativos que s√£o classificados como negativos;

- **Falso Positivo:** referem-se aos coment√°rios positivos que s√£o classificados como negativos;

- **Falso Negativo:** referem-se aos coment√°rios negativos que s√£o classificados como positivos;

- **Verdadeiro Negativo:** referem-se aos coment√°rios positivos que s√£o classificados como positivos.

√â importante ressaltar que a partir de an√°lises feitas, foi poss√≠vel identificar as propor√ß√µes de "falso negativo" como as mais importantes, diante das predi√ß√µes que o modelo deve fazer, e com base na estrat√©gia de neg√≥cio do parceiro.

### Arquitetura da Solu√ß√£o

<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/Arquitetura%20de%20solu%C3%A7%C3%A3o.png" alt="Arquitetura de solu√ß√£o" width="5000">

1. Extra√ß√£o dos dados:
- Utilizando a API da rede social Instagram, √© poss√≠vel extrair dados de coment√°rios em postagens p√∫blicas. Esses dados s√£o retornados em formato JSON, que √© uma forma de representa√ß√£o de dados estruturados que facilita a an√°lise e manipula√ß√£o.

- Depois de extrair os dados da API do Instagram, eles s√£o processados e estruturados em uma fileira de dados, que pode ser um arquivo CSV, por exemplo. Isso facilita a manipula√ß√£o e an√°lise dos dados em etapas posteriores.

- Em seguida, os dados s√£o enviados para um sistema de mensageria, que pode ser um servi√ßo de mensagens instant√¢neas(RabbitMQ), por exemplo. Isso permite que os dados sejam acessados por usu√°rios autorizados em tempo real.

- Depois de serem recebidos pelo sistema de mensageria, os dados passam por um processo de processamento, no qual podem ser manipulados e transformados em uma estrutura mais adequada para an√°lise.

- Por fim, os dados s√£o pr√©-estruturados, o que significa que s√£o organizados em uma estrutura que facilita a consulta e an√°lise. Isso pode incluir a organiza√ß√£o dosdados em tabelas ou a atribui√ß√£o de tags ou categorias espec√≠ficas para cada dado. Com a pr√©-estrutura√ß√£o, √© poss√≠vel realizar consultas mais eficientes e respostas mais r√°pidas a perguntas espec√≠ficas sobre os dados.

2. An√°lise descritiva:

- Tipos de dados: identifica√ß√£o de quais s√£o os tipos de dados presentes no dataset. Por exemplo, os dados podem incluir informa√ß√µes sobre o autor do coment√°rio, a data e hora em que o coment√°rio foi feito, o tipo de post (Reels, Foto, V√≠deo, Carrossel) e o pr√≥prio coment√°rio. Cada um desses tipos de dados pode ser tratado de forma diferente na an√°lise.

- Colunas no dataset: listar todas as colunas no dataset e entender o que cada uma delas representa.

- Dados estat√≠sticos do dataset: √© poss√≠vel calcular estat√≠sticas b√°sicas dos dados, como m√©dia, mediana, desvio padr√£o e intervalos de confian√ßa. Usado para a entender a distribui√ß√£o dos dados e identificar poss√≠veis outliers.

- Coment√°rios por tipo de post: analise da distribui√ß√£o dos coment√°rios por tipo depost (Reels, Foto, V√≠deo, Carrossel). Usado para entender qual tipo de postagem gera mais engajamento e intera√ß√£o com os usu√°rios.

- Palavras que mais aparecem nos coment√°rios: √© poss√≠vel identificar as palavras mais frequentes nos coment√°rios. Isso pode ajudar a entender os principais temas e assuntos abordados pelos usu√°rios e poss√≠veis stop-words. Essa an√°lise pode ser feita utilizando t√©cnicas de processamento de linguagem natural, como a tokeniza√ß√£o e a contagem de frequ√™ncia de palavras.

- Conjunto de tr√™s palavras com maior frequ√™ncia (trigramas): al√©m das palavras individuais, √© poss√≠vel identificar os conjuntos de tr√™s palavras mais frequentes nos coment√°rios. Isso pode ajudar a entender quais as express√µes mais comuns utilizadas pelos usu√°rios.

- Rela√ß√£o determin√≠stica entre as colunas Anomalia e Coment√°rio: An√°lise se h√° alguma rela√ß√£o determin√≠stica entre a presen√ßa de anomalias em uma postagem e a natureza dos coment√°rios feitos pelos usu√°rios.

- Uso de emojis na base de dados: √© poss√≠vel identificar o uso de emojis nos coment√°rios e analisar quais s√£o os emojis mais frequentes. Isso pode ajudar a entender aemo√ß√£o e o sentimento dos usu√°rios em rela√ß√£o √†s postagens. Sendo realizado uma substitui√ß√£o por palavras de seus respectivos significados.


## (Sprint 4) Proposta de uma nova modelagem utilizando novas features (IPYNB)

Colocar o link do artefato (deve estar na pasta src do reposit√≥rio do projeto).

## (Sprint 4) Documenta√ß√£o da proposta de uma nova modelagem

Preencher conforme a descri√ß√£o do artefato na Adalove.

## (Sprint 5) Apresenta√ß√£o Final

Colocar o link do artefato (deve estar na pasta apresentacoes do reposit√≥rio do projeto).

## (Sprint 5) Deploy do melhor modelo

Colocar o link dos artefatos (devem estar nas pastas videos e src do reposit√≥rio do projeto).

## (Sprint 5) Documenta√ß√£o da Solu√ß√£o

Preencher conforme a descri√ß√£o do artefato na Adalove.
