# Documento Principal do Projeto

<!-- Descri√ß√£o sucinta do projeto, com descri√ß√£o do problema, do objetivo e da solu√ß√£o em linhas gerais. -->

Diante do que foi apresentado como objetivo, o principal problema ou necessidade de neg√≥cio identificado pelo Banco BTG Pactual √© compreender as opini√µes, sentimentos e necessidades dos clientes expressos nas redes sociais, em especial, para fins de prova de conceito, na plataforma Instagram. O banco reconhece o impacto das redes sociais em seus processos e entende que ouvir seus clientes √© fundamental para aprimorar seus produtos, servi√ßos e a percep√ß√£o geral da institui√ß√£o.

Dessa forma, o BTG Pactual identificou a necessidade de implementar algum tipo de servi√ßo que realizasse a classifica√ß√£o de acordo com o sentimento nos coment√°rios. Neste sentido, pretendemos entregar um modelo de an√°lise de sentimentos que fa√ßa a detec√ß√£o de palavras-chave nos coment√°rios das redes sociais, utilizando o Processamento de Linguagem Natural (PLN). 

Este projeto visa suprir a necessidade de entender o cliente, o que ele deseja e o que n√£o deseja em rela√ß√£o ao banco e suas campanhas. A an√°lise de sentimento permite ao banco compreender melhor o cliente, adaptando-se e oferecendo uma experi√™ncia cada vez mais satisfat√≥ria.

## (Sprint 1) Entendimento do Neg√≥cio

 ### üìà Matriz de avalia√ß√£o de valor Oceano Azul 

A Matriz Oceano Azul √© a ferramenta que ajuda as empresas √† identificar novas oportunidades de crescimento dentro do mercado e quais fatores √†s diferenciam da concorr√™ncia, criando valor para seus clientes.

Para esta an√°lise, foram setadas outras Intelig√™ncias Artificiais, baseadas em Processamento de Linguagem Natural (PLN), como **Open AI, Amazon Comprehend, Microsoft Azure Cognitive Services, Google Cloud Natural Language, IBM Watson.** 

A OpenAI √© uma plataforma de intelig√™ncia artificial que oferece uma ampla gama de ferramentas e recursos para empresas e indiv√≠duos. A plataforma utiliza modelos de linguagem natural avan√ßados, como o GPT-3, para criar solu√ß√µes personalizadas para problemas espec√≠ficos de neg√≥cios.
A Amazon Comprehend √© um servi√ßo de an√°lise de texto fornecido pela AWS que permite a identifica√ß√£o de insights e informa√ß√µes √∫teis a partir de dados textuais, em grande escala. A tecnologia usa t√©cnicas de aprendizado de m√°quina para extrair informa√ß√µes √∫teis a partir de dados textuais, como sentimentos, emo√ß√µes, t√≥picos, entidades, rela√ß√µes e muito mais. O servi√ßo √© capaz de analisar grandes quantidades de texto, em v√°rias l√≠nguas, e produzir informa√ß√µes significativas para apoiar a tomada de decis√µes de neg√≥cios, como por exemplo conjuntos de dados de texto, incluindo documentos, mensagens de texto, posts de redes sociais e outras formas de comunica√ß√£o escrita, para identificar tend√™ncias.

O Microsoft Azure Cognitive Services √© um conjunto de servi√ßos cognitivos com aloca√ß√£o em nuvem que permite que desenvolvedores agreguem recursos de intelig√™ncia artificial (IA) a seus aplicativos sem a necessidade de experi√™ncia em aprendizado de m√°quina ou an√°lise de dados. Esses servi√ßos s√£o constru√≠dos com base em algoritmos de aprendizado de m√°quina, vis√£o computacional, reconhecimento de fala, processamento de linguagem natural e outras tecnologias. Consequentemente, os servi√ßos cognitivos da Azure s√£o altamente escal√°veis e personaliz√°veis, permitindo que as empresas criem aplicativos sob medida para suas necessidades espec√≠ficas, como por exemplo para realizar tarefas como reconhecimento de voz, an√°lise de sentimentos, detec√ß√£o de imagens, tradu√ß√£o de idiomas e muito mais. Al√©m disso, os servi√ßos podem ser facilmente integrados a outras tecnologias da Microsoft, como o Microsoft Power BI, Microsoft Dynamics 365 e Microsoft Office 365.

A Google Cloud Natural Language √© uma ferramenta de processamento de linguagem natural (PLN) criada pela Google que permite aos usu√°rios extrair informa√ß√µes valiosas a partir de textos n√£o estruturado, incluindo e-mails, documentos, artigos e outras fontes de dados. Com a t√©cnica de aprendizado de m√°quina e processamento de linguagem natural, a Google Cloud Natural Language pode identificar entidades ou indiv√≠duos mencionados em um texto, como nomes de pessoas, locais, organiza√ß√µes e datas, al√©m de extrair sentimentos e emo√ß√µes expressos pelo autor. Al√©m disso, h√° possibilidade de agrega√ß√£o com o Google Cloud Natural Language, que permite as empresas analisarem grandes volumes de dados para obter insights √∫teis que podem ajudar a orientar decis√µes importantes de neg√≥cios.

O IBM Watson √© um sistema de PLN, desenvolvido pela IBM. A ferramenta utiliza da an√°lise de dados e aprendizado de m√°quina para processar informa√ß√µes e fornecer insights relevantes para seus usu√°rios. O mesmo √© utilizado em v√°rios setores, incluindo sa√∫de, finan√ßas, educa√ß√£o, manufatura.
O IBM Watson √© uma plataforma de computa√ß√£o cognitiva e intelig√™ncia artificial desenvolvida pela empresa americana IBM. Ele utiliza tecnologias avan√ßadas de processamento de linguagem natural, machine learning, an√°lise de dados e outras t√©cnicas de intelig√™ncia artificial para fornecer insights e solu√ß√µes para uma ampla variedade de aplica√ß√µes.

**Eliminar ‚Üí Assertividade, Escalabilidade, Robustez, confiabilidade, adaptabilidade, Assertividade, Tecnologia**

A a√ß√£o de eliminar, dentro da Matriz Oceano Azul, est√° relacionada √† enumera√ß√£o de fatores que podem ser retirados ou melhorados, ao analisar os atributos do neg√≥cio. Portanto, analisa-se a distribui√ß√£o da ferramentas, em temos de escalabilidade, assertividade, acessibilidade, robustez e outras se√ß√µes atribu√≠dos √† proposta do Chat-BTG, em compara√ß√£o √† outras solu√ß√µes que utilizam Processamento de Linguagem Natural. Dado que a projeto consiste no desenvolvimento de um MVP, atualmente, o chat-BTG √© uma iniciativa sem afunilamento em sua complexidade, por isso, sua escalabilidade √© um fator a ser pensado nos pr√≥ximos passos do projeto, principalmente com √™nfase na robustez e maior assertividade da tecnologia.
J√° os outros modelos, por j√° estarem no mercado e ter uma infraestrutura posicionada, tendo em vista que servi√ßos de cloud e outros ‚Äúweb services‚Äù fornecem agrega√ß√£o em tempo real, com dados estruturados e j√° disponibilizados no pr√≥prio servi√ßo, conseguem maior escalabilidade e confiabilidade, conforme as tecnologias e funcionalidades contidas em seus servi√ßos.

**Aumentar ‚Üí  Personaliza√ß√£o**

A personaliza√ß√£o, nessa an√°lise, √© um fator de destaque no mercado, em compara√ß√£o aos concorrentes. Destaca-se que o modelo de Processamento de Linguagem Natural √© desenvolvido conforme a base de dados fornecido pelo cliente, voltado aos coment√°rios feitos nos posts da conta BTG Pactual no Instagram.
Ressalta-se que a visualiza√ß√£o dos dados tamb√©m ser√° pensada afim de facilitar o processo de tomada das decis√µes nos times de marketing e produto, como vantagem competitiva do banco.

**Criar ‚Üí Custo do Setup**

Com a vis√£o de ampliar, agregar e integrar diversas ferramentas, pode-se citar alguns fatores determinantes na an√°lise financeira para o escopo do projeto. Neste sentido, √© importante ressaltar que o projeto ‚ÄúChat-BTG‚Äù ainda √© limitado ao escopo m√≠nimo de requisitos de uma rede social espec√≠fica, dentro de dados levantados a partir de uma base de dados pr√©-definida, sem atualiza√ß√£o em tempo real. Consequentemente, a n√≠veis de complexidade baixos, possu√≠mos um custo muito interessante com base no benchmark do mercado. Sendo uma solu√ß√£o nativa, o custo de setup da solu√ß√£o √© muito baixo, e outros custos de ‚Äúweb services‚Äù s√£o mitigados com o processo interno realizado.
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/Oceano_azul.png">

### ‚ö†Ô∏è Matriz de riscos

A Matriz de Riscos consiste em uma ferramenta de gest√£o de riscos e oportunidades, a fim de identificar, avaliar e priorizar os fatores associados ao projeto, por grau de impacto e probabilidade de ocorr√™ncia.
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Matriz%20de%20Risco.png">

Por tanto, entende-se ent√£o, que o desenvolvimento de um Plano de A√ß√£o √© de indubit√°vel import√¢ncia, dado que, o documento descreve as atividades que precisam ser realizadas para mitigar riscos e alcan√ßar objetivos estipulados. O Plano define o que ser√° feito, quem far√°, como, quando e com quais recursos. <br>
No que diz respeito aos riscos evidenciados na Matriz acima, ressalta-se o plano de a√ß√£o desenvolvido:

**Amea√ßa 01:** Thain√° - Identificar quais s√£o as expectativas do cliente e manter o foco nelas durante a produ√ß√£o do modelo, al√©m de revisar o modelo ap√≥s cada mudan√ßa, comparando-o com as expectativas definidas para o projeto.

**Amea√ßa 02:**  Vinicius - Identificar as causas da falha, e implementar medidas de solu√ß√£o monitorar os testes no tratamento dos dados. Revisar a metodologia de pr√©-processamento de dados.

**Amea√ßa 03:** Kathy e Lucas -  Verificar por quais poss√≠veis motivos existe um erro na coleta das palavras chaves, e indentificar prad√µes de poss√≠veis palavras que estejam causando esses erros, buscando informa√ß√µes em rela√ß√£o a campanha de marketing. Al√©m disso, caso o problema n√£o for na coleta das palavras, ajustariamos o algoritmo e procurariamos poss√≠veis falhas e tentariamos treinar o modelo  om mais dados e exemplos para melhorar a precis√£o, realizando testes regulares. 

**Amea√ßa 04:** Henri e Rodrigo -  Avaliar se a m√©trica de classifica√ß√£o dos coment√°rios √© suficientemente complexa para a correta classifica√ß√£o dos coment√°rios, conferir com testes se n√£o h√° algum caso que produza resultados incorretos/indesejados.

**Amea√ßa 05:** Jo√£o - Aplicar o 'word embedding' ap√≥s a realiza√ß√£o de testes, definindo as adapta√ß√µes necess√°rias, validando-as constantemente e utilizando diferentes frases para testar a identifica√ß√£o de contexto no modelo.

### üíä Canvas Proposta de Valor

O Canvas Proposta de Valor √© uma ferramenta de neg√≥cios que auxilia no entendimento e cria√ß√£o do posicionamento do projeto (como um produtos que ser√° desenvolvido), com base na cria√ß√£o de ganho que o cliente realmente valoriza e precisa. 

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/Proposta_Valor.png">

### üíµ‚ÄäAn√°lise Financeira

A an√°lise financeira corresponde √† uma avalia√ß√£o dos aspectos econ√¥micos e financeiros que permeiam o projeto, com o objetivo de estimar a viabilidade e escalabilidade do mesmo.

Conforme o site de not√≠cias Reuters, o BTG Pactual (BTG Pactual S.A.) √© um banco de investimentos brasileiro especializado em capital de investimento e capital de risco. O BTG se configura como uma empresa de capital aberto com cerca de 200 s√≥cios (constitu√≠do por funcion√°rios internos). Mesmo sua sede sendo no Rio de Janeiro, Brasil, sua atua√ß√£o ocorre em escala global, alcan√ßando EUA, Chile, M√©xico, Reino Unido, Portugal, Argentina, Col√¥mbia e Peru. Al√©m das j√° citadas, o banco atua em √°reas como ‚ÄòCorporate Lending (Empr√©stimos e Financiamentos)‚Äô, Sales and Trading, Asset Management, Wealth Management e Ativos Florestais. Apesar da queda das receitas anuais totais (de 2022 para 2023) e tamb√©m das a√ß√µes nos √∫ltimos 6 meses, o banco BTG continua sendo uma das empresas mais relevantes e consolidadas do Brasil.

Custos em rela√ß√£o ao projeto: Os custos estimados pelo cliente foram de R$250.000. N√£o foram informadas proje√ß√µes de receita pelo cliente (projeto interno).

Em paralelo, foi feito uma proje√ß√£o dos custos do projeto, contabilizando os gastos com funcion√°rios e servi√ßos de infraestrutura da AWS, tendo como m√©dia o valor de R$ 167.784.



**Profissionais Necess√°rios Para o Desenvolvimento da Solu√ß√£o:**

- Engenheiro em Machine Learning
- Desenvolver de Software
- Cientista de Dados
- Opcional (Incomum no Brasil): Especialista em lingu√≠stica Computacional

Sal√°rio M√©dio para Engenheiro em Machine Learning: R$ 8.900 /m√™s (Glassdoor) <br>
Sal√°rio M√©dio para Desenvolver Pleno: R$ 10.200 /m√™s (Glassdoor) <br>
Sal√°rio M√©dio para Cientista de Dados: R$ 8.710 /m√™s (Glassdoor)<br>
 
Gasto Mensal com os tr√™s profissionais: R$ 27.810<br>
Gasto em dois meses: R$ 55.620<br>
Gasto em tr√™s meses: R$ 83.430<br>

**Custos da infraestrutura em AWS:**
<br>
<br>

 Sem o Amazon Comprehend: 

- Custo Inicial - R$12.312,18<br>
- Custo Mensal - R$34.341,58<br>

- Gastos em dois meses: R$ 80.995,34<br>
- Gastos em tr√™s meses: R$ 115.336,92<br>

<br>
<br>
Com o Amazon Comprehend: 

- Custo Inicial - R$12.312,18 <br>
- Custo Mensal - R$34.404,18 <br>

- Gastos em dois meses: R$ 81.120,54 <br>
- Gastos em tr√™s meses: R$ 115.524,72 <br>

**Contabiliza√ß√£o total:**

Considerando os custos dos funcion√°rios, somado ao da AWS, o valor do projeto pode variar entre R$136.615 e R$ 198.954, com uma m√©dia de R$ 167.784.  


## (Sprint 1) Entendimento da Experi√™ncia do Usu√°rio

### üë© Persona

A Persona √© a representa√ß√£o fict√≠cia do cliente ideal para o projeto, com o objetivo de compreender os seus comportamentos e necessidades. Nesse momento, entende-se que as personas configuram-se em representantes dos times de marketing, produto e automa√ß√µes.

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/dev-jo%C3%A3o/assets/Imagens/Persona%20-%20Juliano.png" alt="persona juliano" width="600" height="auto" >
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/dev-jo%C3%A3o/assets/Imagens/Persona%20-%20Renata.png"  alt="persona renato" width="600" height="auto">
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/dev-jo%C3%A3o/assets/Imagens/Persona%20-%20Thiago.png"  alt="persona thiago" width="600" height="auto">

### üìÑ User Stories

A User Stories s√£o representa√ß√µes simples e clara dos requisitos e funcionalidades de um software, escritas do ponto de vista do usu√°rio final. Essas hist√≥rias ajudam a manter o foco nas necessidades dos usu√°rios e a priorizar as funcionalidades mais importantes para o projeto. Portanto, a seguir, existem duas User Story por persona (Marketing, Produto e Automa√ß√µes)

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/dev-jo%C3%A3o/assets/Imagens/user_storie_01.png" alt="user storie 1" width="400" height="auto">
<br>
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/dev-jo%C3%A3o/assets/Imagens/user_storie_02.png" alt="user storie 2" width="400" height="auto">
<br>
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/dev-jo%C3%A3o/assets/Imagens/user_storie_03.png" alt="user storie 3" width="400" height="auto">
<br>
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/dev-jo%C3%A3o/assets/Imagens/user_storie_04.png" alt="user storie 4" width="400" height="auto">
<br>
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/dev-jo%C3%A3o/assets/Imagens/user_storie_05.png" alt="user storie 5" width="400" height="auto">
<br>
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/dev-jo%C3%A3o/assets/Imagens/user_storie_06.png" alt="user storie 6" width="400" height="auto">


## (Sprint 2) Modelo Bag Of Words (IPYNB)

Link do Modelo Bag of Words: <a href="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/1_BagOfWords.ipynb">Jupyter Notebook do Modelo Bag of Word</a>

### Prepara√ß√£o dos dados 

A fase de prepara√ß√£o de dados come√ßou com a obten√ß√£o da base de dados com os coment√°rios de posts do Instagram do parceiro de projeto (@btgpactual). Depois que os dados foram coletados, foi feita a prepara√ß√£o do ambiente de desenvolvimento (Colab Notebook), na qual foi feita uma an√°lise explorat√≥ria dos dados, que inclui a verifica√ß√£o da qualidade dos dados, identifica√ß√£o de valores ausentes, duplicados, outliers, infer√™ncias e formula√ß√£o de hip√≥teses, entre outros, e posteriormente as etapas de pr√©-processamento.

###  Importa√ß√£o das bibliotecas:

No Pandas, as importa√ß√µes de bibliotecas s√£o usadas para trazer funcionalidades espec√≠ficas de bibliotecas externas para o seu c√≥digo. O Pandas √© uma biblioteca popular para an√°lise de dados em Python, mas para aproveitar ainda mais recursos, pode ser necess√°rio importar outras bibliotecas. As importa√ß√µes no Pandas geralmente s√£o feitas no in√≠cio do c√≥digo e s√£o usadas para importar m√≥dulos adicionais que fornecem funcionalidades extras.

Em primeira inst√¢ncia utilizamos as seguintes bibliiotecas:

pip install emoji: Este comando utiliza o gerenciador de pacotes pip para instalar a biblioteca emoji. A biblioteca emoji √© uma biblioteca Python que fornece funcionalidades para trabalhar com emojis, como a exibi√ß√£o, codifica√ß√£o e manipula√ß√£o de emojis em texto. Ao executar esse comando, voc√™ estar√° instalando a biblioteca emoji em seu ambiente Python.

pip install pyspellchecker: Este comando utiliza o gerenciador de pacotes pip para instalar a biblioteca pyspellchecker. A biblioteca pyspellchecker √© uma biblioteca Python que fornece corre√ß√£o ortogr√°fica em texto. Ela pode ser usada para verificar e corrigir erros ortogr√°ficos em palavras. Ao executar esse comando, voc√™ estar√° instalando a biblioteca pyspellchecker em seu ambiente Python.

Segue abaixo os c√≥digos:
- ```!pip install emoji```
- ```!pip install pyspellchecker```

### Compreens√£o dos Dados 
Foi implementado o m√©todo de carregamento do Dataframe utilizado. Sendo assim, foi criado o caminho da pasta no Google Drive e sua leitura usando "pd.read_csv".
Reorganizamos dessa forma, e renomeamos algumas colunas com o intuito de facilitar o processo de an√°lise.

Nesta Etapa ser√° explicado as colunas da base de dados, ‚ÄúBASE-DEPRECATED‚Äù, fornecida pela empresa BTG Pactual durante a Sprint 2. A base de dados possui 4549 linhas e foram utilizadas apenas 11 colunas para a realiza√ß√£o das an√°lises. Tais colunas ser√£o descritas abaixo:
  
- **Coluna 'id':** apresenta os √≠ndices das linhas da tabela. | Tipo = *integer64*
- **Coluna 'autor':** mostra a conta do autor do coment√°rio. | Tipo = *object*
- **Coluna 'texto':** o coment√°rio escrito pelo autor. | Tipo = *object*
- **Coluna 'tipoInteracao':** classifica√ß√£o de como foi feito o texto do coment√°rio(resposta, coment√°rio ou marca√ß√£o). | Tipo = *object*
- **Coluna 'tipoMidia':** categoria do tipo de post(reels, feed, image, carousel). | Tipo = *object*
- **Coluna 'anomalia':**  indica se o coment√°rio pode ser golpe ou spam. | Tipo = *float64*
- **Coluna 'probabilidadeAnomalia':** porcentagem de quanto o coment√°rio pode ser um golpe ou spam. | Tipo = *object*
- **Coluna 'processado':** indica se j√° houve uma an√°lise de sentimento daquele coment√°rio. | Tipo = *object*
- **Coluna 'contemHyperlink':** mostra se o coment√°rio tem algum link. | Tipo  = *float64*
- **Coluna 'dataPublicada':** retorna quando o coment√°rio foi publicado. | Tipo = *object*
- **Coluna 'URL':** direciona ao link em que est√° inserido o coment√°rio na rede social. | Tipo = *object*

### An√°lise Descritiva

A an√°lise descritiva √© uma etapa fundamental no acompanhamento e an√°lise de dados. √â uma t√©cnica que aplicada no contexto do nosso proejto em parceria com o BTG-Pactual, envolve a interpreta√ß√£o, comprreens√£o e organiza√ß√£o dos dados de forma a obterpadr√µes e tend√™ncias. Em nosso projeto, essa an√°lise ser√° feita com o intuito de realizar uma an√°lise de sentimentos dos usu√°rios em rela√ß√£o √†s campanhas do BTG, al√©m de facilitar o banco no processo de desenvolver futuras estrta√©gias e ompreender melhor como eles podem gerenciar um bom relacionamento com os clientes. Utilizaremos a an√°lise descritiva para identificar:

- **Coment√°rios por tipo de post (Reels, Foto, V√≠deo, Carrossel):**
    Dado que cada tipo de m√≠dia possui um objetivo diferente, entende-se que, conforme as suas diferencia√ß√µes, as palavras mais comentadas podem ser diferentes e podem agregar para o usu√°rio.

- **Palavras que mais aparecem nos coment√°rios:**
    Com a finalidade de entender quais palavras mais se repetem em todos os coment√°rios no perfil do BTG Pactual, desenvolve-se a an√°lise descritiva tendo a *wordcloud*, al√©m dos gr√°ficos de barra e dispers√£o, como representa√ß√µes visuais.

- **Conjunto de palavras com maior frequ√™ncia (n-grams = 3):**
    Visualizar a frequ√™ncia de poss√≠veis frases para identificar padr√µes em textos que podem ser associados a determinados sentimentos.

-  **Rela√ß√£o determin√≠stica entre as colunas Anomalia e Coment√°rio:**
    Verificar e a partir das hip√≥teses estruturadas, atribuir possibilidades de atua√ß√£o na coluna "anomalia"

- **Emojis na Base de Dados:**
    Entendimento de qual seria o melhor tratamento para os emojis, para que a an√°lise de sentimento seja mais precisa, com base nas apari√ß√µes no dataset.
   
### Pr√©-Processamento

O pr√© processamento √© uma etapa crucial na an√°lise de dados. Esse processo consiste no conjunto de tecnicas aplicados nos dados quando em desenvolvimento de modelos de aprendizado de m√°quina. No contexto do Processamento de Linguagem Natural (PLN), o pr√©-processamento refere-se no na t√©cnica de transformar e preparar os dados em uma forma mais adequada para a realiza√ß√£o de an√°lise de textos. 

Este processo √© crucial no momento de constru√ß√£o de uma an√°lise de dados, e nos modelos de machine learning e geralmente seguem as seguintes etapas: 
* Tokeniza√ß√£o: Processo de dividir um texto em pequenas unidades de texto chamadas de "token". 
* Remo√ß√£o de pontua√ß√µes: Elimina√ß√£o de caracteres de pontua√ß√£o: v√≠rculas, pontos, aspas, entre outros. 
* Convers√£o para min√∫scula: Padronizar as palavras. 
* Remo√ß√£o de stopwords: Remo√ß√£o das palavrad comuns e que n√£o costumam contribuir significativamente para o texto. 
* Stemming e Lematiza√ß√£o: T√©cnica de reduzir as palavras em seus radicais, ou formas mais b√°sicas. 

### Testando etapas do Pr√©-processamento
#### Estrutura√ß√£o do Pr√©-processamento

##### Fun√ß√£o: Retirando valores nulos
Descri√ß√£o: Essa fun√ß√£o remove linhas do DataFrame dados que possuem valores nulos nas colunas 'autor' e 'texto'. O resultado √© armazenado na vari√°vel df_textoAutor.
``` df_textoAutor = dados[['autor', 'texto']].dropna() ```

##### Fun√ß√£o: Retirando posts do btg
Descri√ß√£o: Essa fun√ß√£o remove do DataFrame dados todas as linhas em que o valor da coluna 'autor' √© igual a 'btgpactual'. O resultado √© armazenado na vari√°vel 
```df_textoAutor = dados.drop(dados[dados['autor'] == 'btgpactual'].index) ```

##### Fun√ß√£o: Shape
Descri√ß√£o: Essa fun√ß√£o retorna a dimens√£o do DataFrame df_textoAutor, ou seja, o n√∫mero de linhas e colunas. O resultado ser√° uma tupla com dois elementos, em que o primeiro elemento representa o n√∫mero de linhas e o segundo elemento representa o n√∫mero de colunas.
```df_textoAutor.shape```

#### Fun√ß√£o: Transformando uma frase em min√∫sculas
Descri√ß√£o: Essa fun√ß√£o extrai a frase localizada na linha 100 da coluna 'texto' do DataFrame dados. Em seguida, a fun√ß√£o lower() √© aplicada para converter todos os caracteres da frase em min√∫sculas. O resultado √© armazenado na vari√°vel sentence_teste. Essa transforma√ß√£o √© comumente utilizada para normalizar o texto, tornando-o uniforme e facilitando compara√ß√µes e an√°lises, independentemente das diferen√ßas de capitaliza√ß√£o.
```sentence_teste = dados['texto'].iloc[100].lower()```

### Tokeniza√ß√£o
A tokeniza√ß√£o √© uma etapa importante no pr√©-processamento de texto que envolve a divis√£o de uma sequ√™ncia de texto em unidades menores chamadas de tokens. Esses tokens podem ser palavras individuais, frases, s√≠mbolos ou outros elementos, dependendo do objetivo do processamento.
No contexto do pr√©-processamento de texto no Pandas, a tokeniza√ß√£o geralmente √© realizada em um DataFrame que cont√©m uma coluna de texto. Cada valor nessa coluna, que representa uma senten√ßa ou um documento, √© dividido em tokens individuais. Isso √© √∫til para v√°rias tarefas de processamento de texto, como contagem de palavras, an√°lise de sentimentos, classifica√ß√£o de texto e muito mais.
Existem diferentes abordagens de tokeniza√ß√£o dispon√≠veis, como tokeniza√ß√£o com base em espa√ßos em branco, tokeniza√ß√£o com base em pontua√ß√£o, tokeniza√ß√£o com base em express√µes regulares e tokeniza√ß√£o com base em modelos de linguagem pr√©-treinados. A escolha da t√©cnica de tokeniza√ß√£o depende da natureza dos dados e do objetivo espec√≠fico do processamento de texto que est√° sendo realizado.

#### Fun√ß√µes utilizadas
A fun√ß√£o lower().split() √© utilizada para realizar a tokeniza√ß√£o de uma frase ou sequ√™ncia de texto em Python.

O m√©todo lower() √© aplicado √† vari√°vel sentence_teste e converte todos os caracteres da sequ√™ncia de texto em letras min√∫sculas. Isso √© √∫til para normalizar o texto e garantir consist√™ncia ao realizar a tokeniza√ß√£o.

Em seguida, o m√©todo split() √© chamado para dividir a sequ√™ncia de texto em tokens individuais. Esse m√©todo divide a sequ√™ncia em espa√ßos em branco, resultando em uma lista de tokens.

A lista de tokens √© atribu√≠da √† vari√°vel tokens, que pode ser usada posteriormente para an√°lise de texto, processamento de linguagem natural ou outras tarefas relacionadas ao processamento de texto.

Essa fun√ß√£o √© simples e eficaz para realizar a tokeniza√ß√£o b√°sica de uma frase em Python, dividindo-a em palavras individuais com base nos espa√ßos em branco. No entanto, √© importante observar que essa fun√ß√£o n√£o trata outros tipos de pontua√ß√µes ou casos mais complexos de tokeniza√ß√£o, que podem exigir o uso de bibliotecas ou t√©cnicas mais avan√ßadas.
Segue o c√≥digo abaixo:
```tokens = sentence_teste.lower().split() ```

### Stop-Words
Stop words s√£o palavras comuns que geralmente s√£o removidas durante o pr√©-processamento de texto, pois s√£o consideradas pouco informativas para a an√°lise de texto. Essas palavras incluem artigos, conjun√ß√µes, preposi√ß√µes e outros termos frequentemente encontrados na linguagem, como "a", "o", "em", "de", "e", entre outros.

A remo√ß√£o de stop words √© uma etapa comum no pr√©-processamento de texto, pois ajuda a reduzir o ru√≠do e o tamanho do vocabul√°rio utilizado na an√°lise. Ao remover essas palavras, √© poss√≠vel focar em termos mais relevantes e significativos para a tarefa em quest√£o, como an√°lise de sentimentos, classifica√ß√£o de texto ou minera√ß√£o de t√≥picos.

No contexto do pandas, a remo√ß√£o de stop words geralmente envolve o uso de bibliotecas de processamento de linguagem natural, como NLTK (Natural Language Toolkit) ou spaCy. Essas bibliotecas possuem listas predefinidas de stop words em diferentes idiomas, que podem ser aplicadas aos dados textuais para remover essas palavras desnecess√°rias antes de prosseguir com a an√°lise ou modelagem de texto.

#### Fun√ß√µes utilizadas
A fun√ß√£o translate() √© utilizada para remover pontua√ß√µes de uma string. Nesse caso espec√≠fico, a fun√ß√£o ```str.maketrans('', '', string.punctuation)``` cria uma tabela de tradu√ß√£o que mapeia os caracteres de pontua√ß√£o para um valor vazio (''). Em seguida, a fun√ß√£o translate() aplica essa tabela de tradu√ß√£o √† string sentence_teste, removendo todas as pontua√ß√µes.

J√° a fun√ß√£o ```strip()``` √© utilizada para remover espa√ßos em branco (espa√ßos, tabula√ß√µes, quebras de linha) no in√≠cio e no final de uma string. Ela retorna a vers√£o da string sem os espa√ßos em branco.

Essas duas fun√ß√µes em sequ√™ncia t√™m o objetivo de remover pontua√ß√µes e espa√ßos em branco extras da string sentence_teste, deixando-a limpa e pronta para ser processada ou analisada posteriormente.
Segue o c√≥digo abaixo:
- ```sentence_teste = sentence_teste.translate(str.maketrans('', '', string.punctuation)) ```
- ```sentence_teste = sentence_teste.strip()```

Tamb√©m encontra-se nesse c√≥digo, a vari√°vel stop_words √© inicializada com um conjunto de palavras de parada (stop words) em portugu√™s, obtidas a partir do m√≥dulo nltk.corpus.stopwords. Essas palavras s√£o geralmente consideradas irrelevantes para a an√°lise de texto, como artigos, preposi√ß√µes e pronomes.

Em seguida, uma lista adicional chamada stop_words_add √© criada, contendo palavras adicionais que ser√£o inclu√≠das nas stop words. Essas palavras podem ser personalizadas de acordo com as necessidades do projeto.

O m√©todo update √© usado para adicionar as palavras da lista stop_words_add ao conjunto de stop words existente.

Em seguida, √© criada uma lista vazia chamada new_words. Em um loop, cada palavra em sentence_teste √© verificada se est√° presente no conjunto de stop words. Se a palavra n√£o for uma stop word, ela √© adicionada √† lista new_words.

Por fim, a vari√°vel sentence_teste √© atualizada, substituindo seu valor original pela concatena√ß√£o das palavras contidas em new_words, formando assim uma nova vers√£o da senten√ßa sem as stop words.

Segue o c√≥digo abaixo: 
```top_words = set(nltk.corpus.stopwords.words('portuguese'))```

```
stop_words_add = ['ola', 'ol√°', 'pra', 'para', 'bemvindo','benvindo', 'bem-vindo', 'bemvindos', 'aqui', 'vai', 'btgpactual']
stop_words.update(stop_words_add)
new_words = []
for word in sentence_teste:
    if word not in stop_words:
        new_words.append(word)
        sentence_teste = ''.join(new_words) 
```
        
### Testando corretor de palavras
Nesse c√≥digo, a biblioteca spellchecker √© importada, e em seguida, uma frase incorreta √© atribu√≠da √† vari√°vel frase_errada. A frase √© dividida em palavras individuais usando o m√©todo split() e armazenada na lista words.

A classe SpellChecker √© inicializada com o par√¢metro language='pt', indicando que o corretor ortogr√°fico ser√° utilizado para o idioma portugu√™s.

Por fim, um objeto spell do tipo SpellChecker √© criado e est√° pronto para ser usado para corre√ß√£o ortogr√°fica das palavras contidas na frase incorreta.

Segue o c√≥digo abaixo:
```from spellchecker import SpellChecker```
```frase_errada = 'As veses estol gostandu di vose```
```words = frase_errada.split()```
```spell = SpellChecker(language='pt')``` 

### Testando corretor de abrevia√ß√µes e deletar emojis

Nesse c√≥digo, a biblioteca enelvo √© importada e em seguida a classe Normaliser √© utilizada.

O objetivo desse c√≥digo √© realizar a normaliza√ß√£o de texto, que consiste em aplicar transforma√ß√µes espec√≠ficas para padronizar ou corrigir palavras em um texto.

Na primeira linha, a classe Normaliser √© importada da biblioteca enelvo.

Em seguida, uma mensagem √© atribu√≠da √† vari√°vel msg, que cont√©m o texto a ser corrigido ou normalizado.

Por fim, um objeto norm do tipo Normaliser √© criado, e √© utilizado o par√¢metro tokenizer='readable', indicando que o texto ser√° tokenizado de forma leg√≠vel, ou seja, separando-o em palavras individuais considerando a estrutura gramatical.

O objeto norm est√° pronto para ser utilizado para realizar as corre√ß√µes ou normaliza√ß√µes no texto contido na vari√°vel msg.
Segue o c√≥digo abaixo:
```from enelvo.normaliser import Normaliser```
```msg = 'hj vou usar meu cart√£o do banco btg, pq gosto mt deleüëä'```
```norm = Normaliser(tokenizer='readable')```

### Stemming
Em pr√©-processamento de texto, stemming √© uma t√©cnica utilizada para reduzir palavras √† sua forma b√°sica ou raiz, removendo sufixos e prefixos. O objetivo √© simplificar a an√°lise de texto, tratando diferentes varia√ß√µes da mesma palavra como uma √∫nica forma, o que pode facilitar a compara√ß√£o e agrupamento de palavras semelhantes.

Na pr√°tica, j√° no c√≥digo, a biblioteca nltk √© importada e a classe SnowballStemmer √© utilizada.

O objetivo desse c√≥digo √© realizar a t√©cnica de stemming, que consiste em reduzir palavras √† sua forma b√°sica (ou raiz) removendo sufixos e prefixos, para facilitar a an√°lise de texto.

Na primeira linha, a classe SnowballStemmer √© importada da biblioteca nltk e √© especificado o idioma 'portuguese' como par√¢metro para o construtor do stemmer, indicando que o stemming ser√° realizado para palavras em portugu√™s.

Em seguida, um loop for √© utilizado para iterar sobre cada palavra presente no texto sentence_teste, que provavelmente cont√©m v√°rias palavras tokenizadas.

Dentro do loop, a fun√ß√£o stem() do objeto stemmer √© chamada para cada palavra, retornando a raiz ou forma b√°sica da palavra.

A raiz de cada palavra √© impressa na sa√≠da utilizando a fun√ß√£o print(), representando o resultado do stemming para cada palavra no texto.

Assim, o c√≥digo realiza o processo de stemming para cada palavra no texto sentence_teste, retornando suas formas b√°sicas ou ra√≠zes.
Segue o c√≥digo abaixo:
```from nltk.stem.snowball import SnowballStemmer```
```stemmer = SnowballStemmer('portuguese')```
```for word in sentence_teste.split():```
    ```print(stemmer.stem(word))```
    
    
##### Pipeline 
Utiliza-se o pipeline com a finalidade de evidenciar as etapas utilizadas nesse processo, demonstando que os outputs de um procedimento torna-se input da sequente.

![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/pipeline.jpeg)
    
### Bag Of Words

O modelo Bag-of-Words √© uma abordagem comum no pr√©-processamento de texto usada para representar documentos de texto como vetores num√©ricos. √â uma t√©cnica simples e amplamente utilizada em tarefas de processamento de linguagem natural.

No contexto do Google Colab, o pr√©-processamento com o modelo Bag-of-Words envolve as seguintes etapas:

Tokeniza√ß√£o: O texto √© dividido em unidades menores chamadas "tokens". Geralmente, os tokens s√£o palavras individuais, mas tamb√©m podem ser caracteres, n-grams (sequ√™ncias de n tokens consecutivos) ou outras unidades, dependendo do caso de uso.

Constru√ß√£o do vocabul√°rio: O vocabul√°rio √© criado coletando todos os tokens √∫nicos presentes nos documentos de texto. Cada token √∫nico √© atribu√≠do a um √≠ndice √∫nico no vocabul√°rio.

Codifica√ß√£o dos documentos: Cada documento de texto √© codificado como um vetor num√©rico de acordo com o vocabul√°rio constru√≠do. O tamanho do vetor √© igual ao tamanho do vocabul√°rio. Cada posi√ß√£o no vetor representa uma palavra do vocabul√°rio, e o valor naquela posi√ß√£o indica a frequ√™ncia ou outra medida de import√¢ncia do termo no documento.

Matriz de documentos-termos: Todos os documentos s√£o representados em uma matriz, em que cada linha corresponde a um documento e cada coluna corresponde a um termo do vocabul√°rio. Os valores da matriz s√£o geralmente contagens de frequ√™ncia, mas tamb√©m podem ser pesos TF-IDF (term frequency-inverse document frequency) ou outros esquemas de pondera√ß√£o.

Essa representa√ß√£o baseada no modelo Bag-of-Words permite que os algoritmos de aprendizado de m√°quina trabalhem com dados de texto, que normalmente requerem entrada num√©rica. No Google Colab, voc√™ pode implementar essas etapas usando bibliotecas de processamento de texto, como NLTK (Natural Language Toolkit), e aplic√°-las aos seus dados de texto para prepar√°-los para tarefas de classifica√ß√£o, agrupamento ou outras an√°lises.

#### Fun√ß√µes utilizadas:
O c√≥digo fornecido realiza a vetoriza√ß√£o de texto usando o CountVectorizer da biblioteca scikit-learn. Vejamos o que cada linha faz:

- Importa√ß√£o das bibliotecas:
-Essas linhas importam as classes CountVectorizer e TfidfVectorizer da biblioteca sklearn.feature_extraction.text, necess√°rias para realizar a vetoriza√ß√£o de texto.
```from sklearn.feature_extraction.text import CountVectorizer```
```from sklearn.feature_extraction.text import TfidfVectorizer```

- Instancia√ß√£o do vetorizador:
-Aqui, um objeto CountVectorizer √© criado e atribu√≠do √† vari√°vel vectorizer. O CountVectorizer √© usado para converter o texto em uma matriz de contagens de palavras.
```vectorizer.fit(frases_pre)```

- Ajuste do vetorizador aos dados de entrada:
-Essa linha ajusta o vetorizador aos dados de entrada frases_pre. Ele analisa o texto fornecido, constr√≥i o vocabul√°rio e atribui um √≠ndice num√©rico √∫nico a cada palavra encontrada nas frases.
 ``` vectorizer.fit(frases_pre)```

- Exibi√ß√£o do vocabul√°rio ordenado:
-Essa linha imprime o vocabul√°rio ordenado alfabeticamente. O vocabul√°rio √© um dicion√°rio que mapeia as palavras encontradas nas frases para seus respectivos √≠ndices num√©ricos.
```print(sorted(vectorizer.vocabulary_)) ```
- Transforma√ß√£o dos dados em uma representa√ß√£o vetorial:
-Aqui, o m√©todo transform √© chamado para converter as frases pr√©-processadas frases_pre em uma matriz vetorial. Cada linha da matriz representa uma frase, e cada coluna representa uma palavra do vocabul√°rio. O valor em cada posi√ß√£o da matriz representa a contagem de ocorr√™ncias da palavra correspondente na frase.
```vector = vectorizer.transform(frases_pre)```

- Sumariza√ß√£o dos resultados:
-Essas linhas exibem a forma (shape) da matriz resultante, que indica o n√∫mero de frases e o tamanho do vocabul√°rio. Em seguida, √© impressa a representa√ß√£o em formato de array da matriz vetorizada, mostrando as contagens de palavras para cada frase.
```print(vector.shape)```
```print(vector.toarray())```

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/0cae7c4b-5c5b-43cf-a164-3917b19e4779)

## TFID
 O TfidVectorizer calcula o inverso das frequ√™ncias e codifica os vetores a fim de calcular a relev√¢ncia de cada termo nos documentos. Diferente do CountVectorizer, este algoritmo calcula 'word frequencies'. Isso impede que, por exemplo, artigos ou palavras n√£o muito significantes acabem sendo reconhecidos como muito relevantes apenas pelo grande n√∫mero de ocorr√™ncias na base de dados, uma vez que essa frequ√™ncia inversa leva mais em conta o contexto das palavras empregadas em cada frase.
 Segue o c√≥digo abaixo:
``` vectorizer = TfidfVectorizer()```
``` vectorizer.fit(frases_pre)```
```print(sorted(vectorizer.vocabulary_))```
```vector = vectorizer.transform([frases_pre[0]])```

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/7161d030-b594-4156-82ea-95336c3f50b7)

## Resultados Obtidos

### An√°lise Descritiva

Na An√°lise Descritiva dos dados, trabalhamos com a cria√ß√£o de tabelas para oganizarmos t√©cnicas de visualiza√ß√£o, para identificar informa√ß√µes importantes e relevantes das campanhas, e podermos obter insights valiosos dos dados analisados. 

#### - Coment√°rios por tipo de post (Reels, Foto, V√≠deo, Carrossel): 

O c√≥digo apresentado abaixo foi feito com o intuito de identificar as palavras mais comentadas pelos usu√°rios conforme o tipo de post. Foi identificado a grande quantidade de coment√°rios feitos pelo pr√≥prio BTG-Pactual, dessa forma optamos por remover os coment√°rios feitos pelo BTG para termos uma an√°lise apenas dos autores externos. 

O c√≥digo abaixo foi feito para retirarmos os coment√°rios feitos pelo BTG: 
```df_repete = df_repete.drop(df_repete[df_repete['autor'] == 'btgpactual'].index)```
```df_repete```

#### - Palavras que mais aparecem nos coment√°rios (sem stemming): 

Nessa etapa, passamos pelo processo de pr√©-processamento e fizemos a jun√ß√£o de textos em uma string para ent√£o calcular a frequ6encia de cada palavra como mostra o c√≥digo a seguir: 

```freq_dist = FreqDist(palavras)```
```print(freq_dist.most_common(100))```

Ap√≥s esse processo o c√≥digo abaixo foi criado, com o intuito de exibir em formato Word Cloud os resultados obtidos

```wordcloud = WordCloud(width=2000, height=1200, background_color='white').generate(' '.join(frases_pre)))```
```plt.imshow(wordcloud, interpolation='bilinear'))```
```plt.axis('off'))```
```plt.show())```

![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/wordcloudpalavras.png)

Al√©m disso, criamos a partir do c√≥digo abaixo uma visualiza√ß√£o gr√°fica da frequ√™ncia e dispers√£o das palavras
![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/freqpalavras.png)


#### - Conjunto de tr√™s palavras com maior frequ√™ncia:

Nessa etapa, a fim de ter maior arcabou√ßo de palavras frequentes nos coment√°rios, opta-se pela sele√ß√£o dos conjuntos de tr√™s palavras. O c√≥digo abaixo foi realizado e nos trouxe resultados para entendermos melhor quais os conjuntos de plaavras que aparecem com a maior frequ√™ncia. 

```for frase in frases_pre:```
    ```words = frase.split()```
    ```trigrams += list(ngrams(words, 3))```

```freq_tri = nltk.FreqDist(trigrams)```

```top = freq_tri.most_common(100)```
```top```

Depois disso, criamos uma tabela para represnetarmos qual a frequ√™ncia dos conjuntos. 
![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/freqtigramas.png)


#### - Uso de emoji na base de Dados:

o objetivo desta hip√≥tese foi entender quais s√£o emojis que mais aparecem no dataset e qual seria o melhor tratamento para os mesmos, com o intuito de que a an√°lise de sentimento seja mais precisa, com base nas apari√ß√µes no dataset. Dessa forma, utilizamos o c√≥digo abaixo para calcular a porcentagem de apari√ß√£o dos emojis nos coment√°rios

```emoji_dict = dict(Counter(c for c in texto if emoji.is_emoji(c)))```

```most_common_emojis = Counter(emoji_dict).most_common(15) ```

```total_emojis = sum(emoji_dict.values()) ```
```emoji_percentages = {k: v / total_emojis for k, v in most_common_emojis}```

```df = pd.DataFrame({'emoji': list(emoji_percentages.keys()), 'percentage': list(emoji_percentages.values())})```
```df = df.sort_values(by='percentage', ascending=False)```
```print(df)```

![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/emojidataframe.png)


###  Gr√°fico Word Cloud

O Gr√°fico de Nuvem de Palavras, conhecido tamb√©m como como Word Cloud, √© uma ferramenta de representa√ß√£o visual que trabalha com a *plotagem* das palavras mais frequentes em um conjunto de textos. Nesse contexto, foi desenvolvido com o intuito de mostrar as palavras mais recorrentes e utilizadas pelos usu√°rios nos coment√°rios das postagens. 

![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/wordcloud.png)

### Tokeniza√ß√£o:

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/cd243004-d587-4c77-8be5-4e53b45b750f)

Esta √© uma pequena amostra dos resultados do processo de tokeniza√ß√£o. A partir dele conseguimos ter alguns insights de para novos tratamentos de dados e futuras estapas de pr√©-processamento. A partir desse tokeniza√ß√£o √© que foi poss√≠vel progredir com os outros m√©todos do pr√©-processamento.

### Stop Words:

Remo√ß√£o de caracteres especiais

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/aa0376df-999c-4e07-af04-e66cbebd8546)

Remo√ß√£o de stop words

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/b5c7e5c3-1f48-41eb-8245-f3edd3946fdf)

Esse foi uns dos resultados que obtivemos ao aplicar o m√©todo de remo√ß√£o de stop words, consistindo em retirar das frases palavras descess√°rias que n√£o contribu√©m para a an√°lise de sentimentos.

### Stemming:

Stemming √© uma t√©cnica utilizada para reduzir palavras √† sua forma b√°sica ou raiz, removendo sufixos e prefixos.
 
![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/ccd9595b-7f7c-44bb-8692-03c7fb5aeba8)

O output retorna o texto sem caracteres especiais e sem espa√ßos. 

### Bag of Words:

O modelo Bag-of-Words √© uma abordagem comum no pr√©-processamento de texto usada para representar documentos de texto como vetores num√©ricos. √â uma t√©cnica simples e amplamente utilizada em tarefas de processamento de linguagem natural.

Count Vectorizer

O CountVectorizer tokeniza uma cole√ß√£o de textos e cria um voc√°bulo com as palavras encontradas. 

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/0cae7c4b-5c5b-43cf-a164-3917b19e4779)

TFID Vectorizer

O TfidVectorizer calcula o inverso das frequ√™ncias e codifica os vetores a fim de calcular a relev√¢ncia de cada termo nos documentos. 

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/7161d030-b594-4156-82ea-95336c3f50b7)

## Modelo utilizando Word2Vec (IPYNB)

<a href="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/Notebook/2_Word2Vec.ipynb">Jupyter Notebook do Modelo Word2Vec</a>

## (Sprint 3) Documenta√ß√£o do Modelo utilizando Word2Vec

### Objetivo da Sprint
 
Pr√©-processamento para utiliza√ß√£o de Word2Vec (carregando vetores para cada palavras num modelo j√° treinado) e entrega do modelo word2vec em algortimos classificat√≥rios.

### Sobre o Modelo Word2Vec 

 O modelo Word2Vec √© uma t√©cnica de PLN que permite representar palavras como vetores num√©ricos em um espa√ßo de v√°rias dimens√µes. Este processo consiste em capturar rela√ß√µes entre as palavras com base nos seus contextos. Como resultado final, √© poss√≠vel ter uma representa√ß√£o matem√°tica da similaridade entre as palvras disponibilizas para o treinamento. 
 
 Ele, tamb√©m, permite capturar nuances e contextos que podem influenciar o sentimento de um texto, que s√£o dados pelo n√≠vel de similaridade e proximidade entre as palavras. Al√©m disso, o word2vec pode facilitar a extra√ß√£o de caracter√≠sticas relevantes para a classifica√ß√£o, reduzindo a dimensionalidade e a esparsidade dos dados textuais. Este fatores, s√£o determinates na justificativa da sua utiliza√ß√£o no projeto, t√£o como sua import√¢ncia.
 
 O modelo word2vec pode ser combinado com outros modelos de aprendizado de m√°quina com facilidade, para obter melhores resultados de classifica√ß√£o. O word2vec tamb√©m pode ser usado para gerar incorpora√ß√µes de frases ou documentos inteiros, usando t√©cnicas como m√©dia ou soma dos vetores das palavras (neste caso utilizamos a soma, com adi√ß√£o de uma coluna em um novo dataframe). Portanto, o word2vec √© um modelo muito √∫til para a constru√ß√£o e desenvolvimento de nossa an√°lise, pois permite representar as palavras de forma mais rica e eficiente, capturando aspectos sem√¢nticos e sint√°ticos que afetaram na classifica√ß√£o de determinado corpus.


<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/exemplo_word2vec.png" alt="representa√ß√£o word2vec" width="300" height="auto">

<em> Representa√ß√£o visual do modelo Word2Vec </em>
<br>
<br>

### Vantagens do Modelo Word2Vec 

Em compara√ß√£o ao modelo anterior do Bag of Words, o modelo Word2Vec possui v√°rias vantagens, a principal delas √© conseguir capturar o contexto e sem√¢ntica das palavras. Al√©m disso, o Word2Vec tamb√©m gera uma representa√ß√£o vetorial com uma dimensionalidade muito mais reduzida, ele consegue trabalhar de forma mais efetiva com palavras desconhecidas e √© capaz de fazer esses c√°lculos sem√¢nticos entre similaridade entre palavras.

### Arquitetura do Modelo Word2Vec: CBOW

O modelo Word2Vec possui duas arquiteturas principais: CBOW (Continuos Bag-Of-Words) e Skip-Gram. Nossa equipe optou por utilizar o modelo CBOW pois computacionalmente ele √© mais eficiente. Esse tipo de arquitetura recebe as palavras circundantes e tenta prever a palavra central. Ambos os modelos (CBOW e Skip-Gram) s√£o treinados para maximizar a probabilidade de previs√£o correta das palavras.

<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/modelo_cbow.png" alt="modelo CBOW" width="300" height="auto">

### Constru√ß√£o do Modelo Word2Vec

Para a constru√ß√£o do modelo Word2Vec, a equipe fez uma nova limpeza e pr√©-processamento de dados, s√≥ que agora, na segunda base disponibilizada:

  - Substitui√ß√£o de Emojis: nas frases, substituimos os emojis por palavras. Esse processo melhora e abrange a base de dados trabalhada
   
   <img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/emojis_funcao.png" alt="emoji" width="700" height="auto">
   <br>

  - Substitui√ß√£o de Abrevia√ß√µes: substitu√≠mos abrevia√ß√µes por suas formas originais
    
   <img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/func_abreviacoes.png" alt="abrevia√ßao" width="300" height="auto">
   <br>

  - Lematiza√ß√£o: √© o processo de transforma√ß√£o de palavras para sua forma base (deriva√ß√£o inversa). Para esse m√©todo de pr√©-processamento utilizamos a bibliotecas spaCy
  
   <img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/resultado_pipilina_lematizacao.png" alt="lematizacao" width="900" height="auto">
   <em>Tabela p√≥s pr√©-processamento</em>
   <br>
   <br>
   
  <em>Processos anteriores do √∫ltimo modelo, como remo√ß√£o de stop words e tokeniza√ß√£o, permanecem. O √∫nico processo retirado foi o de stemming, pois a lematiza√ß√£o o substitui.</em>
  
  <br>
  <br>
  
  - Modelo word2Vec e Caracter√≠sticas: j√° na constru√ß√£o do Modelo Word2Vec em si, configuramos seus par√¢metros da seguinte forma: 150 vetores de dimensioanalidade, 5 janelas de contexto, contagem m√≠nima de palavras para 1 e 4 threads para treinamento paralelo
  
  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/modelo_word2vec_persi.png" alt="wor2vec" width="900" height="auto">  
<em>Defini√ß√µes e contru√ß√£o do modelo Wor2Vec</em>
  <br>
  <br>

  
  - Vetoriza√ß√£o para Word2Vec: a vetoriza√ß√£o consiste em transformar dados textuais em representa√ß√µes num√©ricas. √â um processo crucial para a contru√ß√£o do modelo Wor2Vec, s√≥ assim ser√° poss√≠vel organizar a distribui√ß√£o das palavras em um plano. Todos o tokens s√£o vetorizados e suas somas em uma frase tamb√©m s√£o contabilizados.

  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/funcao_vetorizacao.png" alt="vetorizacao" width="900" height="auto">  
<em>Fun√ß√£o para o processo de Vetoriza√ß√£o</em>
  <br>
  <br>
  
  - Output e tabela p√≥s Word2Vec: ao final do processo de Word2Vec, configuramos a tabela que ser√° utilizada para algor√≠timos de aprendizado. Um dos passos √© tranforma√ß√£o da coluna de sentimentos para valores num√©ricos. O segundo passo √© exatamente a atribui√ß√£o dos tokens √†s 150 colunas definidas.

  <br> 
  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/map_sentimentos.png" alt="map_sentimentos" width="800" height="auto">  
<em>Mapeamento da Coluna Sentimentos</em>
  <br> 
  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/alocacao_150vetores.png" alt="150vetores" width="1000" height="auto">  
<em>fun√ß√£o para distribui√ß√£o dos tokens nos vetores</em>
  <br> 
<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/tabel_final_word2Vec.png" alt="tabela w2v" width="900" height="auto">  
<em>Tabela final p√≥s Word2Vec</em>
  <br> 

  <br>
  <br>
  
### Algor√≠timos de Aprendizado: 

 O objetivo no nosso projeto √© fazer a classifica√ß√£o de frases com o intuito de conferir o desempenho de campanhas de marketing. Assim, apenas o modelo Word2Vec n√£o √© o suficente, pois mesmo organizando a similaridade de palavras, ele n√£o consegue fazer a classifica√ß√£o de sentimentos. A solu√ß√£o √© utilizar algor√≠timos de aprendizado supervizionado para fazer esse tipo de classifica√ß√£o. Neste sentido, testamos alguns algoritmos, mas optamos pela utiliza√ß√£o do Naive Bayes e do Catboost.


#### Naive Bayes

 O Naive Bayes foi o primeiro algor√≠timo testado pelo grupo, ele se baseia em uma teoria matem√°tica de probabilidades condicionais (teorema de Bayes). O algor√≠timo se detaca por sua efici√™ncia e simplicidade. A biblioteca utilizada para esse m√©todo foi o sklearn (GaussianNB). A principal inten√ß√£o do grupo, era de usar o algoritmo para realizar o c√°lculo da probabilidade condicional de cada palavra ou "n-grama" ocorrer em cada classe, com o intuito de estimar a probabilidade do texto pertencer a uma classifica√ß√£o de sentimento espec√≠fico.

<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/modelo_naive_bayes.png" alt="naive bayes" width="900" height="auto">  
<em>Constru√ß√£o do modelo Naive Bayes aplicado</em>
<br>
<br>

#### CatBoost

 O catboots √© outro algor√≠timo de classifica√ß√£o, se destacando principalemnte com dados com caracter√≠sticas categ√≥ricas e dados desbalanceados. Esse algor√≠timo se baseia em conhecimentos matem√°ticos de gradiente (gradient boosting). √â importante ressaltar que o Catboost √© muito usado na defini√ß√£o de caracter√≠sticas categ√≥ricas como palavras ou frases, sem a necessidade de codific√°-las numericamente, o que pode reduzir a complexidade e o tempo de processamento. Com base nestes fatores, e mediante o uso pr√©vio de alguns membros de nosso grupo, decidimos optar pela sua utiliza√ß√£o nesta Sprint.


<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/modelo_catboost.png" alt="modelo catboost" width="900" height="auto">  
<em>Constru√ß√£o do modelo CatBoost aplicado</em>
<br>
<br>

- Resultados dos Algor√≠timos de Apredizado Supervisionado:

Diante as etapas exemplificadas acima, dividimos nossos dados para realizar o treinamento e avalia√ß√£o do nosso modelo, com base na estrutura√ß√£o realizada. Sendo dividimos em duas se√ß√µes

- Dados de treino: separa√ß√£o com o intuito de fazer o modelo aprender as caracter√≠sticas e os padr√µes dos dados que permitem fazer previs√µes ou classifica√ß√µes. 

- Dados de teste: separa√ß√£o usada como m√©todo de verifica√ß√£o do modelo, com base nas previs√µes ou classifica√ß√µes corretas e precisas.

### Naive Bayes:

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/2858ee49-47b2-4a80-aa4c-19ab348ef506)

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/27f6dd26-3243-4a51-a9e3-31b614bdf577)

<em>Os resultados conferidos pleo Naive Bayes foram satisfat√≥rios mas n√£o ideais. Com 54% de acur√°rica de treinamento e 55% de acur√°cia total</em>


#### CatBoost:

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/3952b8c2-fca2-4af5-8ea1-027dd886e15c)

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/86c8da56-7164-44b6-ba7c-2613cdf7d996)

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/4bbca284-8b45-4c13-94aa-d7e32abcbae5)

<em>Os resultados conferidos pleo CatBoost foram satisfat√≥rios. Entretanto, apresenta um overfitting, j√° que existe 95% de acur√°cia de treinamento e 72% de acur√°cia total, tendo uma diferen√ßa grande entre as duas separa√ß√µes, portanto, sendo necess√°rio entender o motivo. Tamb√©m foi obtido resultados satisfat√≥rios na matriz de confus√£o </em>

#### Avalia√ß√£o e m√©tricas do Modelo

Para avaliar o desempenho do nosso modelo, precisamos definir algumas m√©tricas que nos permitam quantificar a sua capacidade de classificar corretamente os textos em categorias como positivo, negativo ou neutro. Essas m√©tricas devem levar em conta n√£o apenas a taxa de acerto do modelo, mas tamb√©m cada tipo de erro e o balanceamento das classes nos dados.

Nesta se√ß√£o, vamos apresentar as principais m√©tricas que usamos para avaliar o nosso modelo, explicar como elas s√£o calculadas e interpretadas, e mostrar os resultados obtidos com o nosso conjunto de teste. As m√©tricas que vamos usar s√£o:

- Acur√°cia: foi uma das m√©tricas que mais olhamos na defini√ß√£o e relev√¢ncia dos dados que foram passados. A acur√°cia, em termos mais objetivos, se refere a ‚Äútaxa de acerto‚Äù do modelo. Ela √© calculada dividindo o n√∫mero de previs√µes corretas pelo n√∫mero total de previs√µes. √â importante ressaltar que seu balanceamento foi feito de acordo com a base de dados que nos foi deferida.

- Precis√£o: pode ser definido como a propor√ß√£o de previs√µes positivas que s√£o realmente positivas. Ela mede a confiabilidade do modelo em prever a classe positiva.

- Revoca√ß√£o: √© a propor√ß√£o de positivos reais que s√£o corretamente previstos como positivos. Ela mede a sensibilidade do modelo em capturar a classe positiva, mas pode ignorar a quantidade de falsos positivos.

- F1-score: √© a m√©dia harm√¥nica entre precis√£o e revoca√ß√£o. Ela mede o equil√≠brio entre essas duas m√©tricas, dando mais peso aos valores baixos. Ela √© √∫til para comparar modelos que t√™m trade-offs entre precis√£o e revoca√ß√£o.

Para termos de refer√™ncia, recomendamos a an√°lise dos resultados obtidos com o nosso modelo usando estas m√©tricas como base.

Lembrando que diante da denfini√ß√£o e alinhamento com o professor, pudemos definir:

- **Verdadeiro Positivo:** referem-se aos coment√°rios negativos que s√£o classificados como negativos;

- **Falso Positivo:** referem-se aos coment√°rios positivos que s√£o classificados como negativos;

- **Falso Negativo:** referem-se aos coment√°rios negativos que s√£o classificados como positivos;

- **Verdadeiro Negativo:** referem-se aos coment√°rios positivos que s√£o classificados como positivos.

√â importante ressaltar que a partir de an√°lises feitas, foi poss√≠vel identificar as propor√ß√µes de "falso negativo" como as mais importantes, diante das predi√ß√µes que o modelo deve fazer, e com base na estrat√©gia de neg√≥cio do parceiro.

#### Compara√ß√£o entre BOW e Word2Vec

A acur√°cia do BOW com TF-IDF resultou em 75% e o recall resultou em 72%.

J√° os valores do Word2Vec ficaram como a seguir:

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/c56abbd0-19e7-4d12-ac9a-8b2dc90b9170)

Assim, conclui-se que a m√©trica na qual estamos focando no projeto, o recall, n√£o sofreu muita altera√ß√£o com o uso de um modelo diferente.

### Arquitetura da Solu√ß√£o

<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/Arquitetura%20de%20solu%C3%A7%C3%A3o.png" alt="Arquitetura de solu√ß√£o" width="5000">

1. Extra√ß√£o dos dados:

<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/Extra%C3%A7%C3%A3o.png" alt="Extra√ß√£o de dados" width="5000">

- Utilizando a API da rede social Instagram, √© poss√≠vel extrair dados de coment√°rios em postagens p√∫blicas. Esses dados s√£o retornados em formato JSON, que √© uma forma de representa√ß√£o de dados estruturados que facilita a an√°lise e manipula√ß√£o.

- Depois de extrair os dados da API do Instagram, eles s√£o processados e estruturados em uma fileira de dados, que pode ser um arquivo CSV, por exemplo. Isso facilita a manipula√ß√£o e an√°lise dos dados em etapas posteriores.

- Em seguida, os dados s√£o enviados para um sistema de mensageria, que pode ser um servi√ßo de mensagens instant√¢neas(RabbitMQ), por exemplo. Isso permite que os dados sejam acessados por usu√°rios autorizados em tempo real.

- Depois de serem recebidos pelo sistema de mensageria, os dados passam por um processo de processamento, no qual podem ser manipulados e transformados em uma estrutura mais adequada para an√°lise.

- Por fim, os dados s√£o pr√©-estruturados, o que significa que s√£o organizados em uma estrutura que facilita a consulta e an√°lise. Isso pode incluir a organiza√ß√£o dosdados em tabelas ou a atribui√ß√£o de tags ou categorias espec√≠ficas para cada dado. Com a pr√©-estrutura√ß√£o, √© poss√≠vel realizar consultas mais eficientes e respostas mais r√°pidas a perguntas espec√≠ficas sobre os dados.

2. An√°lise descritiva:

<img src="https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/descritiva.png" alt="An√°lise descritiva" width="5000">

- Tipos de dados: identifica√ß√£o de quais s√£o os tipos de dados presentes no dataset. Por exemplo, os dados podem incluir informa√ß√µes sobre o autor do coment√°rio, a data e hora em que o coment√°rio foi feito, o tipo de post (Reels, Foto, V√≠deo, Carrossel) e o pr√≥prio coment√°rio. Cada um desses tipos de dados pode ser tratado de forma diferente na an√°lise.

- Colunas no dataset: listar todas as colunas no dataset e entender o que cada uma delas representa.

- Dados estat√≠sticos do dataset: √© poss√≠vel calcular estat√≠sticas b√°sicas dos dados, como m√©dia, mediana, desvio padr√£o e intervalos de confian√ßa. Usado para a entender a distribui√ß√£o dos dados e identificar poss√≠veis outliers.

- Coment√°rios por tipo de post: analise da distribui√ß√£o dos coment√°rios por tipo depost (Reels, Foto, V√≠deo, Carrossel). Usado para entender qual tipo de postagem gera mais engajamento e intera√ß√£o com os usu√°rios.

- Palavras que mais aparecem nos coment√°rios: √© poss√≠vel identificar as palavras mais frequentes nos coment√°rios. Isso pode ajudar a entender os principais temas e assuntos abordados pelos usu√°rios e poss√≠veis stop-words. Essa an√°lise pode ser feita utilizando t√©cnicas de processamento de linguagem natural, como a tokeniza√ß√£o e a contagem de frequ√™ncia de palavras.

- Conjunto de tr√™s palavras com maior frequ√™ncia (trigramas): al√©m das palavras individuais, √© poss√≠vel identificar os conjuntos de tr√™s palavras mais frequentes nos coment√°rios. Isso pode ajudar a entender quais as express√µes mais comuns utilizadas pelos usu√°rios.

- Rela√ß√£o determin√≠stica entre as colunas Anomalia e Coment√°rio: An√°lise se h√° alguma rela√ß√£o determin√≠stica entre a presen√ßa de anomalias em uma postagem e a natureza dos coment√°rios feitos pelos usu√°rios.

- Uso de emojis na base de dados: √© poss√≠vel identificar o uso de emojis nos coment√°rios e analisar quais s√£o os emojis mais frequentes. Isso pode ajudar a entender aemo√ß√£o e o sentimento dos usu√°rios em rela√ß√£o √†s postagens. Sendo realizado uma substitui√ß√£o por palavras de seus respectivos significados.

3. Pr√©-processamento do Dataset

No pr√©-processamento do dataset, s√£o realizadas etapas de limpeza e prepara√ß√£o dos dados antes de serem usados em an√°lises ou modelos de machine learning. Isso envolve a aplica√ß√£o de t√©cnicas e bibliotecas para transformar os dados brutos em um formato adequado para an√°lise.

Neste caso espec√≠fico, foram utilizadas as seguintes bibliotecas adicionais para realizar o pr√©-processamento:

- Normaliser:
A biblioteca "Normaliser" √© uma ferramenta utilizada para normalizar e padronizar textos. Ela oferece recursos para lidar com a normaliza√ß√£o de caracteres, remo√ß√£o de caracteres especiais, convers√£o de letras mai√∫sculas para min√∫sculas, entre outros. Essa biblioteca pode ser √∫til para garantir que os textos do dataset estejam em um formato consistente e pronto para serem processados.

- SpaCy: 
A biblioteca "SpaCy" √© uma biblioteca de processamento de linguagem natural (NLP) em Python. Ela fornece uma variedade de recursos para realizar tarefas de processamento de texto, como tokeniza√ß√£o, lematiza√ß√£o, reconhecimento de entidades nomeadas, an√°lise de depend√™ncia, entre outros. O SpaCy √© amplamente utilizado em tarefas de NLP e pode ser aplicado no pr√©-processamento de dados para extrair informa√ß√µes relevantes e realizar an√°lises mais avan√ßadas.

Para utilizar essas bibliotecas, √© necess√°rio instal√°-las previamente no ambiente Python em que o c√≥digo est√° sendo executado. Voc√™ pode instalar as bibliotecas usando os seguintes comandos:

```!pip install Normaliser```
```!pip install spacy```
Esses comandos utilizam o gerenciador de pacotes pip para instalar as bibliotecas "Normaliser" e "SpaCy" em seu ambiente Python. Ap√≥s a instala√ß√£o, voc√™ pode importar essas bibliotecas em seu c√≥digo e utilizar suas funcionalidades para realizar as etapas necess√°rias de pr√©-processamento do dataset.

4. Pipeline
No pr√©-processamento do dataset, s√£o realizadas etapas de limpeza e prepara√ß√£o dos dados antes de serem usados em an√°lises ou modelos de machine learning. Isso envolve a aplica√ß√£o de t√©cnicas e bibliotecas para transformar os dados brutos em um formato adequado para an√°lise.

Neste caso, o pr√©-processamento do dataset foi realizado utilizando um pipeline que incluiu as seguintes etapas:

- Tokeniza√ß√£o: A tokeniza√ß√£o √© o processo de dividir o texto em unidades menores, chamadas de tokens. Os tokens podem ser palavras individuais, pontua√ß√µes, n√∫meros, ou outras unidades significativas. A biblioteca "SpaCy" foi utilizada para realizar a tokeniza√ß√£o dos textos, dividindo-os em tokens que podem ser processados individualmente.

- Retirada de stop words: Stop words s√£o palavras comuns que geralmente n√£o contribuem muito para o significado do texto, como "a", "o", "para", etc. A remo√ß√£o de stop words √© uma etapa comum no pr√©-processamento de textos para reduzir o ru√≠do e melhorar a efici√™ncia da an√°lise. A biblioteca "SpaCy" foi utilizada para remover as stop words dos textos do dataset.

- Corretor de palavras: Um corretor de palavras √© utilizado para identificar e corrigir erros ortogr√°ficos em palavras. A biblioteca "pyspellchecker" foi utilizada como corretor de palavras, verificando e corrigindo erros ortogr√°ficos nas palavras do texto.

- Transcri√ß√£o de emojis: Emojis s√£o s√≠mbolos usados para expressar emo√ß√µes, sentimentos ou ideias em mensagens de texto. A biblioteca "emoji" foi utilizada para realizar a transcri√ß√£o dos emojis presentes nos textos, convertendo-os em uma representa√ß√£o textual adequada.

- Corre√ß√£o de abrevia√ß√µes: Abrevia√ß√µes e formas reduzidas de palavras s√£o comuns em mensagens de texto, mas podem dificultar a compreens√£o e an√°lise dos textos. Foi utilizado um m√©todo para corrigir abrevia√ß√µes, substituindo-as por suas formas completas para facilitar a interpreta√ß√£o dos textos.

- Stemming: O stemming √© um processo de redu√ß√£o de palavras √† sua forma base ou radical. Ele remove os sufixos das palavras, mantendo apenas o n√∫cleo significativo. O stemming pode ajudar a reduzir a dimensionalidade dos dados e agrupar palavras relacionadas. No pr√©-processamento, foi aplicado o stemming nas palavras do texto.

- Lematiza√ß√£o: A lematiza√ß√£o √© um processo semelhante ao stemming, mas em vez de simplesmente remover sufixos, ele retorna a forma base da palavra com base em seu significado l√©xico. A lematiza√ß√£o considera a parte gramatical da palavra e busca o "lemma" correspondente. O objetivo √© obter uma representa√ß√£o mais precisa das palavras no texto. A biblioteca "SpaCy" foi utilizada para realizar a lematiza√ß√£o das palavras.

Ap√≥s a aplica√ß√£o dessas etapas de pr√©-processamento, o texto tratado estar√° pronto para an√°lises posteriores, como modelagem de t√≥picos, classifica√ß√£o de sentimentos, entre outros.

#### Modelos de vetoriza√ß√£o:

Ap√≥s as etapas de pr√©-processamento mencionadas anteriormente, foram utilizados dois modelos de vetoriza√ß√£o para representar os textos de forma num√©rica. Esses modelos s√£o:

- Bag of Words (Saco de Palavras): O modelo Bag of Words √© uma abordagem comum na vetoriza√ß√£o de textos. Ele cria uma representa√ß√£o num√©rica dos textos, considerando a frequ√™ncia de ocorr√™ncia de cada - palavra em um documento ou em todo o corpus. Cada documento √© representado por um vetor em que cada elemento corresponde a uma palavra e seu valor √© a contagem de ocorr√™ncias dessa palavra no documento. Essa representa√ß√£o √© adequada para muitas tarefas de processamento de texto, como classifica√ß√£o de documentos e an√°lise de sentimentos.

- Word2Vec: O Word2Vec √© um modelo de vetoriza√ß√£o baseado em redes neurais que captura as rela√ß√µes sem√¢nticas entre as palavras. Ele mapeia palavras para vetores densos de valores reais, de modo que palavras semanticamente similares fiquem pr√≥ximas no espa√ßo vetorial. Essa t√©cnica permite representar palavras como vetores cont√≠nuos e capturar rela√ß√µes de significado entre elas, como analogias e similaridades. O modelo Word2Vec √© amplamente utilizado para tarefas de processamento de linguagem natural, como an√°lise de sentimento, recomenda√ß√£o de palavras e tradu√ß√£o autom√°tica.

- Ap√≥s a aplica√ß√£o desses modelos de vetoriza√ß√£o, o texto tratado √© representado em forma num√©rica, pronta para ser utilizada como entrada para algoritmos de aprendizado de m√°quina ou an√°lises estat√≠sticas. O output dessas etapas de vetoriza√ß√£o ser√° uma matriz num√©rica em que cada documento √© representado por um vetor de caracter√≠sticas correspondentes √†s palavras ou conceitos relevantes extra√≠dos do texto.

#### Bibliotecas para os modelos de vetoriza√ß√£o
Para o modelo acima, foram utilizadas as seguintes bibliotecas e t√©cnicas para realizar o pr√©-processamento do texto:

- NLTK (Natural Language Toolkit): O NLTK √© uma biblioteca popular em Python para processamento de linguagem natural. Ele oferece uma ampla gama de recursos e ferramentas para tarefas como tokeniza√ß√£o, lematiza√ß√£o, stemming, an√°lise sint√°tica, entre outros. A biblioteca NLTK foi utilizada para realizar algumas etapas de pr√©-processamento, como tokeniza√ß√£o e stemming.

- Gensim: O Gensim √© uma biblioteca de processamento de linguagem natural em Python que oferece ferramentas para modelagem de t√≥picos, indexa√ß√£o e similaridade de documentos. Neste contexto, o Gensim foi utilizado para aplicar o modelo Word2Vec mencionado anteriormente, que permite a vetoriza√ß√£o baseada em palavras.

- CountVectorizer: O CountVectorizer √© uma classe da biblioteca scikit-learn em Python que permite a vetoriza√ß√£o de textos usando a abordagem Bag of Words. Ele transforma o texto em uma matriz num√©rica em que cada documento √© representado por um vetor que cont√©m a contagem de ocorr√™ncia de cada palavra. O CountVectorizer foi utilizado como um modelo de vetoriza√ß√£o baseado em frequ√™ncia de palavras.

- TfidfVectorizer: O TfidfVectorizer tamb√©m √© uma classe da biblioteca scikit-learn em Python que realiza a vetoriza√ß√£o de textos usando a abordagem TF-IDF (Term Frequency-Inverse Document Frequency). Essa abordagem leva em considera√ß√£o a frequ√™ncia de uma palavra em um documento espec√≠fico e sua frequ√™ncia inversa em todo o corpus. Isso permite atribuir maior import√¢ncia a palavras menos frequentes e reduzir a import√¢ncia de palavras muito comuns. O TfidfVectorizer foi utilizado como um modelo de vetoriza√ß√£o baseado no esquema TF-IDF.

- Ap√≥s a aplica√ß√£o dessas t√©cnicas e bibliotecas de pr√©-processamento, o texto tratado √© transformado em uma representa√ß√£o num√©rica adequada para an√°lise posterior, como classifica√ß√£o de documentos, agrupamento de t√≥picos ou outros modelos de aprendizado de m√°quina. O output dessas etapas de vetoriza√ß√£o ser√° uma matriz num√©rica em que cada documento √© representado por um vetor de caracter√≠sticas correspondentes √†s palavras relevantes extra√≠das do texto.


# Treinamento do Modelo
- O treinamento do modelo √© uma etapa fundamental no desenvolvimento de sistemas de aprendizado de m√°quina. Nessa fase, um algoritmo √© alimentado com um conjunto de dados de treinamento para aprender padr√µes e relacionamentos entre as vari√°veis de entrada e sa√≠da. O objetivo √© obter um modelo capaz de fazer previs√µes ou tomar decis√µes com base nos dados fornecidos.

- Durante o treinamento do modelo, os dados de entrada s√£o transformados em vetores num√©ricos por meio de t√©cnicas de vetoriza√ß√£o, como Bag of Words, Word2Vec, CountVectorizer ou TfidfVectorizer. Essa representa√ß√£o num√©rica permite que o modelo compreenda e processe os dados textuais de forma eficiente.

## Algoritmos:
### - CatBoost
- O CatBoost √© um algoritmo de aprendizado de m√°quina desenvolvido pela Yandex. Ele √© particularmente adequado para lidar com dados categ√≥ricos, como vari√°veis com valores discretos. O CatBoost utiliza a t√©cnica de "gradient boosting" para construir um modelo preditivo. Ele √© capaz de tratar automaticamente vari√°veis categ√≥ricas, evitando a necessidade de codifica√ß√£o manual dessas vari√°veis. Al√©m disso, o CatBoost inclui recursos como tratamento autom√°tico de valores ausentes e implementa√ß√µes eficientes para lidar com grandes conjuntos de dados.

### Naive Bayes
- O Naive Bayes √© um algoritmo de aprendizado de m√°quina probabil√≠stico baseado no teorema de Bayes. Ele √© amplamente utilizado em problemas de classifica√ß√£o de texto, como filtragem de spam, an√°lise de sentimento e categoriza√ß√£o de documentos. O Naive Bayes assume a independ√™ncia entre as caracter√≠sticas, o que simplifica o c√°lculo das probabilidades condicionais. Embora essa suposi√ß√£o simplificadora nem sempre seja verdadeira na pr√°tica, o Naive Bayes √© conhecido por sua simplicidade, efici√™ncia computacional e bom desempenho em muitos casos.

## M√©tricas de Avalia√ß√£o do Modelo
- Ao avaliar o desempenho de um modelo de aprendizado de m√°quina para tarefas de classifica√ß√£o, √© comum utilizar v√°rias m√©tricas para medir sua efic√°cia. Algumas m√©tricas comumente usadas incluem:

### Precis√£o: 
- A precis√£o √© a propor√ß√£o de exemplos classificados corretamente como positivos em rela√ß√£o a todos os exemplos classificados como positivos, ou seja, a capacidade do modelo de evitar classificar incorretamente exemplos negativos como positivos.

### F1-Score: 
- O F1-Score √© uma m√©trica que combina a precis√£o e o recall de um modelo. √â a m√©dia harm√¥nica dessas duas m√©tricas e fornece uma medida equilibrada do desempenho do modelo em termos de precis√£o e capacidade de recuperar exemplos positivos.

### Recall: 
- O recall, tamb√©m conhecido como taxa de verdadeiros positivos, √© a propor√ß√£o de exemplos positivos corretamente classificados em rela√ß√£o a todos os exemplos verdadeiramente positivos presentes no conjunto de dados. O recall mede a capacidade do modelo de identificar corretamente os exemplos positivos.

### Acur√°cia (m√©trica central):
- A acur√°cia √© a propor√ß√£o de exemplos classificados corretamente em rela√ß√£o a todos os exemplos do conjunto de dados. √â uma m√©trica amplamente utilizada como medida geral de desempenho de um modelo. Ela fornece uma vis√£o geral de qu√£o bem o modelo est√° fazendo em todas as classes, considerando tanto os verdadeiros positivos quanto os verdadeiros negativos.

- Essas m√©tricas s√£o utilizadas para avaliar diferentes aspectos do desempenho do modelo em tarefas de classifica√ß√£o. A precis√£o √© √∫til quando o foco est√° na minimiza√ß√£o de falsos positivos, enquanto o recall √© importante quando √© necess√°rio evitar falsos negativos. O F1-Score √© uma m√©trica balanceada que leva em considera√ß√£o tanto a precis√£o quanto o recall.

- Ao interpretar essas m√©tricas, √© importante considerar a natureza do problema e os requisitos espec√≠ficos do contexto em que o modelo est√° sendo aplicado. Uma alta precis√£o pode ser fundamental em algumas situa√ß√µes, enquanto um alto recall pode ser mais importante em outras.

- Al√©m dessas m√©tricas, existem outras que tamb√©m podem ser utilizadas, dependendo do contexto. Algumas delas incluem a √°rea sob a curva ROC (Receiver Operating Characteristic), a matriz de confus√£o, o √≠ndice de Gini e a taxa de erro.

- Ao avaliar o desempenho de um modelo, √© essencial considerar v√°rias m√©tricas e analisar seus resultados de forma hol√≠stica, levando em conta as necessidades e os objetivos espec√≠ficos do problema em quest√£o.

## Bibliotecas:
### - Sklearn
- O Sklearn, √© uma biblioteca amplamente utilizada em Python para aprendizado de m√°quina. Ela oferece uma variedade de algoritmos e ferramentas para tarefas como classifica√ß√£o, regress√£o, agrupamento e pr√©-processamento de dados. O sklearn possui uma interface consistente e intuitiva, o que facilita a constru√ß√£o e avalia√ß√£o de modelos de aprendizado de m√°quina. Al√©m disso, ele fornece fun√ß√µes para dividir conjuntos de dados em treinamento e teste, realizar valida√ß√£o cruzada, aplicar m√©tricas de avalia√ß√£o e muito mais.

### SpaCy
- O SpaCy √© uma biblioteca de processamento de linguagem natural (NLP) escrita em Python. Ela fornece uma ampla gama de recursos e ferramentas para realizar tarefas de processamento de texto, como tokeniza√ß√£o, lematiza√ß√£o, identifica√ß√£o de entidades nomeadas, an√°lise sint√°tica, entre outros. O SpaCy √© conhecido por sua efici√™ncia e desempenho, sendo capaz de processar grandes volumes de texto de forma r√°pida. Ele tamb√©m possui modelos pr√©-treinados para v√°rias l√≠nguas, que podem ser utilizados para tarefas de an√°lise de texto sem a necessidade de treinamento adicional. O SpaCy √© uma escolha popular para desenvolvedores e pesquisadores que trabalham com NLP devido √† sua facilidade de uso e efic√°cia em diversas aplica√ß√µes.

# Servi√ßo - API
- Um servi√ßo de API (Interface de Programa√ß√£o de Aplicativos) permite que os modelos de aprendizado de m√°quina sejam implantados e acessados de forma f√°cil e conveniente por meio de uma interface de programa√ß√£o. Ele oferece a capacidade de expor o modelo treinado como um servi√ßo online, permitindo que os usu√°rios fa√ßam solicita√ß√µes e recebam previs√µes ou resultados do modelo em tempo real.

- Ao fornecer um servi√ßo de API, √© poss√≠vel integrar o modelo de aprendizado de m√°quina em diferentes aplicativos, plataformas ou sistemas, permitindo sua utiliza√ß√£o em diversos contextos. Isso facilita a implementa√ß√£o e o consumo do modelo, eliminando a necessidade de executar todo o processo de treinamento e infer√™ncia localmente.

## Pipeline
- Um pipeline, no contexto de processamento de linguagem natural (NLP), √© uma sequ√™ncia de etapas ou transforma√ß√µes aplicadas a um texto durante o pr√©-processamento ou a an√°lise. Ele consiste em encadear diferentes componentes, como tokeniza√ß√£o, remo√ß√£o de stop words, lematiza√ß√£o, entre outros, para obter um resultado final desejado.

- O uso de um pipeline permite automatizar e sistematizar o fluxo de trabalho de processamento de texto, aplicando uma s√©rie de opera√ß√µes em sequ√™ncia. Cada etapa do pipeline √© executada de forma consecutiva, utilizando os resultados intermedi√°rios como entrada para a pr√≥xima etapa.

- Os pipelines s√£o √∫teis para organizar e reutilizar o c√≥digo de pr√©-processamento de texto, permitindo que diferentes textos passem por um conjunto consistente de etapas. Eles tamb√©m simplificam o processo de an√°lise e extra√ß√£o de informa√ß√µes dos textos, fornecendo uma estrutura clara para a implementa√ß√£o de fluxos de trabalho de NLP.

## Modelos de Vetoriza√ß√£o
- Os modelos de vetoriza√ß√£o s√£o t√©cnicas utilizadas para representar palavras ou documentos como vetores num√©ricos, de modo a serem processados por algoritmos de aprendizado de m√°quina. Essas representa√ß√µes vetoriais s√£o essenciais para que os modelos de aprendizado de m√°quina possam trabalhar com dados textuais, que s√£o representados por sequ√™ncias de caracteres.

- Existem diferentes modelos de vetoriza√ß√£o, cada um com suas caracter√≠sticas e abordagens espec√≠ficas. Alguns exemplos comuns incluem o Bag of Words (BoW), que representa cada documento como um vetor contendo a contagem de ocorr√™ncia de cada palavra, e o Word2Vec, que mapeia cada palavra para um espa√ßo vetorial de dimens√µes reduzidas, capturando rela√ß√µes sem√¢nticas entre palavras.

- A vetoriza√ß√£o de textos √© uma etapa crucial no processamento de linguagem natural, pois permite que as informa√ß√µes textuais sejam traduzidas em representa√ß√µes num√©ricas que podem ser processadas pelos algoritmos de aprendizado de m√°quina.

## Visualiza√ß√£o dos Dados
- A visualiza√ß√£o dos dados √© uma t√©cnica utilizada para representar informa√ß√µes de forma gr√°fica ou visual, permitindo uma compreens√£o mais intuitiva dos padr√µes, tend√™ncias e rela√ß√µes presentes nos dados. No contexto de processamento de linguagem natural (NLP), a visualiza√ß√£o dos dados pode ser aplicada para explorar e entender caracter√≠sticas dos textos, tais como a frequ√™ncia de palavras, distribui√ß√£o de t√≥picos ou sentimentos.

- As t√©cnicas de visualiza√ß√£o de dados podem incluir gr√°ficos de barras, gr√°ficos de dispers√£o, nuvens de palavras, mapas de calor, nuvens de palavras, entre outros. Essas representa√ß√µes visuais podem ajudar a identificar insights, detectar padr√µes, destacar outliers e comunicar informa√ß√µes de forma mais eficaz.

- A visualiza√ß√£o dos dados √© uma ferramenta poderosa para auxiliar na explora√ß√£o e an√°lise de grandes conjuntos de dados textuais. Ela permite visualizar as caracter√≠sticas textuais de maneira intuitiva, facilitando a identifica√ß√£o de tend√™ncias e a comunica√ß√£o dos resultados de forma clara e concisa.

- Ao visualizar os dados textuais, √© poss√≠vel obter uma vis√£o geral das informa√ß√µes contidas nos textos, identificar palavras-chave relevantes, verificar a distribui√ß√£o de palavras ao longo do tempo ou comparar diferentes categorias ou grupos de textos.

- A visualiza√ß√£o dos dados textuais pode ser realizada utilizando bibliotecas gr√°ficas como o Matplotlib, Seaborn ou Plotly, que oferecem uma ampla variedade de op√ß√µes e recursos para criar visualiza√ß√µes atrativas e informativas.

## API do Modelo
- A API do modelo √© uma interface que permite a comunica√ß√£o e intera√ß√£o com um modelo de aprendizado de m√°quina implantado em um servi√ßo ou servidor. Essa API define os endpoints e m√©todos dispon√≠veis para fazer solicita√ß√µes e receber respostas do modelo.

- Ao expor um modelo como uma API, √© poss√≠vel enviar dados para o modelo e obter previs√µes ou resultados processados em tempo real. Isso facilita a integra√ß√£o do modelo em diferentes aplicativos, sistemas ou plataformas, permitindo sua utiliza√ß√£o de forma flex√≠vel e escal√°vel.

- A API do modelo pode receber dados como par√¢metros de entrada, realizar a pr√©-processamento necess√°rio, aplicar o modelo treinado e retornar as previs√µes ou resultados ao usu√°rio. Ela pode ser implementada usando frameworks web, como Flask ou Django, que fornecem ferramentas para criar e gerenciar uma API de forma eficiente.

- A API do modelo permite que o modelo de aprendizado de m√°quina seja consumido de maneira mais acess√≠vel e f√°cil, oferecendo uma interface consistente e simplificada para interagir com o modelo e obter os resultados desejados.

<img src = "![Captura de Tela 2023-05-30 aÃÄs 17 27 47](https://github.com/2023M6T4-Inteli/Projeto2/assets/99759369/990249b2-9540-4b8a-923b-3432ccb20ff1)">



## (Sprint 4) Proposta de uma nova modelagem utilizando novas features (IPYNB)

- Elmo: https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/Notebook/ELMo.ipynb 

- BERT: https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/Notebook/3_2_BERT(OVER).ipynb

- DOC2Vec: https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/Notebook/DOC2Vec_2.ipynb

- GloVe: https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/Notebook/GloVe.ipynb 

- FastText: https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/Notebook/Fast_Text.ipynb 

- TF_IDF: https://github.com/2023M6T4-Inteli/Projeto2/blob/main/src/Notebook/TF_IDF_com_Regressao_logistica.ipynb


## (Sprint 4) Documenta√ß√£o da proposta de uma nova modelagem

## 1. Elmo 

O Elmo (Embeddings from Language Models) √© um modelo de vetoriza√ß√£o pr√©-treinado que captura representa√ß√µes contextuais de palavras e frases. 
Tal utiliza√ß√£o √© fundamentada nas caracter√≠sticas estruturais do modelo. Evidencia-se a possibilidade de identificar palavras iguais ou semelhantes em contextos diferentes, agrup√°-los e diferenci√°-los.
O modelo se destaca dado que possui capacidade de generaliza√ß√£o, dado que, foi treinado em grandes quantidades de texto. 
Ao aplic√°-lo, a fim de obter melhores resultados, utiliza-se o csv do texto pr√©-processado. Em seguida, aplica-se no modelo de vetoriza√ß√£o ELMo. 
√â de indubit√°vel import√¢ncia ressaltar que para a utiliza√ß√£o do ELMo os arquivos de pr√©-treinamento, em portugu√™s precisam ser importados.
```
weight_file = "/content/drive/MyDrive/Colab Notebooks/SI-MOD6/entrega_spt4/elmo_pt_weights_dgx1 (1).hdf5
```
J√° a linha a seguir, define o local do arquivo JSON de op√ß√µes para o modelo ELMo em portugu√™s. O arquivo JSON cont√©m a configura√ß√£o e os hiperpar√¢metros do modelo.
```
options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pt/brwac/options.json'
```
As frases de input s√£o convertidas em identificadores de caracteres e, em seguida, √© calculado os embeddings correspondentes. Os resultantes s√£o armazenados em um array. Ap√≥s a vetoriza√ß√£o dos dados, os mesmos s√£o direcionados para a rede neural simples. 
Na rede neural, seta-se 2 dimens√µes dos embeddings redimensionados, uma vez que, a dimensionalidade do input, sem tratamento, √© incompat√≠vel com o suporte da rede. Na mesma, s√£o passados os vetores resultantes do ELMo e os targets. <br>

Defini-se, ent√£o, a arquitetura da rede neural em duas camadas densas. A primeira camada possui 2 unidades com ativa√ß√£o sigmoidal, e a segunda camada possui 1 unidade com ativa√ß√£o sigmoidal. <br>
Destaca-se a utiliza√ß√£o do otimizador ‚ÄúAdam‚Äù devido a frequente utiliza√ß√£o em conjuntos de dados enxutos e com redes profundas. Al√©m disso, o otimizador atualiza os pesos da rede mais rapidamente.<br>

A m√©trica escolhida para avaliar o resultado do modelo √© recall, visto que, dentre todas as classifica√ß√µes positivas como valor esperado, quantas foram assertivas. Entretanto, ocorreram alguns desajustes durante o arranjo da rede neural. Portanto, nesse momento, a fim de avaliar a performance geral, utiliza-se a acur√°cia. <br>
Os resultados emitidos na rede neural demonstram 43.17% de acur√°cia. A porcentagem apresentada, em considera√ß√£o ao que √© considerado ideal ou avaliativo para os modelos, n√£o √© satisfat√≥ria para aplica√ß√£o, em compara√ß√£o √† modelos que obtiveram resultados mais assertivos. <br>

Em s√≠ntese, destaca-se que esse n√∫mero pode ser melhorado em decorr√™ncia da separa√ß√£o dos dados de treino e teste, diferencia√ß√£o no n√∫mero de epochs e varia√ß√£o nas dimens√µes.



## 2. BERT 

O BERT (Bidirectional Encoder Representations from Transformers) √© um modelo de linguagem baseado em Transformers, arquitetura que √© baseada em mecanismos de "aten√ß√£o" e relev√¢ncia. Isto permite que o modelo capture rela√ß√µes de depend√™ncia entre palavras de maneira eficiente e paralela. 

Neste caso, o que utilizamos √© uma vers√£o pr√©-treinada, com o uso do "preenchimento de m√°scara" (masked language modeling), com a inten√ß√£o de uma futura implementa√ß√£o da funcionalidade de predi√ß√£o da "pr√≥xima senten√ßa" (next sentence prediction). Uma das principais caracter√≠sticas do BERT √© sua capacidade de gerar representa√ß√µes contextualizadas de palavras. O modelo leva em considera√ß√£o o contexto em que uma palavra aparece, o que ajuda a capturar rela√ß√µes e significados mais precisos. Consequentemente, essas representa√ß√µes contextualizadas s√£o obtidas atrav√©s do pr√©-treinamento bidirecional e podem ser usadas como entrada para o algoritmo.

- Algoritmo: Rede Neural 

Como algoritmo, utilizamos a Rede Neural na arquitetura Transformers, para realizar as etapas de predi√ß√£o.

Etapas de implementa√ß√£o 

- Prepara√ß√£o dos dados (exemplo com Label Encoder realizado):

```
label_encoder = LabelEncoder()

df_lemma['sentimento_3'] = label_encoder.fit_transform(df_lemma['sentimento'])

```

- Pr√©-treinamento do modelo BERT:

```
model = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')
```

Esta etapa envolve tamb√©m a defini√ß√£o dos par√¢metros, a alimenta√ß√£o dos dados no modelo, atribui√ß√£o dos tensores e da famosa "attention_mask".

- Avalia√ß√£o e ajuste: 

  Nesta etapa realizamos a avalia√ß√£o do modelo, com base nos dados de treino e teste que foram previamente divididos, e realizamos a itera√ß√£o do modelo sobre o conjunto de treinamento, para assim, em seguida, atribuir as predi√ß√µes √†s categorias (e targets) definidos. Por fim, realizamos a impress√£o das m√©tricas de classifica√ß√£o.


Ap√≥s diversas implementa√ß√µes do modelo, e devido a sua complexidade nas etapas de entrada da Rede Neural, obtivemos resultados medianos com o modelo BERT. Consequentemente, com a primeira implementa√ß√£o, na m√©trica de maior relev√¢ncia para o projeto, o "Recall" obtivemos 72% (0.72). Em termos de assertividade, em compara√ß√£o aos outros modelos foi o nosso terceiro maior. 

No entanto, devido ao seu alto n√≠vel de complexidade, obtivemos resultados com muitos sinais de overfitting e do enviesamento na entrada dos tensores, o que ocorreu ap√≥s tentativa de resolu√ß√£o de problemas no c√≥digo.

Devido a isso, tratamos o modelo BERT como um "nice to have", mas n√£o como principal modelo a ser utilizado. Mesmo assim, consideramos que na escala de modelos com algoritmos de Rede Neural, √© com certeza o que mais gera escalabilidade e por isso h√° o interesse em sua implementa√ß√£o.

**Pr√≥ximos passos**

  - Fine-tuning para tarefas espec√≠ficas:
    Depois do pr√©-treinamento, o BERT pode ser ajustado (fine-tuned) para tarefas espec√≠ficas, como classifica√ß√£o de texto, extra√ß√£o de informa√ß√µes e resposta a perguntas. Ao ajustar o BERT para uma tarefa espec√≠fica, as camadas de classifica√ß√£o s√£o adicionadas ao modelo e o modelo √© treinado em um conjunto de dados rotulados para aprender a tarefa espec√≠fica.

- Atention Mask: 
  Na tarefa de preenchimento de m√°scara, palavras s√£o mascaradas aleatoriamente e o modelo √© treinado para prever as palavras mascaradas com base no contexto das palavras vizinhas e sua relev√¢ncia.

√â importante citar que o modelo BERT tem sido amplamente utilizado em uma variedade de tarefas de processamento de linguagem natural e estabeleceu um novo par√¢metro em muitas delas. Isso foi um dos fatores determinantes para a escolha e utiliza√ß√£o deste modelo em nosso grupo. Ele se destaca pela sua capacidade de capturar informa√ß√µes contextuais em textos e fornecer representa√ß√µes de alta qualidade que podem ser usadas em v√°rias aplica√ß√µes de NLP.

## 3. Doc2Vec

O modelo Doc2Vec √© uma t√©cnica de aprendizado de m√°quina utilizada para representar documentos em formato vetorial. Ele √© uma extens√£o do modelo Word2Vec, que √© usado para representar palavras em vetores. O Doc2Vec permite que documentos inteiros sejam representados como vetores cont√≠nuos de valores num√©ricos, capturando o contexto sem√¢ntico dos documentos.

O objetivo do Doc2Vec √© gerar representa√ß√µes vetoriais para documentos que preservem a sem√¢ntica e a similaridade entre eles. Essas representa√ß√µes vetoriais podem ser usadas em v√°rias tarefas de processamento de linguagem natural, como classifica√ß√£o de documentos, recomenda√ß√£o de conte√∫do, agrupamento de documentos semelhantes e recupera√ß√£o de informa√ß√µes.

Algoritmos utilizados: Naive Bayes, Regress√£o Log√≠stica e Rede Neural

No processamento do modelo, realizamos alguns tipos de taggeamento, que s√£o pr√≥prios para treinamento do modelo, assim como a defini√ß√£o de par√¢metros e outros recursos de vetoriza√ß√£o. Abaixo segue o exemplo de taggeamento:

```
tagged_data = [TaggedDocument(words=word_tokenize(text.lower()), tags=[str(i)]) for i, text in enumerate(dados['texto'])]

```

Ap√≥s estas defini√ß√µes realizamos a vetoriza√ß√£o dos tokens com base nas defini√ß√µes da pr√≥pria biblioteca gensim, e ap√≥s impress√£o das itera√ß√µes destes vetores, podemos aplicar nos algoritmos.

** √â importante ressaltar que a m√©trica de avalia√ß√£o central foi o Recall **. 

Na primeira implementa√ß√£o, com Regress√£o Log√≠stica, tivemos a porcentagem de 58% (0.58), em rela√ß√£o √†s classifica√ß√µes, assim como na segunda implementa√ß√£o em Random Search, que obtivemos, tamb√©m, o recall em 58%. J√° na terceira implementa√ß√£o utilizando Naive Bayes, conseguimos o bom resultado de 82% (0.82) em recall. 

Por fim, implementamos a Rede Neural, apenas para satisfazer os termos acad√™micos, e obtivemos a acur√°cia de 73% (0.73).

No caso espec√≠fico, o modelo Doc2Vec foi escolhido como modelo final devido √†s suas vantagens e desempenho em rela√ß√£o aos outros m√©todos de representa√ß√£o de documentos. Al√©m disso, levamos em conta a sua aplicabilidade, tendo em vista que o Doc2Vec √© capaz de capturar rela√ß√µes sem√¢nticas complexas entre palavras e documentos, gerando vetores que preservam a semelhan√ßa sem√¢ntica entre textos.

Tamb√©m foi evidenciado, nos testes que realizamos, que o modelo Doc2Vec possui uma implementa√ß√£o mais simples, eficiente e bem estabelecida na biblioteca Gensim, o que facilita sua utiliza√ß√£o e treinamento. E em termos de assertividade, com a m√©trica de prioridade (Recall), em rela√ß√£o aos outros modelos, o modelo Doc2vec foi muito bem.

## 4. GloVe

O Global Vectors for Word Representation, tamb√©m conhecido como GloVe, √© um modelo de vetoriza√ß√£o de palavras desenvolvido com o intuito de identificar as rela√ß√µes sint√°ticas e sem√¢nticas em um conjunto de um texto. Esse modelo, utiliza estat√≠sticas de co-ocorr√™ncia global de palavras para desenvolver representa√ß√µes vetoriais. O seu processo de treinamento, envolve a constru√ß√£o de uma matriz que registra a frequ√™ncia da ocorr√™ncia das palavras a partir da utiliza√ß√£o da 'fun√ß√£o de perda' (loss function) com o intuito de maximizar a probabilidade de co-ocorr√™ncia de pares de palavras. 

- Algoritmo: Regress√£o Log√≠stica 

O c√≥digo est√° trabalhando com a leitura de um arquivo CSV, contendo os dados lematizados que j√° passaram pelo Pr√©-Processamento. Depois disso, como mostra o c√≥digo abaixo, foi realizado o carregamento do modelo spaCy com vetores GloVe e feito um teste com a palavra 'amor' para calcular seus vetores. 

```
import spacy

# Carregamento do modelo com a utiliza√ß√£o de vetores GloVe
nlp = spacy.load('en_core_web_sm')

# Vetor da palavra teste (amor)
word_vector = nlp('amor')[0].vector
print("Vetor de 'amor':", word_vector)
```

Ap√≥s isso, foi realizado a utiliza√ß√£o da classe CountVectorizer do sklearn.feature_extraction.text para vetorizar os dados contidos na coluna "texto" doDataframe desenvolvido. Assim, a vetroiza√ß√£o foi aplicada aos dados de teste (x_test) e treinamento (x_train) e armazenados em duas vari√°veis com o par√¢metro 'random_state' definido como 42. Depois disso, foi realizado o treinamento do modelo de regress√£o log√≠stica utilizando os dados de treinamento. Dessa forma, a acur√°cia do modelo √© calculada a partir do m√©todo score com X_test e y_test. 



```
X = df_lemma['texto']
y = df_lemma['sentimento']

vectorizer = CountVectorizer()
X_counts = vectorizer.fit_transform(X)
tfidf_transformer = TfidfTransformer()
X_tfidf = tfidf_transformer.fit_transform(X_counts)

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

accuracy = model.score(X_test, y_test)
print("Acur√°cia:", accuracy)
```
Com base nos resultados obtidos √© poss√≠vel fazer as seguintes an√°lises do algoritmo de Regress√£o Log√≠stica. A acur√°cia obtida para o modelo foi de 0.75, o que indica uma boa signific√¢ncia e uma taxa de acerto razoavelmente alta. Al√©m disso foi tamb√©m calculado os valores de precis√£ para cada uma das classes desenvolvidas. O valor obtido para as classe NEGATIVE, NEUTRAL e POSITIVE foram: 0,77; 0,71; e 0,81 respectivamente. Valores relativamente bons. Al√©m disso, foi obtido tamb√©m os valores de recall para cada uma das classes (NEGATIVE = 0,54; NEUTRAL = 0,85 e POSITIVE = 0,75. 
Por fim, foi ent√£o calculado o F1-score, que √© uma medida que combina a precis√£o e o recall em uma √∫nica m√©trica. Para a classe NEGATIVE, o F1-score √© de 0,64, para a classe NEUTRAL √© de 0,77 e para a classe POSITIVE √© de 0,78.que o modelo tem uma taxa de acerto razoavelmente alta.




- Algoritmo: Modelo Naive Bayes 

Primeiramente foi mapeado os r√≥tulos 'POSITIVE', 'NEUTRAL' e 'NEGATIVE' para os valores num√©ricos 3, 1 e 2, respectivamente, e o resultado foi armazenado em uma vari√°vel. 

```
sentimento_mapping = {'POSITIVE': 3, 'NEUTRAL': 1, 'NEGATIVE': 2}
y_mapped = df['sentimento'].map(sentimento_mapping)
```

Depois disso, foi realizado o treinamento do modelo Naive Bayes Gaussiano, por√©m n√£o foi obtido um valor de acur√°cia t√£o satisfat√≥rio, equivalente a 0.57. Isso indicou que o modelo tem uma taxa de acerto um pouco mais baixa com o valor obtido previamente com o algoritmo da regress√£o log√≠stica.

```
model = GaussianNB()
model.fit(X_train, y_train)

# Predi√ß√£o e c√°lculo da acur√°cia
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
```



Por fim, foi realizado um gr√°fico de plotagem da curva ROC, que pode ser visto abaixo. 


<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/GloVe.png">


Em resumo, os resultados obtidos indicam que o modelo de regress√£o log√≠stica teve um desempenho melhor em compara√ß√£o com o modelo de Naive Bayes. A regress√£o log√≠stica obteve uma acur√°cia mais alta e um melhor equil√≠brio entre precis√£o e recall. Al√©m disso, o gr√°fico de curva ROC mostra que o modelo de regress√£o log√≠stica possui uma capacidade satisfat√≥ria de distinguir entre as diferentes classes. No entanto, √© importante lembrar que esse modelo obteve valores bons, por√©m n√£o foram os melhores diante dos outros modelos desenvolvidos.


## 5. FastText

O FastText √© uma biblioteca gratuita e de c√≥digo aberto do Facebook AI Research (FAIR) para aprender embeddings e classifica√ß√µes de palavras. Este modelo permite a cria√ß√£o de um algoritmo de aprendizado para a obten√ß√£o de representa√ß√µes vetoriais de palavras, avaliando esses modelos.

Primeiramente, os valores da coluna alvo foram transformados em valores num√©ricos:

```
df_2['sentimento'] = df_2['sentimento'].map({'NEUTRAL': 0, 'POSITIVE': 1, 'NEGATIVE': -1})
```

Foi feito o teste com o modelo possuindo 50 e 100 dimens√µes mas, como a diferen√ßa n√£o foi significante, foi utilizado o de 50. O modelo de word embeddings pr√© treinadas foi instalado do reposit√≥rio de word embeddings do NILC. Para aplica√ß√£o no projeto, foi utilizada a base de dados com os dados j√° lematizados e tratados. O modelo foi carregado e aplicado em uma vetoriza√ß√£o da coluna 'tokens', assim como √© feito com o modelo Word2Vec. 

```
# Fun√ß√£o para vetorizar um token
def vetorizar_token(token):
    vetor = np.zeros(model.vector_size) # incializa vetor de zeros com a mesma dimens√£o
    if token in model: # verifica se a palavra est√° no word2vec treinado
        vetor = model[token] # adiciona o valor do vetor
    return vetor

# Fun√ß√£o para vetorizar uma frase
def vetorizar_frase(frase):
    vetores_tokens = [vetorizar_token(token) for token in frase] # verifica cada token da lista
    return np.sum(vetores_tokens, axis=0) # retorna a soma dos vetores

# Aplicar a fun√ß√£o 'vetorizar_frase' a todas as frases
df_2['vetores'] = df_2['tokens'].apply(vetorizar_frase)
```

Ap√≥s isso, com os vetores criados, foram aplicados o modelo Naive Bayes (que retornou uma acur√°cia de 0.31) e a Regress√£o Log√≠stica (com esta √∫ltima obtendo um melhor resultado, com acur√°cia de 0.56). Estes resultados foram um pouco inferiores em rela√ß√£o aos testes realizados em outros modelos com os mesmos algoritmos (Regress√£o log√≠sitica e Naive Bayes). Sendo assim, o modelo n√£o foi considerado com grande relev√¢ncia para a escolha do modelo final.

Com base nos resultados e processamentos, constru√≠mos diferentes gr√°ficos: o da curva ROC, o da curva de aprendizado e o da curva de valida√ß√£o. 

Abaixo est√£o os gr√°ficos da curva ROC obtidos com cada modelo:

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/0a41966a-07cd-4309-ae1e-d80c82755166)

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/0a6b22a9-7f79-4ba9-8b96-97f243fa2915)

## 6. TF-IDF

O TF-IDF (Term Frequency-Inverse Document Frequency) √© uma medida estat√≠stica que permite avaliar a import√¢ncia de uma palavra em um documento. Essa t√©cnica foi escolhida por sua capacidade de destacar palavras-chave relevantes e reduzir o peso de palavras comuns, ajudando a identificar a relev√¢ncia de um termo em rela√ß√£o ao contexto espec√≠fico de um documento. Suas vantagens incluem a capacidade de lidar com grandes volumes de texto de maneira eficiente, reduzindo essa influ√™ncia de palavras comuns e destacando exatamente os termos-chave que fornecem insights relevantes. O TF-IDF tamb√©m √© uma t√©cnica simples de implementar e interpretar.

- Algoritmo: Regress√£o Log√≠stica 

A Regress√£o Log√≠stica √© um algoritmo de aprendizado de m√°quina que √© amplamente utilizado para tarefas de classifica√ß√£o bin√°ria. Ele modela a rela√ß√£o entre vari√°veis independentes e a probabilidade de uma resposta pertencer a uma determinada classe. Sua escolha foi feita especialmente pela sua capacidade de lidar com dados categ√≥ricos (como √© o caso da base de dados trabalhada).

A combina√ß√£o de TF-IDF com Regress√£o Log√≠stica aproveita as vantagens de ambos os m√©todos. O TF-IDF fornece uma representa√ß√£o ponderada das palavras em um documento, enquanto a Regress√£o Log√≠stica modela a rela√ß√£o entre essas palavras e a probabilidade de classifica√ß√£o. Essa combina√ß√£o √© comum em processos de an√°lise de sentimentos como esse.
<br>
<br>
A base de dados utilizada j√° passou pelos primeiros processos de PLN (remo√ß√£o de stop words, substitui√ß√£o de g√≠rias e, em destaque, a lematiza√ß√£o). Essa √© a mesma base de dados lematizada que foi desenvolvida na Sprint 3 e utilizada no modelo anterior. Contudo, apenas as colunas das frases e suas qualifica√ß√µes est√£o sendo utilizadas. 
<br>
```
#dividi os dados em conjunto de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(dados["texto"], dados["sentimento"], test_size=0.2, random_state=42)
```
Ap√≥s a importa√ß√£o da base de dados no modelo, foi feita uma divis√£o dos dados em conjunto de treinamento e teste, com o test size de 0.2 e random state de 42.
<br>
```
#cria pipeline com TF-IDF e modelo de classifica√ß√£o
pipeline = Pipeline([
    ("tfidf", TfidfVectorizer()),
    ("model", LogisticRegression(max_iter=1000))  # n√∫mero m√°ximo de itera√ß√µes
])
```
Como j√° indicado, o modelo criado utiliza TF-IDF com Regress√£o Log√≠stica para a an√°lise de sentimentos. Para a regress√£o Log√≠stica o n√∫mero de itera√ß√µes foi configurado para mil a fim de conseguir um modelo que identifica mais correla√ß√µes entre as palavras.
<br>
<br>
Para avaliar os resultados obtidos foi utilizado principalmente os par√¢metros de Recall e Acur√°cia 
<br>

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/TF-IDF_Dados.png" alt="td-idf dados" width="500" height="auto">

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/TF-IDF_grafico.png">

<br>

Com base nos resultados obtidos, podemos concluir que o modelo apresenta um desempenho equilibrado e consistente, com valores de acur√°cia e recall macro pr√≥ximos, na faixa de 0,7. Isso indica que o modelo √© capaz de fazer previs√µes precisas na maioria das amostras de teste, classificando corretamente tanto as inst√¢ncias positivas quanto as negativas.

Al√©m disso, a similaridade entre a acur√°cia e o recall macro sugere que o modelo n√£o est√° enviesado para uma classe espec√≠fica e est√° tratando ambas as classes de forma equilibrada.

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/TF-IDF_MC.png">

A partir da matriz de confus√£o √© poss√≠vel perceber que o modelo tem mais dificuldades ao avaliar coment√°rios neutros e uma facilidade para avaliar positivos e, em segundo lugar, negativos. Os resultados nessa inst√¢ncia foram satisfat√≥rios.

Por fim, o modelo obteve bom resultados em avalia√ß√µes de recall, acur√°cia e matriz de confus√£o, contudo, n√£o foram os melhores diante dos outros modelos desenvolvidos.


## 7.Compara√ß√£o entre os modelos e escolha do modelo final

Os modelos foram submetidos aos seus respectivos processos de vetoriza√ß√£o e, em seguida, aos mesmos algoritmos, sendo eles, regress√£o log√≠stica e naive bayes, a fim de garantir a avalia√ß√£o conforme a mesma otimiza√ß√£o. Al√©m do mais, ressalta-se a utiliza√ß√£o do recall como m√©trica central de avalia√ß√£o, pois, para a problem√°tica estabelecida, √© importante identificar verdadeiros positivos.

Portanto, observa-se, respectivamente, as maiores precis√µes dos modelos: Doc2Vec, GloVe, Bert, TF-IDF e Fast-Text. Dessa forma, nota-se esse resultado como possibilidade de realizar melhores combina√ß√µes no modelo Doc2Vec e os algoritmos associados, com o objetivo de resultantes mais precisos.

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/assets/Imagens/gr%C3%A1fico_comparativo.png">

Esclarece-se que os resultados do ELMo n√£o s√£o expostos na compara√ß√£o entre os modelos, dado que, durante a explora√ß√£o do mesmo, foi poss√≠vel perceber a complexidade no qual est√° equiparado, al√©m da utiliza√ß√£o de redes neurais profundas, diferente dos demais modelos, que utilizam algoritmos.

## 8.Adicionando Features Novas

Foram criadas novas features baseadas no tamanho das frases, em particular o n√∫mero de tokens por frase, para comparar com modelos que incluam ou n√£o essas features. Quatro algoritmos foram utilizados para esta compara√ß√£o: regress√£o log√≠stica, cat-boost, naive-bayes e xg-boost. A vetoriza√ß√£o escolhida para esta compara√ß√£o foi o TF-IDF devido sua facilidade de aplica√ß√£o e a combina√ß√£o com os algoritimos escolhidos. Todos os modelos que utilizaram as novas features tiveram resultados inferiores em compara√ß√£o aos modelos que n√£o as utilizaram, apresentando uma m√©dia de recall de 38.7. No entanto, a inclus√£o das novas features permitiu a utiliza√ß√£o do gr√°fico KDE (Estimativa de Gr√°fico Kernel) que demonstra a probabilidade de um sentimento, dado o n√∫mero de tokens de uma frase. Onde -1 representa sentimentos negativos, 0 representa sentimentos neutros e 1 representa sentimentos positivos.

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99209230/5500412f-e3df-499d-baea-1ff9c862f9b1)

C√°lculo KDE para a Estimativa:
KDE(x) = (1 / (n * h)) * Œ£ K((x - xi) / h)
KDE(x) √© o valor estimado da densidade de probabilidade para um determinado ponto x. n √© o n√∫mero total de pontos de dados no conjunto de dados. h √© o par√¢metro de suaviza√ß√£o ou largura de banda (bandwidth). Œ£ representa a soma ao longo de todos os pontos de dados. K √© a fun√ß√£o kernel, que √© uma fun√ß√£o sim√©trica em torno de zero que define a forma da contribui√ß√£o de cada ponto de dados para a estimativa de densidade.



Compara√ß√£o entre os modelos com a feature aplicada:

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99209230/6212235f-588c-466f-9439-38f3751095c6)


Maior valor(cat-boost):

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99209230/4c7c25b6-fa7a-4f83-8143-d0c970215746)


Modelos sem a adi√ß√£o da feature aplicada:

![newplot](https://github.com/2023M6T4-Inteli/Projeto2/assets/99209230/af9eb689-e0a6-4ce9-b0de-e08411b1dc27)


Maior valor(xg-boost):

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99209230/f4a55487-b5b3-4d7a-94bd-ef23f8c63b03)



## (Sprint 5) Apresenta√ß√£o Final

Colocar o link do artefato (deve estar na pasta apresentacoes do reposit√≥rio do projeto).

## (Sprint 5) Deploy do melhor modelo

Colocar o link dos artefatos (devem estar nas pastas videos e src do reposit√≥rio do projeto).

## (Sprint 5) Documenta√ß√£o da Solu√ß√£o

Preencher conforme a descri√ß√£o do artefato na Adalove.
