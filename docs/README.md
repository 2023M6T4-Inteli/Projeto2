# Documento Principal do Projeto

Descri√ß√£o sucinta do projeto, com descri√ß√£o do problema, do objetivo e da solu√ß√£o em linhas gerais.

## (Sprint 1) Entendimento do Neg√≥cio

 ### üìà Matriz de avalia√ß√£o de valor Oceano Azul 

A Matriz Oceano Azul √© a ferramenta que ajuda as empresas √† identificar novas oportunidades de crescimento dentro do mercado e quais fatores √†s diferenciam da concorr√™ncia, criando valor para seus clientes.

Para esta an√°lise, foram setadas outras Intelig√™ncias Artificiais, baseadas em Processamento de Linguagem Natural (PLN), como **Open AI, Amazon Comprehend, Microsoft Azure Cognitive Services, Google Cloud Natural Language, IBM Watson.** 

A OpenAI √© uma plataforma de intelig√™ncia artificial que oferece uma ampla gama de ferramentas e recursos para empresas e indiv√≠duos. A plataforma utiliza modelos de linguagem natural avan√ßados, como o GPT-3, para criar solu√ß√µes personalizadas para problemas espec√≠ficos de neg√≥cios.
A Amazon Comprehend √© um servi√ßo de an√°lise de texto fornecido pela AWS que permite a identifica√ß√£o de insights e informa√ß√µes √∫teis a partir de dados textuais, em grande escala. A tecnologia usa t√©cnicas de aprendizado de m√°quina para extrair informa√ß√µes √∫teis a partir de dados textuais, como sentimentos, emo√ß√µes, t√≥picos, entidades, rela√ß√µes e muito mais. O servi√ßo √© capaz de analisar grandes quantidades de texto, em v√°rias l√≠nguas, e produzir informa√ß√µes significativas para apoiar a tomada de decis√µes de neg√≥cios, como por exemplo conjuntos de dados de texto, incluindo documentos, mensagens de texto, posts de redes sociais e outras formas de comunica√ß√£o escrita, para identificar tend√™ncias.

O Microsoft Azure Cognitive Services √© um conjunto de servi√ßos cognitivos com aloca√ß√£o em nuvem que permite que desenvolvedores agreguem recursos de intelig√™ncia artificial (IA) a seus aplicativos sem a necessidade de experi√™ncia em aprendizado de m√°quina ou an√°lise de dados. Esses servi√ßos s√£o constru√≠dos com base em algoritmos de aprendizado de m√°quina, vis√£o computacional, reconhecimento de fala, processamento de linguagem natural e outras tecnologias. Consequentemente, os servi√ßos cognitivos da Azure s√£o altamente escal√°veis e personaliz√°veis, permitindo que as empresas criem aplicativos sob medida para suas necessidades espec√≠ficas, como por exemplo para realizar tarefas como reconhecimento de voz, an√°lise de sentimentos, detec√ß√£o de imagens, tradu√ß√£o de idiomas e muito mais. Al√©m disso, os servi√ßos podem ser facilmente integrados a outras tecnologias da Microsoft, como o Microsoft Power BI, Microsoft Dynamics 365 e Microsoft Office 365.

A Google Cloud Natural Language √© uma ferramenta de processamento de linguagem natural (PLN) criada pela Google que permite aos usu√°rios extrair informa√ß√µes valiosas a partir de textos n√£o estruturado, incluindo e-mails, documentos, artigos e outras fontes de dados. Com a t√©cnica de aprendizado de m√°quina e processamento de linguagem natural, a Google Cloud Natural Language pode identificar entidades ou indiv√≠duos mencionados em um texto, como nomes de pessoas, locais, organiza√ß√µes e datas, al√©m de extrair sentimentos e emo√ß√µes expressos pelo autor. Al√©m disso, h√° possibilidade de agrega√ß√£o com o Google Cloud Natural Language, que permite as empresas analisarem grandes volumes de dados para obter insights √∫teis que podem ajudar a orientar decis√µes importantes de neg√≥cios.

O IBM Watson √© um sistema de PLN, desenvolvido pela IBM. A ferramenta utiliza da an√°lise de dados e aprendizado de m√°quina para processar informa√ß√µes e fornecer insights relevantes para seus usu√°rios. O mesmo √© utilizado em v√°rios setores, incluindo sa√∫de, finan√ßas, educa√ß√£o, manufatura.
O IBM Watson √© uma plataforma de computa√ß√£o cognitiva e intelig√™ncia artificial desenvolvida pela empresa americana IBM. Ele utiliza tecnologias avan√ßadas de processamento de linguagem natural, machine learning, an√°lise de dados e outras t√©cnicas de intelig√™ncia artificial para fornecer insights e solu√ß√µes para uma ampla variedade de aplica√ß√µes.

**Eliminar ‚Üí Assertividade, Escalabilidade, Robustez, confiabilidade, adaptabilidade, Assertividade, Tecnologia**

A a√ß√£o de eliminar, dentro da Matriz Oceano Azul, est√° relacionada √† enumera√ß√£o de fatores que podem ser retirados ou melhorados, ao analisar os atributos do neg√≥cio. Portanto, analisa-se a distribui√ß√£o da ferramentas, em temos de escalabilidade, assertividade, acessibilidade, robustez e outras se√ß√µes atribu√≠dos √† proposta do Chat-BTG, em compara√ß√£o √† outras solu√ß√µes que utilizam Processamento de Linguagem Natural. Dado que a projeto consiste no desenvolvimento de um MVP, atualmente, o chat-BTG √© uma iniciativa sem afunilamento em sua complexidade, por isso, sua escalabilidade √© um fator a ser pensado nos pr√≥ximos passos do projeto, principalmente com √™nfase na robustez e maior assertividade da tecnologia.
J√° os outros modelos, por j√° estarem no mercado e ter uma infraestrutura posicionada, tendo em vista que servi√ßos de cloud e outros ‚Äúweb services‚Äù fornecem agrega√ß√£o em tempo real, com dados estruturados e j√° disponibilizados no pr√≥prio servi√ßo, conseguem maior escalabilidade e confiabilidade, conforme as tecnologias e funcionalidades contidas em seus servi√ßos.

**Aumentar ‚Üí  Personaliza√ß√£o**

A personaliza√ß√£o, nessa an√°lise, √© um fator de destaque no mercado, em compara√ß√£o aos concorrentes. Destaca-se que o modelo de Processamento de Linguagem Natural √© desenvolvido conforme a base de dados fornecido pelo cliente, voltado aos coment√°rios feitos nos posts da conta BTG Pactual no Instagram.
Ressalta-se que a visualiza√ß√£o dos dados tamb√©m ser√° pensada afim de facilitar o processo de tomada das decis√µes nos times de marketing e produto, como vantagem competitiva do banco.

**Criar ‚Üí Custo do Setup**

Com a vis√£o de ampliar, agregar e integrar diversas ferramentas, pode-se citar alguns fatores determinantes na an√°lise financeira para o escopo do projeto. Neste sentido, √© importante ressaltar que o projeto ‚ÄúChat-BTG‚Äù ainda √© limitado ao escopo m√≠nimo de requisitos de uma rede social espec√≠fica, dentro de dados levantados a partir de uma base de dados pr√©-definida, sem atualiza√ß√£o em tempo real. Consequentemente, a n√≠veis de complexidade baixos, possu√≠mos um custo muito interessante com base no benchmark do mercado. Sendo uma solu√ß√£o nativa, o custo de setup da solu√ß√£o √© muito baixo, e outros custos de ‚Äúweb services‚Äù s√£o mitigados com o processo interno realizado.
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Matriz_Oceano_Azul_CHATBTG.png">

### ‚ö†Ô∏è Matriz de riscos

A Matriz de Riscos consiste em uma ferramenta de gest√£o de riscos e oportunidades, a fim de identificar, avaliar e priorizar os fatores associados ao projeto, por grau de impacto e probabilidade de ocorr√™ncia.
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Matriz%20de%20Risco.png">

Por tanto, entende-se ent√£o, que o desenvolvimento de um Plano de A√ß√£o √© de indubit√°vel import√¢ncia, dado que, o documento descreve as atividades que precisam ser realizadas para mitigar riscos e alcan√ßar objetivos estipulados. O Plano define o que ser√° feito, quem far√°, como, quando e com quais recursos. <br>
No que diz respeito aos riscos evidenciados na Matriz acima, ressalta-se o plano de a√ß√£o desenvolvido:

**Amea√ßa 01:** Thain√° - Identificar quais s√£o as expectativas do cliente e manter o foco nelas durante a produ√ß√£o do modelo, al√©m de revisar o modelo ap√≥s cada mudan√ßa, comparando-o com as expectativas definidas para o projeto.

**Amea√ßa 02:**  Vinicius - Identificar as causas da falha, e implementar medidas de solu√ß√£o monitorar os testes no tratamento dos dados. Revisar a metodologia de pr√©-processamento de dados.

**Amea√ßa 03:** Kathy e Lucas -  Verificar por quais poss√≠veis motivos existe um erro na coleta das palavras chaves, e indentificar prad√µes de poss√≠veis palavras que estejam causando esses erros, buscando informa√ß√µes em rela√ß√£o a campanha de marketing. Al√©m disso, caso o problema n√£o for na coleta das palavras, ajustariamos o algoritmo e procurariamos poss√≠veis falhas e tentariamos treinar o modelo  om mais dados e exemplos para melhorar a precis√£o, realizando testes regulares. 

**Amea√ßa 04:** Henri e Rodrigo -  Avaliar se a m√©trica de classifica√ß√£o dos coment√°rios √© suficientemente complexa para a correta classifica√ß√£o dos coment√°rios, conferir com testes se n√£o h√° algum caso que produza resultados incorretos/indesejados.

**Amea√ßa 05:** Jo√£o - Aplicar o 'word embedding' ap√≥s a realiza√ß√£o de testes, definindo as adapta√ß√µes necess√°rias, validando-as constantemente e utilizando diferentes frases para testar a identifica√ß√£o de contexto no modelo.

### üíä Canvas Proposta de Valor

O Canvas Proposta de Valor √© uma ferramenta de neg√≥cios que auxilia no entendimento e cria√ß√£o do posicionamento do projeto (como um produtos que ser√° desenvolvido), com base na cria√ß√£o de ganho que o cliente realmente valoriza e precisa. 

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Canva%20value%20proposition.png">

### üíµ‚ÄäAn√°lise Financeira

A an√°lise financeira corresponde √† uma avalia√ß√£o dos aspectos econ√¥micos e financeiros que permeiam o projeto, com o objetivo de estimar a viabilidade e escabilidade do mesmo.

Conforme o site de not√≠cias Reuters, o BTG Pactual (BTG Pactual S.A.). √© um banco de investimentos brasileiro especializado em capital de investimento e capital de risco. O BTG se configura como uma empresa de capital aberto com cerca de 200 s√≥cios (constitu√≠do por funcion√°rios internos). Mesmo sua sede sendo no Rio de Janeiro, Brasil, sua atua√ß√£o ocorre em escala global, alcan√ßando EUA, Chile, M√©xico, Reino Unido, Portugal, Argentina, Col√¥mbia e Peru. Al√©m das j√° citadas, o banco atua em √°reas como ‚ÄòCorporate Lending (Empr√©stimos e Financiamentos)‚Äô, Sales and Trading, Asset Management, Wealth Management e Ativos Florestais. Apesar da queda das receitas anuais totais (de 2022 para 2023) e tamb√©m das a√ß√µes nos √∫ltimos 6 meses, o banco BTG continua sendo uma das empresas mais relevantes e consolidadas do Brasil. 

Custos em rela√ß√£o ao projeto:

O custos estimados pelo cliente foram de R$250.000. N√£o foram informadas proje√ß√µes de receita pelo cliente (projeto interno).

Em paralelo, foi feito uma proje√ß√£o dos custos do projeto, contabilizando os gastos com funcion√°rios e servi√ßos de infraestrutura da AWS, tendo como m√©dia o valor de R$ 167.784.  


**Profissionais Necess√°rios Para o Desenvolvimento da Solu√ß√£o:**

- Engenheiro em Machine Learning
- Desenvolver de Software
- Cientista de Dados
Opcional (Incomum no Brasil):
- Especialista em lingu√≠stica Computacional

Sal√°rio M√©dio para Engenheiro em Machine Learning: R$ 8.900 /m√™s (Glassdoor) <br>
Sal√°rio M√©dio para Desenvolver Pleno: R$ 10.200 /m√™s (Glassdoor) <br>
Sal√°rio M√©dio para Cientista de Dados: R$ 8.710 /m√™s (Glassdoor)<br>
 
Gasto Mensal com os tr√™s profissionais: R$ 27.810<br>
Gasto em dois meses: R$ 55.620<br>
Gasto em tr√™s meses: R$ 83.430<br>

**Custos da infraestrutura em AWS:**

Sem o Amazon Comprehend: 

Custo Inicial - R$12.312,18<br>
Custo Mensal - R$34.341,58<br>

Gastos em dois meses: R$ 80.995,34<br>
Gastos em tr√™s meses: R$ 115.336,92<br>


Com o Amazon Comprehend: 

Custo Inicial - R$12.312,18 <br>
Custo Mensal - R$34.404,18 <br>

Gastos em dois meses: R$ 81.120,54 <br>
Gastos em tr√™s meses: R$ 115.524,72 <br>


**Contabiliza√ß√£o total:**

Considerando os custos dos funcion√°rios, somado ao da AWS, o valor do projeto pode variar entre R$136.615 e R$ 198.954, com uma m√©dia de R$ 167.784.  


## (Sprint 1) Entendimento da Experi√™ncia do Usu√°rio

### üë© Persona

A Persona √© a representa√ß√£o fict√≠cia do cliente ideal para o projeto, com o objetivo de compreender os seus comportamentos e necessidades. Nesse momento, entende-se que as personas configuram-se em representantes dos times de marketing, produto e automa√ß√µes.

<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Persona%20-%20Juliano.png">
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Persona%20-%20Renata.png">
<img src = "https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/Persona%20-%20Thiago.png">

### üìÑ User Stories

A User Stories s√£o representa√ß√µes simples e clara dos requisitos e funcionalidades de um software, escritas do ponto de vista do usu√°rio final. Essas hist√≥rias ajudam a manter o foco nas necessidades dos usu√°rios e a priorizar as funcionalidades mais importantes para o projeto. Portanto, a seguir, existem duas User Story por persona (Marketing, Produto e Automa√ß√µes)

![image](https://user-images.githubusercontent.com/99209230/235374746-f4fcbf5d-c13b-491c-a069-743cc575f91c.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374769-140f5bc8-12ba-470f-a625-c0979539f583.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374825-d97405fd-d990-4eaa-8755-552ff7552542.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374856-03169cd6-0cf1-469a-a458-776be352b7d7.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374896-b24aa25c-29b0-4b3b-aa57-20777dd3d50f.png)
<br>
![image](https://user-images.githubusercontent.com/99209230/235374917-cb1c8680-9119-4e06-8d2d-769b34971709.png)


## (Sprint 2) Modelo Bag Of Words (IPYNB)


###  Importa√ß√£o das bibliotecas:

No Pandas, as importa√ß√µes de bibliotecas s√£o usadas para trazer funcionalidades espec√≠ficas de bibliotecas externas para o seu c√≥digo. O Pandas √© uma biblioteca popular para an√°lise de dados em Python, mas para aproveitar ainda mais recursos, pode ser necess√°rio importar outras bibliotecas. As importa√ß√µes no Pandas geralmente s√£o feitas no in√≠cio do c√≥digo e s√£o usadas para importar m√≥dulos adicionais que fornecem funcionalidades extras.

Em primeira inst√¢ncia utilizamos as seguintes bibliiotecas:

pip install emoji: Este comando utiliza o gerenciador de pacotes pip para instalar a biblioteca emoji. A biblioteca emoji √© uma biblioteca Python que fornece funcionalidades para trabalhar com emojis, como a exibi√ß√£o, codifica√ß√£o e manipula√ß√£o de emojis em texto. Ao executar esse comando, voc√™ estar√° instalando a biblioteca emoji em seu ambiente Python.

pip install pyspellchecker: Este comando utiliza o gerenciador de pacotes pip para instalar a biblioteca pyspellchecker. A biblioteca pyspellchecker √© uma biblioteca Python que fornece corre√ß√£o ortogr√°fica em texto. Ela pode ser usada para verificar e corrigir erros ortogr√°ficos em palavras. Ao executar esse comando, voc√™ estar√° instalando a biblioteca pyspellchecker em seu ambiente Python.

Segue abaixo os c√≥digos:
- ```!pip install emoji```
- ```!pip install pyspellchecker```

### Compreens√£o dos Dados 
Foi implementado o m√©todo de carregamento do Dataframe utilizado. Sendo assim, foi criado o caminho da pasta no Google Drive e sua leitura usando "pd.read_csv".
Reorganizamos dessa forma, e renomeamos algumas colunas com o intuito de facilitar o processo de an√°lise.

### An√°lise Descritiva

A an√°lise descritiva √© uma etapa fundamental no acompanhamento e an√°lise de dados. √â uma t√©cnica que aplicada no contexto do nosso proejto em parceria com o BTG-Pactual, envolve a interpreta√ß√£o, comprreens√£o e organiza√ß√£o dos dados de forma a obterpadr√µes e tend√™ncias. Em nosso projeto, essa an√°lise ser√° feita com o intuito de realizar uma an√°lise de sentimentos dos usu√°rios em rela√ß√£o √†s campanhas do BTG, al√©m de facilitar o banco no processo de desenvolver futuras estrta√©gias e ompreender melhor como eles podem gerenciar um bom relacionamento com os clientes. Utilizaremos a an√°lise descritiva para identificar:

- Coment√°rios por tipo de post (Reels, Foto, V√≠deo, Carrossel):
    Dado que cada tipo de m√≠dia possui um objetivo diferente, entende-se que, conforme as suas diferencia√ß√µes, as palavras mais comentadas podem ser diferentes e podem agregar para o usu√°rio.

- Palavras que mais aparecem nos coment√°rios:
    Com a finalidade de entender quais palavras mais se repetem em todos os coment√°rios no perfil do BTG Pactual, desenvolve-se a an√°lise descritiva tendo a *wordcloud*, al√©m dos gr√°ficos de barra e dispers√£o, como representa√ß√µes visuais.

- Conjunto de palavras com maior frequ√™ncia (n-grams = 3):
    Visualizar a frequ√™ncia de poss√≠veis frases para identificar padr√µes em textos que podem ser associados a determinados sentimentos.

-  Rela√ß√£o determin√≠stica entre as colunas Anomalia e Coment√°rio:
    Verificar e a partir das hip√≥teses estruturadas, atribuir possibilidades de atua√ß√£o na coluna "anomalia"

- Emojis na Base de Dados:
    Entendimento de qual seria o melhor tratamento para os emojis, para que a an√°lise de sentimento seja mais precisa, com base nas apari√ß√µes no dataset.
    
   
   ### Pr√©-Processamento

O pr√© processamento √© uma etapa crucial na an√°lise de dados. Esse processo consiste no conjunto de tecnicas aplicados nos dados quando em desenvolvimento de modelos de aprendizado de m√°quina. No contexto do Processamento de Linguagem Natural (PLN), o pr√©-processamento refere-se no na t√©cnica de transformar e preparar os dados em uma forma mais adequada para a realiza√ß√£o de an√°lise de textos. 

Este processo √© crucial no momento de constru√ß√£o de uma an√°lise de dados, e nos modelos de machine learning e geralmente seguem as seguintes etapas: 
* Tokeniza√ß√£o: Processo de dividir um texto em pequenas unidades de texto chamadas de "token". 
* Remo√ß√£o de pontua√ß√µes: Elimina√ß√£o de caracteres de pontua√ß√£o: v√≠rculas, pontos, aspas, entre outros. 
* Convers√£o para min√∫scula: Padronizar as palavras. 
* Remo√ß√£o de stopwords: Remo√ß√£o das palavrad comuns e que n√£o costumam contribuir significativamente para o texto. 
* Stemming e Lematiza√ß√£o: T√©cnica de reduzir as palavras em seus radicais, ou formas mais b√°sicas. 

### Testando etapas do Pr√©-processamento
#### Estrutura√ß√£o do Pr√©-processamento
##### Fun√ß√£o: Retirando valores nulos
Descri√ß√£o: Essa fun√ß√£o remove linhas do DataFrame dados que possuem valores nulos nas colunas 'autor' e 'texto'. O resultado √© armazenado na vari√°vel df_textoAutor.
``` df_textoAutor = dados[['autor', 'texto']].dropna() ```

##### Fun√ß√£o: Retirando posts do btg
Descri√ß√£o: Essa fun√ß√£o remove do DataFrame dados todas as linhas em que o valor da coluna 'autor' √© igual a 'btgpactual'. O resultado √© armazenado na vari√°vel 
```df_textoAutor = dados.drop(dados[dados['autor'] == 'btgpactual'].index) ```

##### Fun√ß√£o: Shape
Descri√ß√£o: Essa fun√ß√£o retorna a dimens√£o do DataFrame df_textoAutor, ou seja, o n√∫mero de linhas e colunas. O resultado ser√° uma tupla com dois elementos, em que o primeiro elemento representa o n√∫mero de linhas e o segundo elemento representa o n√∫mero de colunas.
```df_textoAutor.shape```

#### Fun√ß√£o: Transformando uma frase em min√∫sculas
Descri√ß√£o: Essa fun√ß√£o extrai a frase localizada na linha 100 da coluna 'texto' do DataFrame dados. Em seguida, a fun√ß√£o lower() √© aplicada para converter todos os caracteres da frase em min√∫sculas. O resultado √© armazenado na vari√°vel sentence_teste. Essa transforma√ß√£o √© comumente utilizada para normalizar o texto, tornando-o uniforme e facilitando compara√ß√µes e an√°lises, independentemente das diferen√ßas de capitaliza√ß√£o.
```sentence_teste = dados['texto'].iloc[100].lower()```

### Tokeniza√ß√£o
A tokeniza√ß√£o √© uma etapa importante no pr√©-processamento de texto que envolve a divis√£o de uma sequ√™ncia de texto em unidades menores chamadas de tokens. Esses tokens podem ser palavras individuais, frases, s√≠mbolos ou outros elementos, dependendo do objetivo do processamento.
No contexto do pr√©-processamento de texto no Pandas, a tokeniza√ß√£o geralmente √© realizada em um DataFrame que cont√©m uma coluna de texto. Cada valor nessa coluna, que representa uma senten√ßa ou um documento, √© dividido em tokens individuais. Isso √© √∫til para v√°rias tarefas de processamento de texto, como contagem de palavras, an√°lise de sentimentos, classifica√ß√£o de texto e muito mais.
Existem diferentes abordagens de tokeniza√ß√£o dispon√≠veis, como tokeniza√ß√£o com base em espa√ßos em branco, tokeniza√ß√£o com base em pontua√ß√£o, tokeniza√ß√£o com base em express√µes regulares e tokeniza√ß√£o com base em modelos de linguagem pr√©-treinados. A escolha da t√©cnica de tokeniza√ß√£o depende da natureza dos dados e do objetivo espec√≠fico do processamento de texto que est√° sendo realizado.

#### Fun√ß√µes utilizadas
A fun√ß√£o lower().split() √© utilizada para realizar a tokeniza√ß√£o de uma frase ou sequ√™ncia de texto em Python.

O m√©todo lower() √© aplicado √† vari√°vel sentence_teste e converte todos os caracteres da sequ√™ncia de texto em letras min√∫sculas. Isso √© √∫til para normalizar o texto e garantir consist√™ncia ao realizar a tokeniza√ß√£o.

Em seguida, o m√©todo split() √© chamado para dividir a sequ√™ncia de texto em tokens individuais. Esse m√©todo divide a sequ√™ncia em espa√ßos em branco, resultando em uma lista de tokens.

A lista de tokens √© atribu√≠da √† vari√°vel tokens, que pode ser usada posteriormente para an√°lise de texto, processamento de linguagem natural ou outras tarefas relacionadas ao processamento de texto.

Essa fun√ß√£o √© simples e eficaz para realizar a tokeniza√ß√£o b√°sica de uma frase em Python, dividindo-a em palavras individuais com base nos espa√ßos em branco. No entanto, √© importante observar que essa fun√ß√£o n√£o trata outros tipos de pontua√ß√µes ou casos mais complexos de tokeniza√ß√£o, que podem exigir o uso de bibliotecas ou t√©cnicas mais avan√ßadas.
Segue o c√≥digo abaixo:
```tokens = sentence_teste.lower().split() ```

### Stop-Words
Stop words s√£o palavras comuns que geralmente s√£o removidas durante o pr√©-processamento de texto, pois s√£o consideradas pouco informativas para a an√°lise de texto. Essas palavras incluem artigos, conjun√ß√µes, preposi√ß√µes e outros termos frequentemente encontrados na linguagem, como "a", "o", "em", "de", "e", entre outros.

A remo√ß√£o de stop words √© uma etapa comum no pr√©-processamento de texto, pois ajuda a reduzir o ru√≠do e o tamanho do vocabul√°rio utilizado na an√°lise. Ao remover essas palavras, √© poss√≠vel focar em termos mais relevantes e significativos para a tarefa em quest√£o, como an√°lise de sentimentos, classifica√ß√£o de texto ou minera√ß√£o de t√≥picos.

No contexto do pandas, a remo√ß√£o de stop words geralmente envolve o uso de bibliotecas de processamento de linguagem natural, como NLTK (Natural Language Toolkit) ou spaCy. Essas bibliotecas possuem listas predefinidas de stop words em diferentes idiomas, que podem ser aplicadas aos dados textuais para remover essas palavras desnecess√°rias antes de prosseguir com a an√°lise ou modelagem de texto.

#### Fun√ß√µes utilizadas
A fun√ß√£o translate() √© utilizada para remover pontua√ß√µes de uma string. Nesse caso espec√≠fico, a fun√ß√£o str.maketrans('', '', string.punctuation) cria uma tabela de tradu√ß√£o que mapeia os caracteres de pontua√ß√£o para um valor vazio (''). Em seguida, a fun√ß√£o translate() aplica essa tabela de tradu√ß√£o √† string sentence_teste, removendo todas as pontua√ß√µes.

J√° a fun√ß√£o strip() √© utilizada para remover espa√ßos em branco (espa√ßos, tabula√ß√µes, quebras de linha) no in√≠cio e no final de uma string. Ela retorna a vers√£o da string sem os espa√ßos em branco.

Essas duas fun√ß√µes em sequ√™ncia t√™m o objetivo de remover pontua√ß√µes e espa√ßos em branco extras da string sentence_teste, deixando-a limpa e pronta para ser processada ou analisada posteriormente.
Segue o c√≥digo abaixo:
- ```sentence_teste = sentence_teste.translate(str.maketrans('', '', string.punctuation)) ```
- ```sentence_teste = sentence_teste.strip()```

Tamb√©m encontra-se nesse c√≥digo, a vari√°vel stop_words √© inicializada com um conjunto de palavras de parada (stop words) em portugu√™s, obtidas a partir do m√≥dulo nltk.corpus.stopwords. Essas palavras s√£o geralmente consideradas irrelevantes para a an√°lise de texto, como artigos, preposi√ß√µes e pronomes.

Em seguida, uma lista adicional chamada stop_words_add √© criada, contendo palavras adicionais que ser√£o inclu√≠das nas stop words. Essas palavras podem ser personalizadas de acordo com as necessidades do projeto.

O m√©todo update √© usado para adicionar as palavras da lista stop_words_add ao conjunto de stop words existente.

Em seguida, √© criada uma lista vazia chamada new_words. Em um loop, cada palavra em sentence_teste √© verificada se est√° presente no conjunto de stop words. Se a palavra n√£o for uma stop word, ela √© adicionada √† lista new_words.

Por fim, a vari√°vel sentence_teste √© atualizada, substituindo seu valor original pela concatena√ß√£o das palavras contidas em new_words, formando assim uma nova vers√£o da senten√ßa sem as stop words.

Segue o c√≥digo abaixo: 
```top_words = set(nltk.corpus.stopwords.words('portuguese'))`` 
stop_words_add = ['ola', 'ol√°', 'pra', 'para', 'bemvindo','benvindo', 'bem-vindo', 'bemvindos', 'aqui', 'vai', 'btgpactual']
stop_words.update(stop_words_add)
new_words = []
for word in sentence_teste:
    if word not in stop_words:
        new_words.append(word)
        sentence_teste = ''.join(new_words) ``` 
        
### Testando corretor de palavras
Nesse c√≥digo, a biblioteca spellchecker √© importada, e em seguida, uma frase incorreta √© atribu√≠da √† vari√°vel frase_errada. A frase √© dividida em palavras individuais usando o m√©todo split() e armazenada na lista words.

A classe SpellChecker √© inicializada com o par√¢metro language='pt', indicando que o corretor ortogr√°fico ser√° utilizado para o idioma portugu√™s.

Por fim, um objeto spell do tipo SpellChecker √© criado e est√° pronto para ser usado para corre√ß√£o ortogr√°fica das palavras contidas na frase incorreta.

Segue o c√≥digo abaixo:
```from spellchecker import SpellChecker```
```frase_errada = 'As veses estol gostandu di vose```
```words = frase_errada.split()```
```spell = SpellChecker(language='pt')``` 

### Testando corretor de abrevia√ß√µes e deletar emojis

Nesse c√≥digo, a biblioteca enelvo √© importada e em seguida a classe Normaliser √© utilizada.

O objetivo desse c√≥digo √© realizar a normaliza√ß√£o de texto, que consiste em aplicar transforma√ß√µes espec√≠ficas para padronizar ou corrigir palavras em um texto.

Na primeira linha, a classe Normaliser √© importada da biblioteca enelvo.

Em seguida, uma mensagem √© atribu√≠da √† vari√°vel msg, que cont√©m o texto a ser corrigido ou normalizado.

Por fim, um objeto norm do tipo Normaliser √© criado, e √© utilizado o par√¢metro tokenizer='readable', indicando que o texto ser√° tokenizado de forma leg√≠vel, ou seja, separando-o em palavras individuais considerando a estrutura gramatical.

O objeto norm est√° pronto para ser utilizado para realizar as corre√ß√µes ou normaliza√ß√µes no texto contido na vari√°vel msg.
Segue o c√≥digo abaixo:
```from enelvo.normaliser import Normaliser```
```msg = 'hj vou usar meu cart√£o do banco btg, pq gosto mt deleüëä'```
```norm = Normaliser(tokenizer='readable')```

### Stemming
Em pr√©-processamento de texto, stemming √© uma t√©cnica utilizada para reduzir palavras √† sua forma b√°sica ou raiz, removendo sufixos e prefixos. O objetivo √© simplificar a an√°lise de texto, tratando diferentes varia√ß√µes da mesma palavra como uma √∫nica forma, o que pode facilitar a compara√ß√£o e agrupamento de palavras semelhantes.

Na pr√°tica, j√° no c√≥digo, a biblioteca nltk √© importada e a classe SnowballStemmer √© utilizada.

O objetivo desse c√≥digo √© realizar a t√©cnica de stemming, que consiste em reduzir palavras √† sua forma b√°sica (ou raiz) removendo sufixos e prefixos, para facilitar a an√°lise de texto.

Na primeira linha, a classe SnowballStemmer √© importada da biblioteca nltk e √© especificado o idioma 'portuguese' como par√¢metro para o construtor do stemmer, indicando que o stemming ser√° realizado para palavras em portugu√™s.

Em seguida, um loop for √© utilizado para iterar sobre cada palavra presente no texto sentence_teste, que provavelmente cont√©m v√°rias palavras tokenizadas.

Dentro do loop, a fun√ß√£o stem() do objeto stemmer √© chamada para cada palavra, retornando a raiz ou forma b√°sica da palavra.

A raiz de cada palavra √© impressa na sa√≠da utilizando a fun√ß√£o print(), representando o resultado do stemming para cada palavra no texto.

Assim, o c√≥digo realiza o processo de stemming para cada palavra no texto sentence_teste, retornando suas formas b√°sicas ou ra√≠zes.
Segue o c√≥digo abaixo:
```from nltk.stem.snowball import SnowballStemmer```
```stemmer = SnowballStemmer('portuguese')```
```for word in sentence_teste.split():```
    ```print(stemmer.stem(word))```
    
### Bag Of Words

O modelo Bag-of-Words √© uma abordagem comum no pr√©-processamento de texto usada para representar documentos de texto como vetores num√©ricos. √â uma t√©cnica simples e amplamente utilizada em tarefas de processamento de linguagem natural.

No contexto do Google Colab, o pr√©-processamento com o modelo Bag-of-Words envolve as seguintes etapas:

Tokeniza√ß√£o: O texto √© dividido em unidades menores chamadas "tokens". Geralmente, os tokens s√£o palavras individuais, mas tamb√©m podem ser caracteres, n-grams (sequ√™ncias de n tokens consecutivos) ou outras unidades, dependendo do caso de uso.

Constru√ß√£o do vocabul√°rio: O vocabul√°rio √© criado coletando todos os tokens √∫nicos presentes nos documentos de texto. Cada token √∫nico √© atribu√≠do a um √≠ndice √∫nico no vocabul√°rio.

Codifica√ß√£o dos documentos: Cada documento de texto √© codificado como um vetor num√©rico de acordo com o vocabul√°rio constru√≠do. O tamanho do vetor √© igual ao tamanho do vocabul√°rio. Cada posi√ß√£o no vetor representa uma palavra do vocabul√°rio, e o valor naquela posi√ß√£o indica a frequ√™ncia ou outra medida de import√¢ncia do termo no documento.

Matriz de documentos-termos: Todos os documentos s√£o representados em uma matriz, em que cada linha corresponde a um documento e cada coluna corresponde a um termo do vocabul√°rio. Os valores da matriz s√£o geralmente contagens de frequ√™ncia, mas tamb√©m podem ser pesos TF-IDF (term frequency-inverse document frequency) ou outros esquemas de pondera√ß√£o.

Essa representa√ß√£o baseada no modelo Bag-of-Words permite que os algoritmos de aprendizado de m√°quina trabalhem com dados de texto, que normalmente requerem entrada num√©rica. No Google Colab, voc√™ pode implementar essas etapas usando bibliotecas de processamento de texto, como NLTK (Natural Language Toolkit), e aplic√°-las aos seus dados de texto para prepar√°-los para tarefas de classifica√ß√£o, agrupamento ou outras an√°lises.

#### Fun√ß√µes utilizadas:
O c√≥digo fornecido realiza a vetoriza√ß√£o de texto usando o CountVectorizer da biblioteca scikit-learn. Vejamos o que cada linha faz:

- Importa√ß√£o das bibliotecas:
-Essas linhas importam as classes CountVectorizer e TfidfVectorizer da biblioteca sklearn.feature_extraction.text, necess√°rias para realizar a vetoriza√ß√£o de texto.
```from sklearn.feature_extraction.text import CountVectorizer```
```from sklearn.feature_extraction.text import TfidfVectorizer```

- Instancia√ß√£o do vetorizador:
-Aqui, um objeto CountVectorizer √© criado e atribu√≠do √† vari√°vel vectorizer. O CountVectorizer √© usado para converter o texto em uma matriz de contagens de palavras.
```vectorizer.fit(frases_pre)```

- Ajuste do vetorizador aos dados de entrada:
-Essa linha ajusta o vetorizador aos dados de entrada frases_pre. Ele analisa o texto fornecido, constr√≥i o vocabul√°rio e atribui um √≠ndice num√©rico √∫nico a cada palavra encontrada nas frases.
 ``` vectorizer.fit(frases_pre)```

- Exibi√ß√£o do vocabul√°rio ordenado:
-Essa linha imprime o vocabul√°rio ordenado alfabeticamente. O vocabul√°rio √© um dicion√°rio que mapeia as palavras encontradas nas frases para seus respectivos √≠ndices num√©ricos.
```print(sorted(vectorizer.vocabulary_)) ```
- Transforma√ß√£o dos dados em uma representa√ß√£o vetorial:
-Aqui, o m√©todo transform √© chamado para converter as frases pr√©-processadas frases_pre em uma matriz vetorial. Cada linha da matriz representa uma frase, e cada coluna representa uma palavra do vocabul√°rio. O valor em cada posi√ß√£o da matriz representa a contagem de ocorr√™ncias da palavra correspondente na frase.
```vector = vectorizer.transform(frases_pre)```

- Sumariza√ß√£o dos resultados:
-Essas linhas exibem a forma (shape) da matriz resultante, que indica o n√∫mero de frases e o tamanho do vocabul√°rio. Em seguida, √© impressa a representa√ß√£o em formato de array da matriz vetorizada, mostrando as contagens de palavras para cada frase.
```print(vector.shape)```
```print(vector.toarray())```

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/0cae7c4b-5c5b-43cf-a164-3917b19e4779)

## TFID
 O TfidVectorizer calcula o inverso das frequ√™ncias e codifica os vetores a fim de calcular a relev√¢ncia de cada termo nos documentos. Diferente do CountVectorizer, este algoritmo calcula 'word frequencies'. Isso impede que, por exemplo, artigos ou palavras n√£o muito significantes acabem sendo reconhecidos como muito relevantes apenas pelo grande n√∫mero de ocorr√™ncias na base de dados, uma vez que essa frequ√™ncia inversa leva mais em conta o contexto das palavras empregadas em cada frase.
 Segue o c√≥digo abaixo:
``` vectorizer = TfidfVectorizer()```
``` vectorizer.fit(frases_pre)```
```print(sorted(vectorizer.vocabulary_))```
```vector = vectorizer.transform([frases_pre[0]])```

![image](https://github.com/2023M6T4-Inteli/Projeto2/assets/99270135/7161d030-b594-4156-82ea-95336c3f50b7)

### Resultados 

###  Gr√°fico Word Cloud

O Gr√°fico de Nuvem de Palavras, conhecido tamb√©m como como Word Cloud, √© uma ferramenta de representa√ß√£o visual que trabalha com a *plotagem* das palavras mais frequentes em um conjunto de textos. Nesse contexto, foi desenvolvido com o intuito de mostrar as palavras mais recorrentes e utilizadas pelos usu√°rios nos coment√°rios das postagens. 

![image](https://github.com/2023M6T4-Inteli/Projeto2/blob/main/docs/Imagens/wordcloud.png)

## (Sprint 3) Modelo utilizando Word2Vec (IPYNB)

Colocar o link do artefato (deve estar na pasta src do reposit√≥rio do projeto).

## (Sprint 3) Documenta√ß√£o do Modelo utilizando Word2Vec

Preencher conforme a descri√ß√£o do artefato na Adalove.

## (Sprint 4) Proposta de uma nova modelagem utilizando novas features (IPYNB)

Colocar o link do artefato (deve estar na pasta src do reposit√≥rio do projeto).

## (Sprint 4) Documenta√ß√£o da proposta de uma nova modelagem

Preencher conforme a descri√ß√£o do artefato na Adalove.

## (Sprint 5) Apresenta√ß√£o Final

Colocar o link do artefato (deve estar na pasta apresentacoes do reposit√≥rio do projeto).

## (Sprint 5) Deploy do melhor modelo

Colocar o link dos artefatos (devem estar nas pastas videos e src do reposit√≥rio do projeto).

## (Sprint 5) Documenta√ß√£o da Solu√ß√£o

Preencher conforme a descri√ß√£o do artefato na Adalove.
